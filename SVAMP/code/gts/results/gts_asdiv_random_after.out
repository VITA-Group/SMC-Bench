wandb: WARNING W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.
2022-09-10 07:20:15,644 | INFO | main_after.py: 96 : main() ::	 Experiment Name: run_cv_asdiv-a
2022-09-10 07:20:15,644 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:20:15,644 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:20:16,022 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:20:16,023 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 979
2022-09-10 07:20:16,023 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 238
2022-09-10 07:20:16,023 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: []
2022-09-10 07:20:16,023 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:20:16,023 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:20:16,203 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 2882 / 2882 = 1.0
2022-09-10 07:20:16,234 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 2885 words in input language, 12 words in output
2022-09-10 07:20:16,242 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:20:19,479 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:20:19,479 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:20:23,182 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:20:23,182 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/asdiv/dense/models/run_cv_asdiv-a_fold0.
2022-09-10 07:20:28,832 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:20:36,872 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.1722689075630252	
 equation acc epoch: 0.16806722689075632	
 max val acc: 0.1722689075630252	
 equation acc: 0.16806722689075632	
2022-09-10 07:20:36,872 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 8s
2022-09-10 07:20:36,875 | INFO | main_after.py: 96 : main() ::	 Experiment Name: run_cv_asdiv-a
2022-09-10 07:20:36,875 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:20:36,875 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:20:37,165 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:20:37,165 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 979
2022-09-10 07:20:37,165 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 238
2022-09-10 07:20:37,165 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: []
2022-09-10 07:20:37,165 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:20:37,165 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:20:37,340 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 2882 / 2882 = 1.0
2022-09-10 07:20:37,370 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 2885 words in input language, 12 words in output
2022-09-10 07:20:37,378 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:20:40,375 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:20:40,375 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:20:40,455 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:20:40,455 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/asdiv/dense/models/run_cv_asdiv-a_fold1.
2022-09-10 07:20:45,848 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:20:53,873 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.15126050420168066	
 equation acc epoch: 0.058823529411764705	
 max val acc: 0.15126050420168066	
 equation acc: 0.058823529411764705	
2022-09-10 07:20:53,873 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 8s
2022-09-10 07:20:53,877 | INFO | main_after.py: 96 : main() ::	 Experiment Name: run_cv_asdiv-a
2022-09-10 07:20:53,877 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:20:53,877 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:20:54,163 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:20:54,163 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 979
2022-09-10 07:20:54,163 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 238
2022-09-10 07:20:54,163 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: []
2022-09-10 07:20:54,163 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:20:54,163 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:20:54,338 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 2882 / 2882 = 1.0
2022-09-10 07:20:54,369 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 2885 words in input language, 12 words in output
2022-09-10 07:20:54,377 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:20:57,235 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:20:57,235 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:20:57,315 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:20:57,315 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/asdiv/dense/models/run_cv_asdiv-a_fold2.
2022-09-10 07:21:02,649 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:21:10,466 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.1638655462184874	
 equation acc epoch: 0.13025210084033614	
 max val acc: 0.1638655462184874	
 equation acc: 0.13025210084033614	
2022-09-10 07:21:10,467 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 7s
2022-09-10 07:21:10,469 | INFO | main_after.py: 96 : main() ::	 Experiment Name: run_cv_asdiv-a
2022-09-10 07:21:10,469 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:21:10,470 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:21:10,760 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:21:10,760 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 980
2022-09-10 07:21:10,760 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 237
2022-09-10 07:21:10,760 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: []
2022-09-10 07:21:10,760 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:21:10,760 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:21:10,935 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 2882 / 2882 = 1.0
2022-09-10 07:21:10,965 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 2885 words in input language, 12 words in output
2022-09-10 07:21:10,973 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:21:13,826 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:21:13,826 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:21:13,906 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:21:13,906 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/asdiv/dense/models/run_cv_asdiv-a_fold3.
2022-09-10 07:21:19,278 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:21:28,621 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.1350210970464135	
 equation acc epoch: 0.0970464135021097	
 max val acc: 0.1350210970464135	
 equation acc: 0.0970464135021097	
2022-09-10 07:21:28,621 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 9s
2022-09-10 07:21:28,623 | INFO | main_after.py: 96 : main() ::	 Experiment Name: run_cv_asdiv-a
2022-09-10 07:21:28,624 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:21:28,624 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:21:28,915 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:21:28,915 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 951
2022-09-10 07:21:28,915 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 266
2022-09-10 07:21:28,915 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: []
2022-09-10 07:21:28,915 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:21:28,915 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:21:29,092 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 2882 / 2882 = 1.0
2022-09-10 07:21:29,122 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 2885 words in input language, 12 words in output
2022-09-10 07:21:29,130 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:21:31,935 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:21:31,935 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:21:32,013 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:21:32,014 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/asdiv/dense/models/run_cv_asdiv-a_fold4.
2022-09-10 07:21:37,182 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:21:47,600 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.12030075187969924	
 equation acc epoch: 0.09774436090225563	
 max val acc: 0.12030075187969924	
 equation acc: 0.09774436090225563	
2022-09-10 07:21:47,600 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 10s
2022-09-10 07:21:47,600 | INFO | main_after.py: 272 : main() ::	 Final Val score: 0.14790468364831552
Transfer numbers...
initialize by random pruning
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.19998248864352264
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.2020762402723736
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 0.234375
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.19978162977430558
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.19936116536458337
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.1998816596137153
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.20094129774305558
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.19986936781141496
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.1999532911512587
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.1995849609375
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.19980367024739587
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.20005459255642366
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.20010545518663192
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.19980578952365446
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.1994149949815538
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.2005700005425347
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.19984096950954866
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.19969516330295134
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.19951205783420134
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.19989734225802946
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.20001263088650179
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.20005628797743058
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.2004784478081597
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.2000037299262153
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.20067003038194442
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.2001580132378472
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.2001300387912326
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.20008341471354163
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.19972398546006942
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.19938998752170134
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.1983727349175347
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.20001390245225692
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.2003589206271701
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.20004442003038192
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.20046827528211808
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.20051744249131942
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.20014784071180558
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.19986894395616317
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.19969219631618929
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.20020548502604163
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.2009497748480903
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.199371337890625
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.199798583984375
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.19995837741427946
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.20019192165798616
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.19923231336805558
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.19896104600694442
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.20009019639756942
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.1997460259331597
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.20041826036241317
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.1998664008246528
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.20007832845052087
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.2001274956597222
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.19958157009548616
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.2002936469184028
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.20013682047526038
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.19960276285807288
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.19999864366319442
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.19957139756944442
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.1992407904730903
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.2004174126519097
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.200079600016276
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.19969262017144096
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.19976806640625
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.19970194498697913
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.20077853732638884
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.2004326714409722
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.19988081190321183
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.19978502061631942
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.19985622829861116
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.19987996419270837
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.19967481825086808
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.2000732421875
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.20008977254231775
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.2002368503146701
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.200408935546875
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.1993122100830078
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.19967174530029297
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.19991811116536462
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.20038414001464844
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.19959115982055664
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.19985675811767578
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.2001953125
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.20010662078857422
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.1993865966796875
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.20126724243164062
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.19866561889648438
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.2005786895751953
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.198974609375
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.1999034881591797
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.185546875
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.19935862223307288
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.1875
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.19466145833333337
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.2006617954799107
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.2001473563058036
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.20008959089006695
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.1991010393415179
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.2007184709821429
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.1997440883091518
Final sparsity level of 0.2: 0.19996648636925907
Transfer numbers...
initialize by random pruning
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.19993308900328255
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.20126307149805445
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 0.19270833333333337
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.19993082682291663
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.20062594943576384
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.20062764485677087
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.20056491427951384
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.2000359429253472
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.1995476616753472
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.2002343071831597
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.200164794921875
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.20126173231336808
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.20013088650173616
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.20041063096788192
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.19950951470269096
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.1997460259331597
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.20115322536892366
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.1989661322699653
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.19923739963107634
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.20002831353081596
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.19998423258463538
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.19924418131510413
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.20021396213107634
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.19978162977430558
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.20006306966145837
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.2000011867947049
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.19996515909830725
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.19981045193142366
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.1993950737847222
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.19977315266927087
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.19939676920572913
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.1998358832465278
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.1998769972059462
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.20003763834635413
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.19990878634982634
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.1988372802734375
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.2002495659722222
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.1999532911512587
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.20008977254231775
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.19972737630208337
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.19906785753038192
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.1999749077690972
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.20020887586805558
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.19973924424913192
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.19989183213975692
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.1996527777777778
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.20103793674045134
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.20147365993923616
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.20036485460069442
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.20003763834635413
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.1999732123480903
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.20010206434461808
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.2007073296440972
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.20009867350260413
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.19953579372829866
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.1995849609375
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.19996092054578996
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.19999525282118058
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.20033772786458337
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.20000033908420134
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.20002237955729163
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.20004230075412321
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.19989140828450525
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.1998969184027778
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.20051066080729163
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.20055135091145837
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.19983079698350692
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.19996049669053817
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.20006857977973092
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.19963582356770837
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.20072597927517366
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.19976298014322913
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.20017496744791663
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.19956843058268225
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.2001452975802951
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.2005615234375
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.20005480448404944
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.2003622055053711
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.19965998331705725
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.19954776763916016
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.20053434371948242
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.19986724853515625
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.20043611526489258
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.19963359832763672
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.19877243041992188
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.2005748748779297
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.19975662231445312
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.19977569580078125
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.20458984375
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.19985198974609375
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.203125
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.19961039225260413
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.19921875
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.20084635416666663
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.20056043352399555
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.1994759695870536
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.1993691580636161
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.2004677908761161
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.1990596226283482
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.19941166469029015
Final sparsity level of 0.2: 0.19996037372447173
Transfer numbers...
initialize by random pruning
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.19999453417719415
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.20090335197795073
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 0.19140625
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.20012071397569442
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.2002326117621528
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.20032925075954866
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.19993082682291663
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.20016013251410592
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.2000647650824653
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.20033603244357634
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.20049879286024308
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.19977823893229163
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.19820827907986116
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.20028644137912321
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.2001385158962674
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.19977315266927087
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.1998138427734375
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.20020718044704866
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.20066663953993058
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.2000206841362847
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.2001126607259115
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.1997918023003472
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.20039198133680558
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.19912380642361116
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.19995456271701384
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.20000712076822913
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.1999727884928385
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.19860670301649308
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.19953579372829866
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.20087856716579866
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.20007832845052087
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.20021141899956596
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.20003721449110246
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.19990030924479163
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.20058186848958337
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.20006306966145837
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.20064968532986116
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.19974729749891496
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.20009316338433158
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.20091586642795134
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.199554443359375
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.2003241644965278
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.20035807291666663
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.19966167873806429
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.2000643412272135
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.19991726345486116
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.20054287380642366
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.2007208930121528
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.20036824544270837
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.1994849310980903
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.1996680365668403
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.19970872667100692
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.1998206244574653
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.20045132107204866
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.19949679904513884
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.19971974690755212
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.20009231567382812
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.2000969780815972
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.1999732123480903
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.19973246256510413
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.20017666286892366
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.19975450303819442
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.19984563191731775
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.19942050509982634
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.20093790690104163
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.20106845431857634
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.200042724609375
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.1997595893012153
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.20005332099066842
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.20073106553819442
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.1998986138237847
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.20000881618923616
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.20077684190538192
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.2002326117621528
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.2000622219509549
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.20013427734375
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.19999313354492188
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.20011615753173828
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.19952265421549475
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.20007038116455078
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.20037555694580078
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.19953155517578125
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.19951200485229492
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.2003011703491211
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.20069122314453125
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.19940567016601562
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.19854736328125
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.1992053985595703
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.19873046875
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.2002716064453125
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.166015625
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.1997159322102865
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.197265625
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.20865885416666663
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.19964926583426335
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.1998574393136161
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.2005266462053571
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.1997353690011161
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.19968196323939735
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.19943128313337055
Final sparsity level of 0.2: 0.199986804163654
Transfer numbers...
initialize by random pruning
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.19993187149772873
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.20208637321660183
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 0.19791666666666663
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.19966295030381942
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.20013258192274308
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.19963412814670134
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.19940355088975692
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.20016394721137154
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.19989564683702254
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.20031399197048616
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.20019192165798616
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.19956631130642366
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.199554443359375
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.19954469468858504
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.20025549994574654
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.20095655653211808
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.20021226671006942
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.19992404513888884
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.19998508029513884
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.19958623250325525
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.20007408989800346
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.19911532931857634
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.1995018853081597
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.20012240939670134
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.20013597276475692
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.20020336574978304
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.19997914632161462
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.20054117838541663
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.2002495659722222
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.2005615234375
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.19984775119357634
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.20016818576388884
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.19999821980794275
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.1993560791015625
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.20020548502604163
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.20023939344618058
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.19973585340711808
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.20042928059895837
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.19996727837456596
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.19930691189236116
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.1997765435112847
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.19963582356770837
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.1989508734809028
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.20024151272243929
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.20037926567925346
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.2009039984809028
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.20019700792100692
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.20010715060763884
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.20117696126302087
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.20023388332790804
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.20016521877712679
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.20049879286024308
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.19949679904513884
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.20026143391927087
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.19959513346354163
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.19982994927300346
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.19983842637803817
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.19917636447482634
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.19958157009548616
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.1996375189887153
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.20028516981336808
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.20014402601453996
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.20004018147786462
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.2000495062934028
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.20063951280381942
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.1995154486762153
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.19983249240451384
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.19989988538953996
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.20019827948676217
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.20038011338975692
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.2008446587456597
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.20035298665364587
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.19983418782552087
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.19999101426866317
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.19965659247504342
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.19995625813802087
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.200469970703125
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.19936656951904297
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.19970194498697913
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.19976329803466797
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.20040273666381836
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.19997406005859375
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.19970130920410156
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.20087337493896484
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.20040512084960938
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.19954872131347656
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.19977951049804688
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.20043182373046875
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.19775390625
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.2002582550048828
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.208984375
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.20064163208007812
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.201171875
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.19498697916666663
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.19976915631975445
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.19957733154296875
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.2002519880022321
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.19951411655970985
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.20019640241350445
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.1998051234654018
Final sparsity level of 0.2: 0.20000057319024733
Transfer numbers...
initialize by random pruning
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.20006999361716238
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.20100468142023342
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 0.22395833333333337
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.20053270128038192
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.1996086968315972
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.1995476616753472
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.20032755533854163
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.20013427734375
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.20009824964735246
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.20031568739149308
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.2000054253472222
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.199432373046875
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.19945780436197913
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.1999121771918403
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.2000774807400174
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.199615478515625
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.19985622829861116
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.19909498426649308
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.19936794704861116
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.1997138129340278
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.19986258612738717
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.19986300998263884
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.1985846625434028
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.20074293348524308
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.19942728678385413
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.1997138129340278
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.20015207926432288
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.19949849446614587
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.20068528917100692
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.19987318250868058
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.19952223036024308
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.19935014512803817
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.19975535074869788
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.1996002197265625
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.19933064778645837
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.1996680365668403
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.19989522298177087
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.20040724012586808
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.20006815592447913
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.19921535915798616
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.1998443603515625
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.1992034912109375
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.19952562120225692
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.1998333401150174
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.199883778889974
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.20001051161024308
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.19916788736979163
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.20014444986979163
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.2005683051215278
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.20021777682834196
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.20021989610460067
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.2001105414496528
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.20137532552083337
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.20027499728732634
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.20005967881944442
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.19976425170898438
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.19984351264105904
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.19952901204427087
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.20020887586805558
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.19955274793836808
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.19986470540364587
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.20013766818576384
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.1999227735731337
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.20021565755208337
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.2007666693793403
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.19990878634982634
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.19990709092881942
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.20041020711263025
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.19986300998263884
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.20003085666232634
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.1997595893012153
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.2003411187065972
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.19979688856336808
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.19998762342664933
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.19988420274522567
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.19994947645399308
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.20025634765625
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.1999835968017578
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.20027669270833337
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.19984817504882812
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.19992923736572266
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.20005035400390625
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.20022153854370117
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.19989013671875
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.19969558715820312
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.2005767822265625
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.20128631591796875
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.2009296417236328
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.2060546875
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.20067977905273438
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.20703125
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.199975331624349
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.19140625
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.19759114583333337
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.200653076171875
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.20001329694475445
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.2003370012555804
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.2003958565848214
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.2000449044363839
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.20013972691127235
Final sparsity level of 0.2: 0.20000700427406093
wandb: WARNING W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.
2022-09-10 07:21:51,088 | INFO | main_after.py: 96 : main() ::	 Experiment Name: run_cv_asdiv-a
2022-09-10 07:21:51,088 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:21:51,088 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:21:51,452 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:21:51,453 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 979
2022-09-10 07:21:51,453 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 238
2022-09-10 07:21:51,453 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: []
2022-09-10 07:21:51,453 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:21:51,453 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:21:51,633 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 2882 / 2882 = 1.0
2022-09-10 07:21:51,664 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 2885 words in input language, 12 words in output
2022-09-10 07:21:51,671 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:21:54,647 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:21:54,647 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:21:57,976 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:21:57,976 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/asdiv/dense/models/run_cv_asdiv-a_fold0.
2022-09-10 07:22:02,754 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:22:10,473 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.16806722689075632	
 equation acc epoch: 0.14285714285714285	
 max val acc: 0.16806722689075632	
 equation acc: 0.14285714285714285	
2022-09-10 07:22:10,473 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 7s
2022-09-10 07:22:10,475 | INFO | main_after.py: 96 : main() ::	 Experiment Name: run_cv_asdiv-a
2022-09-10 07:22:10,475 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:22:10,475 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:22:10,767 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:22:10,767 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 979
2022-09-10 07:22:10,767 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 238
2022-09-10 07:22:10,767 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: []
2022-09-10 07:22:10,768 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:22:10,768 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:22:10,942 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 2882 / 2882 = 1.0
2022-09-10 07:22:10,973 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 2885 words in input language, 12 words in output
2022-09-10 07:22:10,980 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:22:13,930 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:22:13,930 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:22:14,029 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:22:14,029 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/asdiv/dense/models/run_cv_asdiv-a_fold1.
2022-09-10 07:22:18,938 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:22:26,710 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.1638655462184874	
 equation acc epoch: 0.0546218487394958	
 max val acc: 0.1638655462184874	
 equation acc: 0.0546218487394958	
2022-09-10 07:22:26,710 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 7s
2022-09-10 07:22:26,713 | INFO | main_after.py: 96 : main() ::	 Experiment Name: run_cv_asdiv-a
2022-09-10 07:22:26,713 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:22:26,713 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:22:27,004 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:22:27,004 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 979
2022-09-10 07:22:27,004 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 238
2022-09-10 07:22:27,005 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: []
2022-09-10 07:22:27,005 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:22:27,005 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:22:27,178 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 2882 / 2882 = 1.0
2022-09-10 07:22:27,209 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 2885 words in input language, 12 words in output
2022-09-10 07:22:27,219 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:22:30,393 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:22:30,393 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:22:30,473 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:22:30,473 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/asdiv/dense/models/run_cv_asdiv-a_fold2.
2022-09-10 07:22:35,328 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:22:43,086 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.1722689075630252	
 equation acc epoch: 0.13025210084033614	
 max val acc: 0.1722689075630252	
 equation acc: 0.13025210084033614	
2022-09-10 07:22:43,086 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 7s
2022-09-10 07:22:43,088 | INFO | main_after.py: 96 : main() ::	 Experiment Name: run_cv_asdiv-a
2022-09-10 07:22:43,089 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:22:43,089 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:22:43,378 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:22:43,379 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 980
2022-09-10 07:22:43,379 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 237
2022-09-10 07:22:43,379 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: []
2022-09-10 07:22:43,379 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:22:43,379 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:22:43,553 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 2882 / 2882 = 1.0
2022-09-10 07:22:43,584 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 2885 words in input language, 12 words in output
2022-09-10 07:22:43,592 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:22:46,417 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:22:46,417 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:22:46,498 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:22:46,498 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/asdiv/dense/models/run_cv_asdiv-a_fold3.
2022-09-10 07:22:49,026 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:22:56,001 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.10970464135021098	
 equation acc epoch: 0.04219409282700422	
 max val acc: 0.10970464135021098	
 equation acc: 0.04219409282700422	
2022-09-10 07:22:56,001 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 6s
2022-09-10 07:22:56,003 | INFO | main_after.py: 96 : main() ::	 Experiment Name: run_cv_asdiv-a
2022-09-10 07:22:56,003 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:22:56,003 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:22:56,295 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:22:56,295 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 951
2022-09-10 07:22:56,295 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 266
2022-09-10 07:22:56,295 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: []
2022-09-10 07:22:56,295 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:22:56,295 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:22:56,473 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 2882 / 2882 = 1.0
2022-09-10 07:22:56,503 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 2885 words in input language, 12 words in output
2022-09-10 07:22:56,512 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:22:59,366 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:22:59,367 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:22:59,446 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:22:59,446 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/asdiv/dense/models/run_cv_asdiv-a_fold4.
2022-09-10 07:23:01,522 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:23:08,119 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.015037593984962405	
 equation acc epoch: 0.007518796992481203	
 max val acc: 0.015037593984962405	
 equation acc: 0.007518796992481203	
2022-09-10 07:23:08,119 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 6s
2022-09-10 07:23:08,119 | INFO | main_after.py: 272 : main() ::	 Final Val score: 0.12325390304026294
Transfer numbers...
initialize by random pruning
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.3599404406644783
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.3620931622892347
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 0.38411458333333337
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.3595648871527778
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.35982937282986116
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.3598564995659722
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.36079915364583337
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.359710693359375
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.36013497246636283
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.3596886528862847
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.359222412109375
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.35976155598958337
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.36064656575520837
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.3598289489746094
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.35997305976019967
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.3601226806640625
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.3594377305772569
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.3600548638237847
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.35978190104166663
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.35993364122178817
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.35980733235677087
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.35971577962239587
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.3598717583550347
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.3602617051866319
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.35997348361545134
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.3601290384928385
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.3597568935818143
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.36027187771267366
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.35947842068142366
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.3595208062065972
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.3591630723741319
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.3601701524522569
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.3601667616102431
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.3602125379774306
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.36115010579427087
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.3610466851128472
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.36000569661458337
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.35972679985894096
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.3595360649956597
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.36060757107204866
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.36093478732638884
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.3595598008897569
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.35959201388888884
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.35968017578125
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.3599679734971788
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.35921054416232634
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.35878499348958337
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.35983784993489587
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.3597700330946181
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.3606478373209635
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.36016718546549475
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.36005655924479163
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.3599310980902778
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.3596937391493056
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.3599378797743056
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.36017778184678817
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.35994932386610246
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.3593902587890625
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.36060078938802087
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.35907999674479163
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.35981241861979163
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.3602104187011719
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.3597373962402344
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.3608686659071181
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.360015869140625
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.3609534369574653
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.3607499864366319
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.3601383633083768
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.3594495985243056
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.3599565294053819
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.35969034830729163
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.359466552734375
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.3599853515625
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.36020872328016496
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.36007775200737846
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.36026340060763884
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.3593050638834635
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.3595161437988281
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.35931332906087243
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.35993385314941406
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.3596224784851074
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.3598594665527344
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.36002206802368164
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.3599720001220703
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.359954833984375
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.36131858825683594
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.3593025207519531
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.3599681854248047
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.361328125
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.3596038818359375
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.34375
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.35988871256510413
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.361328125
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.353515625
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.3604496547154018
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.3594730922154018
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.3604256766183036
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.3592049734933036
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.3603101457868304
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.35982949393136165
Final sparsity level of 0.36: 0.3599446458825114
Transfer numbers...
initialize by random pruning
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.3599329543005405
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.3606745500972762
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 0.35026041666666663
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.3603346082899306
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.36140102810329866
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.36014980740017366
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.3605634901258681
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.3600879245334201
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.35957929823133683
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.3609483506944444
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.36023457845052087
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.3601531982421875
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.3604804144965278
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.36003239949544275
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.35957209269205725
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.3599480523003472
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.36040072970920134
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.3595954047309028
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.3594495985243056
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.36039182874891496
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.35958523220486116
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.3587561713324653
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.3598090277777778
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.36012607150607634
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.3601311577690972
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.35959455702039933
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.3595648871527778
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.3602532280815972
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.35937160915798616
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.359161376953125
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.3603295220269097
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.36000612046983504
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.35979461669921875
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.36072285970052087
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.36018032497829866
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.3590681287977431
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.3604583740234375
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.35986455281575525
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.3597564697265625
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.3601904975043403
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.3593987358940972
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.3599175347222222
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.360565185546875
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.36027865939670134
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.35985098944769967
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.3597344292534722
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.3613213433159722
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.36127048068576384
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.3599226209852431
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.36008665296766496
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.35997941758897567
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.35976155598958337
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.36087544759114587
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.36005655924479163
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.35933430989583337
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.3599552578396268
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.3599722120496962
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.35946316189236116
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.3609161376953125
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.36023457845052087
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.36063639322916663
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.35985141330295134
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.3601120842827691
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.35986836751302087
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.36077541775173616
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.3606330023871528
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.3595123291015625
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.3597102695041232
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.3601642184787326
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.3605804443359375
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.36139933268229163
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.3604956732855903
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.36017354329427087
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.35967042711046004
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.3598615858289931
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.36023288302951384
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.3596458435058594
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.36035919189453125
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.35937563578287757
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.3595094680786133
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.36029672622680664
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.3601837158203125
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.3602108955383301
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.3594036102294922
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.3586692810058594
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.3603477478027344
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.3597068786621094
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.36021995544433594
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.3720703125
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.3597755432128906
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.36328125
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.35984293619791663
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.357421875
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.35286458333333337
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.3602752685546875
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.3595690046037946
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.3588344029017857
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.36110687255859375
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.35933903285435265
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.3596670968191964
Final sparsity level of 0.36: 0.35996084473732715
Transfer numbers...
initialize by random pruning
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.3599438082330316
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.36056815418287935
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 0.36458333333333337
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.36025661892361116
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.3600073920355903
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.3602142333984375
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.35924106174045134
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.3605762057834201
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.3597089979383681
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.35920206705729163
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.3604515923394097
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.35973103841145837
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.3580780029296875
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.36037911309136283
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.3597611321343316
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.35982767740885413
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.35952419704861116
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.3596343994140625
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.36084493001302087
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.3598365783691406
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.3600052727593316
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.35948689778645837
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.3602006700303819
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.3587561713324653
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.3602464463975694
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.35978825887044275
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.3600811428493924
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.35874430338541663
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.3590460883246528
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.3609619140625
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.3601752387152778
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.36032613118489587
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.3598005506727431
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.35944620768229163
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.36046854654947913
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.36035664876302087
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.36124165852864587
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.35983827379014754
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.36000357733832467
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.36069234212239587
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.3600599500868056
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.3598107231987847
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.3607872856987847
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.3595513237847222
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.3603426615397135
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.35951741536458337
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.3604414198133681
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.3598853217230903
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.35991414388020837
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.35985098944769967
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.35948053995768225
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.35937160915798616
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.3598175048828125
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.36032443576388884
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.35954793294270837
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.3598107231987847
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.3601417541503906
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.3599327935112847
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.3598175048828125
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.35992940266927087
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.35979037814670134
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.3598475986056857
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.35973358154296875
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.3597412109375
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.3613128662109375
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.3611161973741319
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.3606635199652778
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.3596348232693143
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.3597848680284288
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.36046346028645837
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.3599107530381944
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.36053805881076384
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.36024983723958337
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.35996076795789933
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.35918638441297746
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.3593071831597222
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.3599573771158854
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.3604450225830078
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.35999488830566406
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.35978031158447266
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.36005544662475586
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.359710693359375
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.3594365119934082
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.3602428436279297
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.3607215881347656
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.3588752746582031
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.3595466613769531
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.3585319519042969
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.3525390625
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.36074066162109375
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.384765625
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.359887440999349
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.3671875
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.37369791666666663
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.35874611990792415
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.3598153250558036
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.3608943394252232
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.36019352504185265
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.35969870431082585
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.3589804513113839
Final sparsity level of 0.36: 0.35991701091309924
Transfer numbers...
initialize by random pruning
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.35987179407473724
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.36283286721789887
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 0.36197916666666663
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.3597734239366319
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.36004638671875
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.3598090277777778
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.3595750596788194
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.3605321248372396
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.3597547743055556
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.3598700629340278
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.3606041802300347
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.3599412706163194
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.3590562608506944
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.36001502143012154
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.3602248297797309
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.36065165201822913
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.36075168185763884
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.3598717583550347
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.3600785997178819
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.3597217135959201
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.3602074517144097
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.3593224419487847
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.3594190809461806
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.3602311876085069
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.35905795627170134
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.3600120544433594
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.3600552876790365
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.3609771728515625
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.3612348768446181
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.36025492350260413
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.35976833767361116
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.35999933878580725
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.36015828450520837
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.35826619466145837
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.3601972791883681
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.3600921630859375
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.3600904676649306
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.35989083184136283
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.35994042290581596
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.3591139051649306
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.36044650607638884
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.35976664225260413
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.3587358262803819
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.3600938585069444
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.36008877224392366
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.36094156901041663
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.3603057861328125
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.36019558376736116
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.36104329427083337
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.3599891662597656
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.35994890001085067
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.3594682481553819
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.36040072970920134
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.3606635199652778
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.3602057562934028
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.35985607571072054
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.35969670613606775
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.35918002658420134
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.3589952256944444
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.35944620768229163
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.359832763671875
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.3602125379774306
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.36037445068359375
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.359649658203125
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.36092800564236116
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.36056857638888884
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.3603278266059028
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.36038335164388025
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.3605257670084635
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.3603040907118056
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.3605109320746528
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.3605363633897569
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.35956827799479163
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.3599217732747396
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.35979292127821183
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.35999043782552087
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.360321044921875
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.3595085144042969
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.35946528116861975
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.3592243194580078
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.3607497215270996
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.35924243927001953
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.3596792221069336
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.36046314239501953
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.3606910705566406
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.35958290100097656
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.36017608642578125
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.3605937957763672
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.357177734375
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.3603687286376953
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.361328125
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.36107381184895837
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.361328125
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.35481770833333337
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.3599439348493304
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.3598545619419643
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.36047690255301335
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.35988834926060265
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.36023058210100445
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.36005619594029015
Final sparsity level of 0.36: 0.3600012172677909
Transfer numbers...
initialize by random pruning
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.36004760705759475
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.35988671368352787
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 0.38932291666666663
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.36018880208333337
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.35941738552517366
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.35827128092447913
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.36018710666232634
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.3600090874565972
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.35988871256510413
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.36068216959635413
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.36113315158420134
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.35997348361545134
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.35994466145833337
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.3602714538574219
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.3597895304361979
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.3595038519965278
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.36078898111979163
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.35985141330295134
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.3603736029730903
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.3597242567274306
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.35990354749891496
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.359771728515625
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.35819668240017366
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.3612993028428819
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.3587510850694444
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.35976621839735246
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.3602748446994357
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.35952250162760413
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.3596174452039931
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.3600141737196181
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.3593512641059028
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.35938093397352433
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.35985098944769967
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.35992770724826384
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.3596937391493056
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.3596479627821181
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.35972086588541663
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.3601616753472222
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.35984887017144096
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.35970052083333337
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.3600599500868056
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.3594597710503472
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.35988362630208337
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.35996500651041663
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.35999976264105904
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.359832763671875
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.3586798773871528
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.3604515923394097
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.3611229790581597
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.3599815368652344
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.3599116007486979
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.35986667209201384
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.3612297905815972
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.36076863606770837
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.3600684271918403
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.35980945163302946
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.35969034830729163
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.35956319173177087
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.36004130045572913
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.3599785698784722
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.3595258924696181
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.3603926764594184
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.3599573771158854
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.3604566786024306
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.36069064670138884
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.36040920681423616
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.3592546251085069
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.36046812269422746
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.3600785997178819
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.3598768446180556
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.3587053087022569
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.3607398139105903
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.3599073621961806
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.36011505126953125
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.35989930894639754
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.36020914713541663
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.3605016072591146
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.3598365783691406
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.3604927062988281
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.36026763916015625
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.36016273498535156
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.3598489761352539
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.35998058319091797
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.3603324890136719
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.3604698181152344
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.3612785339355469
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.3618965148925781
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.36069679260253906
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.36767578125
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.360595703125
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.345703125
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.36002349853515625
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.34765625
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.36295572916666663
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.3607461111886161
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.3595341273716518
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.35978589739118305
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.3604234967912946
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.36038099016462055
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.3595908028738839
Final sparsity level of 0.36: 0.3600153047334106
wandb: WARNING W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.
2022-09-10 07:23:11,385 | INFO | main_after.py: 96 : main() ::	 Experiment Name: run_cv_asdiv-a
2022-09-10 07:23:11,385 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:23:11,385 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:23:11,747 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:23:11,747 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 979
2022-09-10 07:23:11,747 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 238
2022-09-10 07:23:11,747 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: []
2022-09-10 07:23:11,747 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:23:11,747 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:23:11,928 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 2882 / 2882 = 1.0
2022-09-10 07:23:11,958 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 2885 words in input language, 12 words in output
2022-09-10 07:23:11,966 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:23:15,315 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:23:15,316 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:23:18,841 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:23:18,841 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/asdiv/dense/models/run_cv_asdiv-a_fold0.
2022-09-10 07:23:21,278 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:23:30,142 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.1722689075630252	
 equation acc epoch: 0.09243697478991597	
 max val acc: 0.1722689075630252	
 equation acc: 0.09243697478991597	
2022-09-10 07:23:30,142 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 8s
2022-09-10 07:23:30,144 | INFO | main_after.py: 96 : main() ::	 Experiment Name: run_cv_asdiv-a
2022-09-10 07:23:30,144 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:23:30,145 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:23:30,438 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:23:30,438 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 979
2022-09-10 07:23:30,438 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 238
2022-09-10 07:23:30,438 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: []
2022-09-10 07:23:30,438 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:23:30,438 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:23:30,612 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 2882 / 2882 = 1.0
2022-09-10 07:23:30,642 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 2885 words in input language, 12 words in output
2022-09-10 07:23:30,650 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:23:33,623 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:23:33,623 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:23:33,705 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:23:33,706 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/asdiv/dense/models/run_cv_asdiv-a_fold1.
2022-09-10 07:23:36,349 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:23:44,174 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.18067226890756302	
 equation acc epoch: 0.029411764705882353	
 max val acc: 0.18067226890756302	
 equation acc: 0.029411764705882353	
2022-09-10 07:23:44,174 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 7s
2022-09-10 07:23:44,177 | INFO | main_after.py: 96 : main() ::	 Experiment Name: run_cv_asdiv-a
2022-09-10 07:23:44,177 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:23:44,177 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:23:44,465 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:23:44,465 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 979
2022-09-10 07:23:44,465 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 238
2022-09-10 07:23:44,465 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: []
2022-09-10 07:23:44,465 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:23:44,465 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:23:44,639 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 2882 / 2882 = 1.0
2022-09-10 07:23:44,671 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 2885 words in input language, 12 words in output
2022-09-10 07:23:44,679 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:23:47,602 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:23:47,602 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:23:47,683 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:23:47,683 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/asdiv/dense/models/run_cv_asdiv-a_fold2.
2022-09-10 07:23:51,690 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:23:58,872 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.12605042016806722	
 equation acc epoch: 0.12184873949579832	
 max val acc: 0.12605042016806722	
 equation acc: 0.12184873949579832	
2022-09-10 07:23:58,872 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 7s
2022-09-10 07:23:58,876 | INFO | main_after.py: 96 : main() ::	 Experiment Name: run_cv_asdiv-a
2022-09-10 07:23:58,876 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:23:58,876 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:23:59,166 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:23:59,166 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 980
2022-09-10 07:23:59,167 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 237
2022-09-10 07:23:59,167 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: []
2022-09-10 07:23:59,167 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:23:59,167 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:23:59,340 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 2882 / 2882 = 1.0
2022-09-10 07:23:59,371 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 2885 words in input language, 12 words in output
2022-09-10 07:23:59,379 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:24:02,248 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:24:02,248 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:24:02,328 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:24:02,328 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/asdiv/dense/models/run_cv_asdiv-a_fold3.
2022-09-10 07:24:06,229 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:24:14,040 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.1350210970464135	
 equation acc epoch: 0.12236286919831224	
 max val acc: 0.1350210970464135	
 equation acc: 0.12236286919831224	
2022-09-10 07:24:14,041 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 7s
2022-09-10 07:24:14,043 | INFO | main_after.py: 96 : main() ::	 Experiment Name: run_cv_asdiv-a
2022-09-10 07:24:14,043 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:24:14,043 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:24:14,333 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:24:14,333 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 951
2022-09-10 07:24:14,333 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 266
2022-09-10 07:24:14,333 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: []
2022-09-10 07:24:14,333 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:24:14,333 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:24:14,510 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 2882 / 2882 = 1.0
2022-09-10 07:24:14,539 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 2885 words in input language, 12 words in output
2022-09-10 07:24:14,549 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:24:17,447 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:24:17,447 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:24:17,529 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:24:17,529 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/asdiv/dense/models/run_cv_asdiv-a_fold4.
2022-09-10 07:24:21,070 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:24:28,936 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.041353383458646614	
 equation acc epoch: 0.03007518796992481	
 max val acc: 0.041353383458646614	
 equation acc: 0.03007518796992481	
2022-09-10 07:24:28,936 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 7s
2022-09-10 07:24:28,936 | INFO | main_after.py: 272 : main() ::	 Final Val score: 0.12900575184880855
Transfer numbers...
initialize by random pruning
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.4879076312211943
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.49002918287937747
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 0.5091145833333333
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.4876776801215278
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.4873809814453125
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.4880523681640625
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.4880693223741319
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.48818037245008683
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.48810831705729163
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.4873792860243056
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.487884521484375
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.48813883463541663
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.4888136121961806
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.48751746283637154
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.48789003160264754
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.4881913926866319
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.4875861273871528
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.4873877631293403
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.4882490370008681
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.4878866407606337
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.4879807366265191
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.48718600802951384
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.48798116048177087
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.48878139919704866
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.4872504340277778
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.48806889851888025
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.48765267266167533
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.48834228515625
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.48736572265625
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.48815578884548616
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.48772854275173616
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.48761325412326384
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.4879413180881076
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.4883795844184028
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.4890984429253472
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.48844401041666663
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.48824564615885413
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.4878273010253906
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.4878929985894097
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.4884779188368056
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.48948330349392366
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.48746066623263884
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.4879675971137153
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.48798624674479163
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.48770692613389754
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.4878929985894097
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.4868537055121528
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.48759799533420134
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.48822360568576384
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.48823886447482634
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.4880633884006076
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.48793538411458337
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.4879014756944444
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.4876386854383681
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.48793368869357634
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.48822953965928817
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.488043467203776
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.4878692626953125
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.48762003580729163
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.4881727430555556
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.48811848958333337
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.48780949910481775
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.4876488579644097
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.4896104600694444
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.48795064290364587
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.4883490668402778
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.4888763427734375
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.48819986979166663
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.4875267876519097
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.4879286024305556
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.4882354736328125
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.48860507541232634
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.48811001247829866
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.4881329006618924
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.48787646823459196
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.48783365885416663
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.48753039042154944
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.48748207092285156
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.4870948791503906
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.4876689910888672
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.4877471923828125
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.48827075958251953
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.48802900314331055
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.4885826110839844
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.4873008728027344
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.48835182189941406
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.4879264831542969
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.48793601989746094
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.488037109375
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.487823486328125
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.4765625
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.48800150553385413
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.486328125
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.48600260416666663
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.48814501081194195
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.48785727364676335
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.4888065883091518
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.4875401088169643
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.4882921491350446
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.4880349295479911
Final sparsity level of 0.488: 0.4879435800255548
Transfer numbers...
initialize by random pruning
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.48795400004973644
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.4897125283722439
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 0.49479166666666663
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.4883185492621528
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.4885847303602431
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.4882337782118056
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.488677978515625
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.48812357584635413
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.48736657036675346
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.48927646213107634
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.4882744683159722
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.48781161838107634
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.4878913031684028
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.48819478352864587
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.487677256266276
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.4872504340277778
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.48942057291666663
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.48739284939236116
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.48747762044270837
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.4885694715711806
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.4878497653537326
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.48634847005208337
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.4876878526475694
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.4885033501519097
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.4881880018446181
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.48789003160264754
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.4877497355143229
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.48819478352864587
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.4877031114366319
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.4868537055121528
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.487945556640625
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.4878103468153212
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.4877433776855469
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.4888780381944444
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.4885932074652778
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.4875471327039931
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.48797946506076384
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.4878209431966146
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.4880943298339844
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.4876319037543403
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.48790995279947913
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.4878624810112847
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.48858303493923616
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.48829311794704866
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.4876840379503038
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.4873487684461806
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.4895002577039931
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.48846435546875
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.4877031114366319
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.4883384704589844
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.48760647243923616
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.4872300889756944
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.4879913330078125
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.488037109375
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.48760986328125
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.4880498250325521
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.48813459608289933
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.48725382486979163
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.48755900065104163
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.48865254720052087
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.48901875813802087
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.48759799533420134
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.48813162909613717
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.48763699001736116
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.48863559299045134
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.48833211263020837
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.4868299696180556
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.4876234266493056
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.48795572916666663
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.4882269965277778
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.489105224609375
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.487884521484375
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.48792521158854163
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.48760435316297746
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.48789469401041663
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.48769124348958337
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.4876486460367838
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.4887876510620117
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.48773320515950525
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.4875469207763672
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.48832082748413086
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.48827457427978516
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.4880809783935547
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.4881000518798828
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.4871482849121094
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.4883308410644531
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.4878387451171875
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.48769569396972656
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.50146484375
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.4881134033203125
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.4921875
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.48837407430013025
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.501953125
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.48828125
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.4879869733537946
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.48754555838448665
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.4867946079799107
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.4891183035714286
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.48658207484654015
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.4874856131417411
Final sparsity level of 0.488: 0.48795294628648034
Transfer numbers...
initialize by random pruning
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.4880037105424583
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.48880562986381326
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 0.48828125
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.4881218804253472
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.48781840006510413
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.48766750759548616
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.48697916666666663
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.48829099867078996
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.48787646823459196
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.48691813151041663
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.48796420627170134
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.48825581868489587
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.4862433539496528
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.48811679416232634
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.4879430135091146
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.48862711588541663
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.4873182508680556
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.48795572916666663
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.4888322618272569
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.4876725938585069
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.4880087110731337
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.487152099609375
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.4885949028862847
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.48640780978732634
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.48828633626302087
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.4879328409830729
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.48784891764322913
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.48721652560763884
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.4864281548394097
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.48873562282986116
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.48846096462673616
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.48857031928168404
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.48774210611979163
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.4881439208984375
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.48752339680989587
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.4879777696397569
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.4889984130859375
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.48786841498480904
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.48796166314019096
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.48788791232638884
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.4879608154296875
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.4880761040581597
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.4886322021484375
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.48746532864040804
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.487969716389974
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.4871283637152778
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.4885169135199653
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.48795742458767366
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.4882439507378472
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.4875615437825521
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.48768022325303817
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.4874945746527778
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.4879777696397569
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.4887068006727431
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.48795742458767366
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.48759757147894967
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.48837407430013025
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.487640380859375
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.48850674099392366
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.4879218207465278
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.4884813096788194
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.4877599080403646
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.4879586961534288
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.4871673583984375
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.48904758029513884
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.48928324381510413
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.4885932074652778
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.48806465996636283
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.4878887600368924
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.4886491563585069
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.48805745442708337
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.4882524278428819
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.4889051649305556
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.4874110751681857
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.4873703850640191
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.4868859185112847
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.4881064097086588
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.4889240264892578
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.4880174001057943
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.48748302459716797
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.4881720542907715
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.48799800872802734
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.48747920989990234
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.4881744384765625
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.488800048828125
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.4862384796142578
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.48839569091796875
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.4865837097167969
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.489990234375
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.4880714416503906
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.51953125
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.4869511922200521
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.521484375
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.49544270833333337
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.4874354771205357
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.4880589076450893
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.4884229387555804
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.48834773472377235
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.48800768171037945
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.48747144426618305
Final sparsity level of 0.488: 0.4879317493234694
Transfer numbers...
initialize by random pruning
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.48785382783746145
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.4911615393968871
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 0.49088541666666663
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.48764546712239587
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.488677978515625
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.4881659613715278
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.4883338080512153
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.4881375630696615
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.48793580796983504
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.48817782931857634
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.48814053005642366
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.48768107096354163
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.4873063829210069
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.48841349283854163
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.48825836181640625
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.4883897569444444
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.4885711669921875
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.48775736490885413
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.4882032606336806
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.48786205715603304
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.48795530531141496
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.48738437228732634
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.48750813802083337
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.48817952473958337
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.4870079888237847
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.4881261189778646
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.4882685343424479
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.48891025119357634
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.4887610541449653
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.48851352267795134
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.4872148301866319
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.4883295694986979
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.4880744086371528
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.48627726236979163
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.48798794216579866
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.48793368869357634
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.4883253309461806
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.4881638420952691
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.4875250922309028
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.48639255099826384
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.4892900254991319
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.4874182807074653
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.4875098334418403
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.4878777398003472
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.4883138868543837
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.48835415310329866
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.4880744086371528
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.4873131646050347
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.4896070692274306
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.4879417419433594
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.48808161417643225
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.4883711073133681
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.4886644151475694
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.4885338677300347
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.48723178439670134
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.4876814948187934
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.48810577392578125
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.48729451497395837
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.4875098334418403
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.4873216417100694
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.48751322428385413
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.48847622341579866
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.48865254720052087
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.48798116048177087
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.4881371392144097
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.48896450466579866
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.4877336290147569
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.4884270562065972
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.4886822170681424
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.4872351752387153
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.48846774631076384
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.48906962076822913
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.4879082573784722
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.48827997843424475
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.4878383212619357
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.4881913926866319
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.48791821797688806
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.4873619079589844
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.4874960581461588
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.48810482025146484
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.48878908157348633
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.4872713088989258
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.48769521713256836
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.4882221221923828
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.4878273010253906
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.4882698059082031
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.4885215759277344
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.4887962341308594
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.482421875
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.4876117706298828
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.5078125
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.4885342915852865
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.482421875
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.4814453125
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.4881831577845982
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.4881395612444196
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.4880458286830357
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.48783765520368305
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.4887215750558036
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.48826708112444195
Final sparsity level of 0.488: 0.48802821558067333
Transfer numbers...
initialize by random pruning
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.48802101466394776
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.48881829604409854
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 0.49609375
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.4879675971137153
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.4874335394965278
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.4858940972222222
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.48844570583767366
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.4878646002875434
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.4880761040581597
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.4880303276909722
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.488372802734375
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.48792012532552087
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.488433837890625
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.48814943101671004
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.4879722595214844
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.4878692626953125
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.4885101318359375
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.48779635959201384
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.48823208279079866
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.48770480685763884
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.4882269965277778
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.48732503255208337
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.4870215521918403
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.4887830946180556
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.4873199462890625
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.4874852498372396
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.48816045125325525
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.4871537950303819
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.48793368869357634
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.48765903049045134
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.48772684733072913
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.4877925448947482
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.4874725341796875
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.4879218207465278
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.4878777398003472
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.4874793158637153
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.4879302978515625
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.4881019592285156
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.4874547322591146
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.4877031114366319
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.48702663845486116
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.4876946343315972
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.4874487982855903
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.4879459804958768
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.4880082872178819
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.4878082275390625
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.48732333713107634
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.4887101915147569
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.4892255995008681
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.48792902628580725
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.4881150987413194
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.48786587185329866
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.48827107747395837
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.48903910319010413
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.4870317247178819
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.487982432047526
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.48784425523546004
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.4878319634331597
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.488067626953125
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.48788791232638884
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.48722330729166663
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.48818037245008683
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.4873767428927951
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.4889611138237847
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.4888085259331597
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.4882117377387153
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.48727077907986116
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.48827192518446183
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.48779381646050346
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.4888916015625
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.48694186740451384
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.48836093478732634
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.48742336697048616
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.4876903957790799
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.4879909091525607
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.48707241482204866
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.4882488250732422
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.48740100860595703
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.4885126749674479
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.48798465728759766
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.487978458404541
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.48763084411621094
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.48831748962402344
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.48851871490478516
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.4882240295410156
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.4890003204345703
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.4899635314941406
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.48932838439941406
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.490478515625
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.48925018310546875
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.4609375
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.48798624674479163
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.490234375
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.4892578125
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.48883601597377235
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.48763493129185265
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.4877057756696429
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.4878278459821429
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.48848724365234375
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.48818206787109375
Final sparsity level of 0.488: 0.48797092424629085
wandb: WARNING W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.
2022-09-10 07:24:32,236 | INFO | main_after.py: 96 : main() ::	 Experiment Name: run_cv_asdiv-a
2022-09-10 07:24:32,236 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:24:32,236 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:24:32,610 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:24:32,610 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 979
2022-09-10 07:24:32,610 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 238
2022-09-10 07:24:32,610 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: []
2022-09-10 07:24:32,610 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:24:32,610 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:24:32,789 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 2882 / 2882 = 1.0
2022-09-10 07:24:32,819 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 2885 words in input language, 12 words in output
2022-09-10 07:24:32,827 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:24:36,027 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:24:36,027 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:24:39,454 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:24:39,454 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/asdiv/dense/models/run_cv_asdiv-a_fold0.
2022-09-10 07:24:43,915 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:24:54,255 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.08823529411764706	
 equation acc epoch: 0.06302521008403361	
 max val acc: 0.08823529411764706	
 equation acc: 0.06302521008403361	
2022-09-10 07:24:54,255 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 10s
2022-09-10 07:24:54,257 | INFO | main_after.py: 96 : main() ::	 Experiment Name: run_cv_asdiv-a
2022-09-10 07:24:54,257 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:24:54,257 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:24:54,551 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:24:54,552 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 979
2022-09-10 07:24:54,552 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 238
2022-09-10 07:24:54,552 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: []
2022-09-10 07:24:54,552 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:24:54,552 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:24:54,725 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 2882 / 2882 = 1.0
2022-09-10 07:24:54,756 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 2885 words in input language, 12 words in output
2022-09-10 07:24:54,763 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:24:57,705 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:24:57,705 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:24:57,785 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:24:57,785 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/asdiv/dense/models/run_cv_asdiv-a_fold1.
2022-09-10 07:24:59,985 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:25:07,269 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.19327731092436976	
 equation acc epoch: 0.02100840336134454	
 max val acc: 0.19327731092436976	
 equation acc: 0.02100840336134454	
2022-09-10 07:25:07,269 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 7s
2022-09-10 07:25:07,271 | INFO | main_after.py: 96 : main() ::	 Experiment Name: run_cv_asdiv-a
2022-09-10 07:25:07,271 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:25:07,271 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:25:07,560 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:25:07,560 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 979
2022-09-10 07:25:07,561 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 238
2022-09-10 07:25:07,561 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: []
2022-09-10 07:25:07,561 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:25:07,561 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:25:07,733 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 2882 / 2882 = 1.0
2022-09-10 07:25:07,853 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 2885 words in input language, 12 words in output
2022-09-10 07:25:07,861 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:25:10,713 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:25:10,713 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:25:10,792 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:25:10,792 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/asdiv/dense/models/run_cv_asdiv-a_fold2.
2022-09-10 07:25:12,722 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:25:18,995 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.008403361344537815	
 equation acc epoch: 0.0	
 max val acc: 0.008403361344537815	
 equation acc: 0.0	
2022-09-10 07:25:18,995 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 6s
2022-09-10 07:25:18,998 | INFO | main_after.py: 96 : main() ::	 Experiment Name: run_cv_asdiv-a
2022-09-10 07:25:18,998 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:25:18,998 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:25:19,291 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:25:19,292 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 980
2022-09-10 07:25:19,292 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 237
2022-09-10 07:25:19,292 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: []
2022-09-10 07:25:19,292 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:25:19,292 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:25:19,466 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 2882 / 2882 = 1.0
2022-09-10 07:25:19,497 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 2885 words in input language, 12 words in output
2022-09-10 07:25:19,505 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:25:22,367 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:25:22,367 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:25:22,445 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:25:22,445 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/asdiv/dense/models/run_cv_asdiv-a_fold3.
2022-09-10 07:25:25,744 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:25:33,523 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.08016877637130802	
 equation acc epoch: 0.0759493670886076	
 max val acc: 0.08016877637130802	
 equation acc: 0.0759493670886076	
2022-09-10 07:25:33,523 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 7s
2022-09-10 07:25:33,526 | INFO | main_after.py: 96 : main() ::	 Experiment Name: run_cv_asdiv-a
2022-09-10 07:25:33,526 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:25:33,526 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:25:33,817 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:25:33,817 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 951
2022-09-10 07:25:33,817 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 266
2022-09-10 07:25:33,818 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: []
2022-09-10 07:25:33,818 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:25:33,818 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:25:33,992 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 2882 / 2882 = 1.0
2022-09-10 07:25:34,023 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 2885 words in input language, 12 words in output
2022-09-10 07:25:34,031 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:25:36,899 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:25:36,899 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:25:36,986 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:25:36,986 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/asdiv/dense/models/run_cv_asdiv-a_fold4.
2022-09-10 07:25:41,834 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:25:48,974 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.007518796992481203	
 equation acc epoch: 0.0	
 max val acc: 0.007518796992481203	
 equation acc: 0.0	
2022-09-10 07:25:48,975 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 7s
2022-09-10 07:25:48,975 | INFO | main_after.py: 272 : main() ::	 Final Val score: 0.07395234182415776
Transfer numbers...
initialize by random pruning
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.5898991335505156
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.5918374067769131
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 0.6067708333333333
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5903388129340278
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5897420247395833
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5900285508897569
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5901353624131944
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5900561014811199
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5901930067274306
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5898657904730903
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5896759033203125
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5906321207682292
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5908610026041667
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5897967020670574
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.589886983235677
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5905863444010417
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5905897352430556
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5900234646267362
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5910000271267362
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5897950066460503
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5899997287326388
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5903167724609375
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.58953857421875
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5910441080729167
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5897420247395833
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5904159545898438
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5898119608561199
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5893758138020833
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5894741482204862
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5899030897352431
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5895182291666667
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5895322163899739
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5896343655056424
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5908135308159722
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5907830132378472
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5902269151475694
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5909339057074653
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5900285508897569
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5899357265896268
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5901150173611112
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5905880398220487
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5897013346354167
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5897505018446181
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.589917500813802
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5896555582682292
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5897674560546875
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5893385145399306
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5899522569444444
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5898030598958333
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5901052686903212
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5901209513346355
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5898861355251737
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5898098415798612
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5902099609375
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5897301567925347
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5902578565809462
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5903566148546007
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5899590386284722
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5895809597439237
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5902014838324653
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5895792643229167
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.589864518907335
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.590075174967448
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5905168321397569
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5899522569444444
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5905083550347222
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5906880696614583
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5903197394476997
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5898382398817275
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5903235541449653
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5899878607855903
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5910864935980903
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.590484619140625
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5899314880371094
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5896809895833333
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.5898573133680556
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.5898056030273438
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.5897560119628906
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.5891176859537761
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.5894145965576172
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.5899615287780762
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.590144157409668
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.590031623840332
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.5900516510009766
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.5893821716308594
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.5903282165527344
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.5896797180175781
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.5898418426513672
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.58984375
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.5903053283691406
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.57421875
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.5898806254069011
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.61328125
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.5862630208333333
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.5897511073521206
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.590710231236049
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.5902949741908482
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.5893707275390625
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.590160914829799
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.5896922520228794
Final sparsity level of 0.59: 0.5899746768979188
Transfer numbers...
initialize by random pruning
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.5900388617411054
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.5915714169909209
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 0.5963541666666667
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5898607042100694
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5907914903428819
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5895724826388888
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5906032986111112
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.58990478515625
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5896335177951388
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5908728705512153
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5899929470486112
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5901370578342013
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.590118408203125
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5900849236382378
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5898759629991319
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5898980034722222
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5910373263888888
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5891503228081597
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5895741780598958
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.590413835313585
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5896144443088107
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5889672173394097
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5901201036241319
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5905558268229167
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5898980034722222
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.589752197265625
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5897979736328125
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5905219184027778
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5895640055338542
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5895741780598958
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.589569091796875
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5902676052517362
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5898573133680556
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5905982123480903
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5902116563585069
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5892588297526042
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5897403293185763
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.589577145046658
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5900408426920574
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5897115071614583
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5898047553168403
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5900387234157987
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5903472900390625
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5899980333116319
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5903438991970487
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5898352728949653
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5919731987847222
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5901760525173612
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5897928873697917
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.59039306640625
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5896470811631944
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5896030002170138
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5900760226779513
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5902828640407987
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5898725721571181
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5901128980848525
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5900599161783855
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5893147786458333
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5889434814453125
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5900929768880208
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5906405978732638
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5898467169867622
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5900353325737847
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5895775689019097
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5906914605034722
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5906083848741319
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5881381564670138
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5895462036132812
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5899183485243056
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5903201633029513
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5902726915147569
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5896759033203125
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5894673665364583
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5896915859646268
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5898679097493489
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.5892503526475694
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.5897916158040364
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.5905246734619141
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.5899620056152344
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.5897674560546875
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.5902833938598633
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.5902280807495117
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.5901336669921875
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.5895957946777344
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.5890464782714844
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.5898246765136719
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.59088134765625
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.5899982452392578
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.597412109375
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.5904006958007812
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.583984375
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.5903879801432292
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.58984375
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.5885416666666667
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.5900224958147322
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.5891374860491072
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.5890208653041294
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.590832301548549
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.5895233154296875
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.5903560093470982
Final sparsity level of 0.59: 0.5899814541111088
Transfer numbers...
initialize by random pruning
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.5899918453032262
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.5909431744487679
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 0.58984375
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5897928873697917
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5896521674262153
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.589630126953125
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5887705485026042
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.590218014187283
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5899899800618489
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5892401801215278
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5900743272569444
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5901167127821181
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5887637668185763
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5898128085666232
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.590193854437934
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5907151963975694
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5888214111328125
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5903065999348958
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5911848280164931
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5898174709743924
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.589935302734375
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5888756646050347
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5905626085069444
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5882398817274306
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5910508897569444
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.590004391140408
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5898077223036025
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5885908338758681
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5883873833550347
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5906897650824653
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5900828043619792
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5905206468370225
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5895470513237847
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5899556477864583
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5898284912109375
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5901319715711806
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5911356608072917
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.589920891655816
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5898929172092013
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5899454752604167
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5899776882595487
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5899081759982638
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5907118055555556
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5895890129937066
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5899548000759549
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5891299777560763
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5899488661024306
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5896284315321181
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5909423828125
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5900391472710503
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5895902845594618
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5895351833767362
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5900099012586806
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5909474690755208
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5899369981553819
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5895962185329862
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5902930365668403
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5894012451171875
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5904880099826388
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5897691514756944
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5904930962456597
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5899518330891926
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5899382697211372
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5890706380208333
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5914340549045138
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.59130859375
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5904998779296875
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5901374816894531
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5897644890679253
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5906592475043403
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.59039306640625
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5909356011284722
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5903642442491319
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5900175306532118
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5895038180881076
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.5894419352213542
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.5900344848632812
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.5902137756347656
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.5902182261149089
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.5891199111938477
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.589838981628418
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.5903091430664062
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.5895829200744629
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.5904397964477539
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.5895538330078125
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.5885829925537109
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.59002685546875
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.5886631011962891
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.589599609375
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.5900669097900391
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.640625
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.5894762674967449
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.60546875
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.5947265625
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.589827401297433
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.589735848563058
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.5902077811104911
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.5903363909040178
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.5900519234793526
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.589674813406808
Final sparsity level of 0.59: 0.5899470834640318
Transfer numbers...
initialize by random pruning
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.5898902483504095
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.5926252431906615
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 0.6080729166666667
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5896589491102431
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.59027099609375
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5898200141059028
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5899878607855903
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5899628533257378
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5901658799913194
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5902370876736112
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5901709662543403
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5899692111545138
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5893469916449653
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5904079013400607
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5901989407009549
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5901743570963542
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5906490749782987
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5894487169053819
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5903998480902778
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5898492601182725
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5895169576009114
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5900912814670138
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5898488362630208
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5904083251953125
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5892300075954862
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5899798075358074
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5901552836100261
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5909779866536458
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5903438991970487
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5904286702473958
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5896097819010417
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5901764763726128
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5902048746744792
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5887603759765625
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5901099310980903
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5900166829427083
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5906016031901042
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5902235243055556
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5897632175021701
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5889078776041667
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.591156005859375
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5899217393663194
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5899997287326388
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5900815327962239
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5900908576117622
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5896233452690972
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5905490451388888
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5894775390625
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5913679334852431
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5899043613009982
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.590047624376085
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5909339057074653
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5901217990451388
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5906117757161458
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5899217393663194
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5894190470377605
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5900192260742188
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5903744167751737
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5895657009548612
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5892757839626737
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5891655815972222
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5904032389322917
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5906889173719618
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5897233751085069
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5902726915147569
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5902794731987847
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5897640652126737
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5905732048882378
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5905350579155816
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5893198649088542
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5906812879774306
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5908932156032987
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5907728407118056
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5902137756347656
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5900891621907551
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.5899793836805556
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.5901743570963542
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.5894975662231445
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.5896326700846355
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.5900115966796875
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.5901141166687012
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.5898094177246094
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.5899720191955566
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.5903911590576172
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.589385986328125
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.5902118682861328
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.5903739929199219
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.590972900390625
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.580078125
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.5897846221923828
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.595703125
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.5904769897460938
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.5859375
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.5826822916666667
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.5899407523018974
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.5901816231863839
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.589733668736049
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.5895189557756697
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.5903494698660714
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.5901412963867188
Final sparsity level of 0.59: 0.5900361563977556
Transfer numbers...
initialize by random pruning
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.5899340267416029
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.5903807960440985
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 0.5703125
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5903557671440972
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5894063313802083
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5875278049045138
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5903235541449653
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.589943355984158
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5901785956488715
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5904659695095487
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5904964870876737
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5901879204644097
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5904218885633681
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5899704827202691
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5901239183213975
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5899234347873263
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5907321506076388
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5897606743706597
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5899980333116319
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5896305508083768
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5899912516276042
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5894927978515625
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5893385145399306
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5906456841362847
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5892825656467013
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5894796583387587
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5900548299153645
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5893724229600694
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5903727213541667
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.58935546875
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5902693006727431
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5899526807996962
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5896051194932725
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5895741780598958
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5894911024305556
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5892435709635417
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5898963080512153
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5898450215657551
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5892274644639757
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5894656711154513
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.588897705078125
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5898030598958333
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5897301567925347
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5897865295410156
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5896224975585938
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5896487765842013
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5896369086371528
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5909220377604167
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5909525553385417
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5899336073133681
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.590047624376085
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5898623996310763
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5899692111545138
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5907847086588542
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5893656412760417
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5899959140353732
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5900929768880208
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5901574028862847
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5906558566623263
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5902642144097222
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5898793538411458
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5903600056966145
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.589583502875434
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5904880099826388
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5903116861979167
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5907474093967013
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5891927083333333
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.590193854437934
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5895508660210503
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5903540717230903
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5889366997612847
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5910763210720487
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5892469618055556
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5899895562065972
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5896339416503906
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.5894792344835069
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.5905927022298176
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.5894088745117188
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.5903822580973308
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.5902366638183594
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.5899300575256348
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.5896329879760742
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.5901598930358887
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.5902023315429688
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.5900611877441406
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.5908317565917969
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.5916824340820312
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.5916996002197266
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.596435546875
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.5909976959228516
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.564453125
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.5898081461588542
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.603515625
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.5882161458333333
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.5902154105050224
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.5901238577706474
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.5899407523018974
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.589489528111049
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.5903309413364956
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.5903102329799107
Final sparsity level of 0.59: 0.5899394340048132
wandb: WARNING W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.
2022-09-10 07:25:52,144 | INFO | main_after.py: 96 : main() ::	 Experiment Name: run_cv_asdiv-a
2022-09-10 07:25:52,144 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:25:52,144 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:25:52,514 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:25:52,514 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 979
2022-09-10 07:25:52,514 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 238
2022-09-10 07:25:52,514 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: []
2022-09-10 07:25:52,514 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:25:52,514 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:25:52,695 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 2882 / 2882 = 1.0
2022-09-10 07:25:52,725 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 2885 words in input language, 12 words in output
2022-09-10 07:25:52,733 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:25:56,149 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:25:56,149 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:25:59,585 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:25:59,586 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/asdiv/dense/models/run_cv_asdiv-a_fold0.
2022-09-10 07:26:03,695 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:26:13,531 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.06302521008403361	
 equation acc epoch: 0.01680672268907563	
 max val acc: 0.06302521008403361	
 equation acc: 0.01680672268907563	
2022-09-10 07:26:13,531 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 9s
2022-09-10 07:26:13,534 | INFO | main_after.py: 96 : main() ::	 Experiment Name: run_cv_asdiv-a
2022-09-10 07:26:13,534 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:26:13,534 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:26:13,822 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:26:13,823 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 979
2022-09-10 07:26:13,823 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 238
2022-09-10 07:26:13,823 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: []
2022-09-10 07:26:13,823 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:26:13,823 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:26:13,998 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 2882 / 2882 = 1.0
2022-09-10 07:26:14,029 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 2885 words in input language, 12 words in output
2022-09-10 07:26:14,037 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:26:17,004 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:26:17,004 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:26:17,082 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:26:17,082 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/asdiv/dense/models/run_cv_asdiv-a_fold1.
2022-09-10 07:26:21,259 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:26:27,783 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.01680672268907563	
 equation acc epoch: 0.0	
 max val acc: 0.01680672268907563	
 equation acc: 0.0	
2022-09-10 07:26:27,784 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 6s
2022-09-10 07:26:27,786 | INFO | main_after.py: 96 : main() ::	 Experiment Name: run_cv_asdiv-a
2022-09-10 07:26:27,786 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:26:27,786 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:26:28,073 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:26:28,073 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 979
2022-09-10 07:26:28,073 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 238
2022-09-10 07:26:28,073 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: []
2022-09-10 07:26:28,073 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:26:28,073 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:26:28,247 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 2882 / 2882 = 1.0
2022-09-10 07:26:28,362 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 2885 words in input language, 12 words in output
2022-09-10 07:26:28,370 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:26:31,229 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:26:31,229 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:26:31,317 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:26:31,317 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/asdiv/dense/models/run_cv_asdiv-a_fold2.
2022-09-10 07:26:33,382 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:26:39,372 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.008403361344537815	
 equation acc epoch: 0.0	
 max val acc: 0.008403361344537815	
 equation acc: 0.0	
2022-09-10 07:26:39,373 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 5s
2022-09-10 07:26:39,376 | INFO | main_after.py: 96 : main() ::	 Experiment Name: run_cv_asdiv-a
2022-09-10 07:26:39,376 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:26:39,376 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:26:39,664 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:26:39,664 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 980
2022-09-10 07:26:39,664 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 237
2022-09-10 07:26:39,664 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: []
2022-09-10 07:26:39,664 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:26:39,664 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:26:39,839 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 2882 / 2882 = 1.0
2022-09-10 07:26:39,872 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 2885 words in input language, 12 words in output
2022-09-10 07:26:39,880 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:26:42,710 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:26:42,711 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:26:42,786 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:26:42,786 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/asdiv/dense/models/run_cv_asdiv-a_fold3.
2022-09-10 07:26:44,718 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:26:53,217 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.016877637130801686	
 equation acc epoch: 0.012658227848101266	
 max val acc: 0.016877637130801686	
 equation acc: 0.012658227848101266	
2022-09-10 07:26:53,218 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 8s
2022-09-10 07:26:53,220 | INFO | main_after.py: 96 : main() ::	 Experiment Name: run_cv_asdiv-a
2022-09-10 07:26:53,220 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:26:53,220 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:26:53,509 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:26:53,509 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 951
2022-09-10 07:26:53,509 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 266
2022-09-10 07:26:53,509 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: []
2022-09-10 07:26:53,509 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:26:53,509 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:26:53,685 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 2882 / 2882 = 1.0
2022-09-10 07:26:53,716 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 2885 words in input language, 12 words in output
2022-09-10 07:26:53,724 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:26:56,652 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:26:56,653 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:26:56,736 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:26:56,736 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/asdiv/dense/models/run_cv_asdiv-a_fold4.
2022-09-10 07:26:58,432 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:27:04,828 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.007518796992481203	
 equation acc epoch: 0.0	
 max val acc: 0.007518796992481203	
 equation acc: 0.0	
2022-09-10 07:27:04,828 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 6s
2022-09-10 07:27:04,828 | INFO | main_after.py: 272 : main() ::	 Final Val score: 0.02218570254724733
Transfer numbers...
initialize by random pruning
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.6719264978944925
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.6733544098573281
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 0.7057291666666667
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6723175048828125
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6720937093098958
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6722412109375
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6721310085720487
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6721653408474393
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6722581651475694
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6719343397352431
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6716054280598958
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6724616156684028
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6728736029730903
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6717198689778645
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6717542012532551
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.672271728515625
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6726057264539931
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6719648573133681
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6727261013454862
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6717537773980035
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6722865634494357
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.67266845703125
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6715952555338542
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6723192003038194
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6717936197916667
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6722246805826824
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6717864142523872
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6713087293836806
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6715003119574653
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6717902289496528
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6712459988064237
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6718372768825955
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6717732747395833
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6728837754991319
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6728108723958333
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6718037923177083
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6733144124348958
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6719563802083333
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6717211405436199
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6719868977864583
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6723361545138888
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6715443929036458
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6714867485894097
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6721873813205295
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6717529296875
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6716986762152778
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6720411512586806
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6726091172960069
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6722903781467013
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6720216539171007
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6718902587890625
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6722835964626737
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6713748508029513
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6722191704644097
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6715053982204862
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6721365186903212
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6721988254123263
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6722903781467013
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6718936496310763
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6718393961588542
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6714901394314237
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6718182033962674
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6721017625596788
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6727244059244792
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6723853217230903
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6724294026692708
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6726345486111112
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6724459330240886
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6722420586480035
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6718377007378472
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6716122097439237
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6730906168619792
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6714087592230903
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6722081502278645
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6719813876681857
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.6722174750434028
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.6718406677246094
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.6716976165771484
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.6714083353678386
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.6712465286254883
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.6720490455627441
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.6720657348632812
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.671870231628418
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.6724119186401367
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.6710357666015625
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.6719341278076172
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.6719779968261719
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.6720790863037109
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.68017578125
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.6728343963623047
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.65234375
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.6713879903157551
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.7265625
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.6682942708333333
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.6714739118303572
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.6728788103376115
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.6722324916294643
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.6709540230887276
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.6728254045758928
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.6715382167271206
Final sparsity level of 0.672: 0.6719943592541593
Transfer numbers...
initialize by random pruning
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.6720508129828575
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.6736203996433203
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 0.6692708333333333
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6720140245225694
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6727532280815972
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6716901991102431
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6725650363498263
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6719610426161025
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6715715196397569
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6727735731336806
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6713748508029513
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6727549235026042
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6718071831597222
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6721233791775174
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.671976301405165
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6717664930555556
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6728396945529513
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6715070936414931
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6721089680989583
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6722946166992188
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6718728807237413
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6708696153428819
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6722174750434028
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6727803548177083
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6714884440104167
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6719390021430122
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6722178988986545
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6724277072482638
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6722683376736112
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6720445421006944
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6715613471137153
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6723043653700087
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6721458435058594
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6719953748914931
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6724514431423612
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.671875
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6720750596788194
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6718262566460503
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6721132066514757
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6720106336805556
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6728091769748263
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6718817816840278
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6725226508246528
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6720288594563801
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6721305847167969
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6714121500651042
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6734059651692708
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6724938286675347
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6712154812282987
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6722102695041232
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.671892801920573
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6709730360243056
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6725548638237847
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6722937689887153
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6721988254123263
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6718728807237413
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.672040303548177
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6715884738498263
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6711680094401042
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6723293728298612
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6727057562934028
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6719139946831597
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6719173855251737
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6720004611545138
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6723107231987847
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6723141140407987
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6706339518229167
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6720462375217013
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6721195644802518
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6729651557074653
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6721852620442708
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6716851128472222
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6720937093098958
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6716274685329862
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6720822652180989
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.6712188720703125
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.6721115112304688
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.6723260879516602
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.6716702779134114
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.6715154647827148
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.6723175048828125
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.6722517013549805
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.6719894409179688
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.6715974807739258
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.6722602844238281
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.6717910766601562
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.6727638244628906
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.6716098785400391
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.6806640625
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.6720943450927734
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.671875
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.672716776529948
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.666015625
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.6754557291666667
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.6721507481166294
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.6713027954101562
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.6713976178850447
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.6730575561523438
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.6714390345982143
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.6728264944893974
Final sparsity level of 0.672: 0.6720390722470033
Transfer numbers...
initialize by random pruning
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.6720617964372161
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.6736457320038911
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 0.6731770833333333
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6711900499131944
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6725141737196181
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6719801161024306
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6709340413411458
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6723615858289931
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6717563205295138
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6713799370659722
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6721327039930556
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6724022759331597
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6709916856553819
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.671897464328342
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6718860202365451
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.67236328125
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6702355278862847
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6726158989800347
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6729346381293403
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6717715793185763
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6723264058430989
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6716495090060763
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6724124484592013
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6708051893446181
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6731550428602431
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6719495985243056
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6718504163953993
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.670989990234375
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6708119710286458
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6725785997178819
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.672607421875
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6725095113118489
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6716367933485243
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6708543565538194
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6718546549479167
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6717020670572917
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6732957628038194
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6717703077528212
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6720000372992622
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6719801161024306
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6720038519965278
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6718292236328125
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6725582546657987
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6717029147677951
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6719661288791232
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6715240478515625
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6723564995659722
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6719512939453125
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6724260118272569
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6719021267361112
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6717037624782987
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6718597412109375
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6721038818359375
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6730109320746528
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6717732747395833
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6715727912055122
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6723344590928819
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6715477837456597
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6721428765190972
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6720479329427083
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.671539306640625
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6722522311740451
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.67193603515625
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6707797580295138
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6736467149522569
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6731160481770833
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6725870768229167
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6718733045789931
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6716579861111112
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6725785997178819
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6727583143446181
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6725785997178819
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6717681884765625
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6719894409179688
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6717228359646268
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.6714935302734375
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.6719417572021484
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.6722793579101562
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.6723836263020833
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.6715946197509766
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.6717782020568848
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.6727180480957031
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.6714024543762207
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.6720952987670898
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.6719169616699219
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.6708583831787109
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.6719818115234375
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.6707210540771484
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.674072265625
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.6719608306884766
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.71875
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.6714922587076824
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.671875
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.6783854166666667
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.6716134207589286
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.6719000680106026
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.6720166887555803
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.6721420288085938
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.6716221400669643
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.6715567452566964
Final sparsity level of 0.672: 0.6719730099942227
Transfer numbers...
initialize by random pruning
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.6718930553483206
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.6744285019455253
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 0.7122395833333333
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6714562310112847
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6719868977864583
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6724378797743056
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6716444227430556
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6720076666937934
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6720347934299045
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6717885335286458
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6719038221571181
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6716105143229167
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6713612874348958
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6724806891547309
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6720873514811199
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6722513834635417
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6725650363498263
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6716766357421875
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6721988254123263
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6719534132215712
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6712917751736112
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6719072129991319
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6719139946831597
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6727159288194444
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6717258029513888
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6720246209038628
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6719779968261719
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6733737521701388
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6728956434461806
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6720191107855903
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6708611382378472
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6721700032552083
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6725201076931424
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6705135769314237
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6723141140407987
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6714884440104167
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6724039713541667
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6720369127061632
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.671736823187934
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6708747016059028
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6733347574869792
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6718970404730903
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6718868679470487
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6724060906304253
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6720627678765191
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6717139350043403
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6730024549696181
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6717987060546875
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6729939778645833
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6716431511773003
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6721098158094618
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6731194390190972
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6718800862630208
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6725480821397569
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6720835367838542
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6711832682291667
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6722034878200955
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6732872856987847
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6714630126953125
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6712799072265625
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6710764567057292
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6724489000108507
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6725192599826388
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6722666422526042
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6716630723741319
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6722903781467013
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6715460883246528
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.672437032063802
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6724234686957465
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6720089382595487
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6725260416666667
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6727481418185763
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6721598307291667
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.672314961751302
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6722073025173612
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.6720292833116319
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.6717898050944011
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.6719112396240234
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.6718902587890625
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.6717500686645508
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.6722640991210938
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.6716184616088867
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.6722121238708496
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.6729297637939453
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.6717796325683594
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.6727333068847656
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.6720085144042969
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.6724720001220703
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.6650390625
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.6721744537353516
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.67578125
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.6724688212076824
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.66015625
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.6650390625
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.6718499319893974
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.672041756766183
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.6717823573521206
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.671740940638951
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.6726433890206474
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.6721104213169643
Final sparsity level of 0.672: 0.6720385392077636
Transfer numbers...
initialize by random pruning
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.6719299172717929
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.67202192769131
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 0.6510416666666667
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6719597710503472
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6713578965928819
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6697269015842013
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6724616156684028
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6719195048014324
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6722933451334636
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6728922526041667
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6726498074001737
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6726108127170138
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6721954345703125
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6722123887803819
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.671982659233941
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.672332763671875
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6726040310329862
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6722039116753472
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6717325846354167
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6717686123318143
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.671826680501302
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6712324354383681
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6717410617404513
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6722496880425347
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6716071234809028
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6720233493381076
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6719139946831597
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6719546847873263
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.67230224609375
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6717987060546875
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6726599799262153
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6717147827148438
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6718470255533855
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6718817816840278
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6711035834418403
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6714969211154513
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6722310384114583
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6720619201660156
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6712667677137587
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6714443630642362
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6713663736979167
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6716478135850694
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6716783311631944
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6721144782172309
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6715948316786025
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6719072129991319
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6716240776909722
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6730940077039931
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6725192599826388
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6718215942382812
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6719818115234375
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6719784206814237
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6716800265842013
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6730024549696181
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6716172960069444
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6718016730414497
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6724026997884114
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6726548936631944
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6723853217230903
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6726616753472222
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6723887125651042
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6725290086534288
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6717274983723958
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6727057562934028
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.672210693359375
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6726701524522569
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6716223822699653
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6723798116048176
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6714295281304253
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6725006103515625
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6716664632161458
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6729380289713542
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6711374918619792
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6721250745985243
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6718050638834636
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.6715223524305556
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.6723950703938801
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.6717195510864258
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.6721172332763672
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.6719064712524414
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.6716156005859375
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.6714210510253906
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.6722068786621094
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.6720914840698242
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.6707191467285156
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.6727333068847656
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.6730918884277344
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.6731395721435547
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.676513671875
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.672882080078125
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.638671875
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.6718152364095051
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.673828125
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.6728515625
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.6720875331333706
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.6721910749162947
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.6719000680106026
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.6715022495814732
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.672381809779576
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.6724025181361607
Final sparsity level of 0.672: 0.6719724077291078
wandb: WARNING W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.
2022-09-10 07:27:08,160 | INFO | main_after.py: 96 : main() ::	 Experiment Name: run_cv_asdiv-a
2022-09-10 07:27:08,160 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:27:08,160 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:27:08,535 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:27:08,535 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 979
2022-09-10 07:27:08,535 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 238
2022-09-10 07:27:08,535 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: []
2022-09-10 07:27:08,535 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:27:08,535 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:27:08,715 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 2882 / 2882 = 1.0
2022-09-10 07:27:08,745 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 2885 words in input language, 12 words in output
2022-09-10 07:27:08,752 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:27:12,162 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:27:12,162 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:27:15,632 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:27:15,632 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/asdiv/dense/models/run_cv_asdiv-a_fold0.
2022-09-10 07:27:19,428 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:28:30,993 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.029411764705882353	
 equation acc epoch: 0.0	
 max val acc: 0.029411764705882353	
 equation acc: 0.0	
2022-09-10 07:28:30,994 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 1m 11s
2022-09-10 07:28:30,997 | INFO | main_after.py: 96 : main() ::	 Experiment Name: run_cv_asdiv-a
2022-09-10 07:28:30,997 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:28:30,997 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:28:31,290 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:28:31,290 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 979
2022-09-10 07:28:31,290 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 238
2022-09-10 07:28:31,290 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: []
2022-09-10 07:28:31,290 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:28:31,290 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:28:31,464 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 2882 / 2882 = 1.0
2022-09-10 07:28:31,494 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 2885 words in input language, 12 words in output
2022-09-10 07:28:31,502 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:28:34,430 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:28:34,430 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:28:34,515 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:28:34,515 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/asdiv/dense/models/run_cv_asdiv-a_fold1.
2022-09-10 07:28:39,807 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:28:46,164 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.012605042016806723	
 equation acc epoch: 0.0	
 max val acc: 0.012605042016806723	
 equation acc: 0.0	
2022-09-10 07:28:46,164 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 6s
2022-09-10 07:28:46,167 | INFO | main_after.py: 96 : main() ::	 Experiment Name: run_cv_asdiv-a
2022-09-10 07:28:46,167 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:28:46,167 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:28:46,456 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:28:46,456 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 979
2022-09-10 07:28:46,456 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 238
2022-09-10 07:28:46,456 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: []
2022-09-10 07:28:46,456 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:28:46,456 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:28:46,629 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 2882 / 2882 = 1.0
2022-09-10 07:28:46,659 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 2885 words in input language, 12 words in output
2022-09-10 07:28:46,667 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:28:49,596 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:28:49,596 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:28:49,683 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:28:49,684 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/asdiv/dense/models/run_cv_asdiv-a_fold2.
2022-09-10 07:28:55,056 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:29:01,078 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.01680672268907563	
 equation acc epoch: 0.0	
 max val acc: 0.01680672268907563	
 equation acc: 0.0	
2022-09-10 07:29:01,079 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 6s
2022-09-10 07:29:01,081 | INFO | main_after.py: 96 : main() ::	 Experiment Name: run_cv_asdiv-a
2022-09-10 07:29:01,081 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:29:01,081 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:29:01,373 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:29:01,373 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 980
2022-09-10 07:29:01,373 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 237
2022-09-10 07:29:01,373 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: []
2022-09-10 07:29:01,373 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:29:01,374 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:29:01,547 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 2882 / 2882 = 1.0
2022-09-10 07:29:01,577 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 2885 words in input language, 12 words in output
2022-09-10 07:29:01,585 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:29:04,437 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:29:04,438 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:29:04,519 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:29:04,519 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/asdiv/dense/models/run_cv_asdiv-a_fold3.
2022-09-10 07:29:09,663 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:29:17,083 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.0	
 equation acc epoch: 0.0	
 max val acc: 0.0	
 equation acc: 0.0	
2022-09-10 07:29:17,083 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 7s
2022-09-10 07:29:17,086 | INFO | main_after.py: 96 : main() ::	 Experiment Name: run_cv_asdiv-a
2022-09-10 07:29:17,086 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:29:17,086 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:29:17,377 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:29:17,378 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 951
2022-09-10 07:29:17,378 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 266
2022-09-10 07:29:17,378 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: []
2022-09-10 07:29:17,378 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:29:17,378 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:29:17,554 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 2882 / 2882 = 1.0
2022-09-10 07:29:17,585 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 2885 words in input language, 12 words in output
2022-09-10 07:29:17,594 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:29:20,433 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:29:20,433 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:29:20,512 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:29:20,512 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/asdiv/dense/models/run_cv_asdiv-a_fold4.
2022-09-10 07:29:22,292 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:29:28,711 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.0037593984962406013	
 equation acc epoch: 0.0	
 max val acc: 0.0037593984962406013	
 equation acc: 0.0	
2022-09-10 07:29:28,711 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 6s
Traceback (most recent call last):
  File "/home/sliu/miniconda3/envs/prune_cry/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/sliu/miniconda3/envs/prune_cry/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/gpfs/home6/sliu/Projects/SVAMP/code/gts/src/main_after.py", line 276, in <module>
    main()
  File "/gpfs/home6/sliu/Projects/SVAMP/code/gts/src/main_after.py", line 266, in main
    folds_scores.append(float(best_acc[w][0])/best_acc[w][1])
ZeroDivisionError: float division by zero
Transfer numbers...
initialize by random pruning
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.7379340795865248
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.7388841601815823
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 0.7578125
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7383405897352431
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7377878824869792
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7385118272569444
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7379252115885417
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7381502787272136
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7381418016221788
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7381863064236112
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7377336290147569
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7378370496961806
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7384745279947917
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7376857333713107
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7376026577419705
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7384897867838542
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7387746175130208
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7379489474826388
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7381642659505208
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7378078036838107
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7383719550238715
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7384117974175347
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7371232774522569
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7381354437934028
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7380133734809028
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7381591796875
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7378938462999132
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7374386257595487
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7376997205946181
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7382269965277778
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7374284532335069
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7381193372938368
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.737809075249566
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7388085259331597
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7384762234157987
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7379387749565972
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7390051947699653
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7380536397298176
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.737990485297309
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7384151882595487
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7382880316840278
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7375403510199653
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.73760986328125
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7380727132161458
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7378650241427951
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7375759548611112
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7375454372829862
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7381150987413194
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7383185492621528
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7379315694173176
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7378696865505643
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7384253607855903
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7376319037543403
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7384880913628472
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7378048366970487
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7382032606336806
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7379497951931424
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7378285725911458
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7376980251736112
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7377065022786458
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7374708387586806
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7380434672037761
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7383164299858941
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7385592990451388
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7385949028862847
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7387644449869792
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7385609944661458
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7381028069390191
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7381791008843316
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7378624810112847
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7376081678602431
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7390797932942708
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7375386555989583
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7381596035427518
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.737845950656467
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.7380828857421875
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.7380078633626301
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.7380638122558594
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.7374369303385417
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.7370471954345703
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.7380771636962891
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.7379341125488281
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.7378063201904297
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.7386388778686523
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.7373428344726562
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.7379741668701172
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.7369232177734375
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.7380695343017578
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.742919921875
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.7386665344238281
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.728515625
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.7377675374348958
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.763671875
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.7314453125
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.7376294817243303
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.7384534563337053
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.7381886073521206
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.737091064453125
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.7387662615094865
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.7372872488839286
Final sparsity level of 0.738: 0.7379910135952974
Transfer numbers...
initialize by random pruning
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.7380315836483304
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.7394769374189365
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 0.7291666666666667
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7379574245876737
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7389102511935763
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7375454372829862
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7381727430555556
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7377522786458333
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7376217312282987
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7390662299262153
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7373216417100694
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7387034098307292
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7378133138020833
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7378417121039497
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7381884256998699
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7375776502821181
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7382168240017362
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7365536159939237
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7379319932725694
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7380989922417535
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7380604214138455
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7370758056640625
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7381134033203125
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7388034396701388
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7371758355034722
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7378489176432292
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7382969326443143
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7382236056857638
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7387915717230903
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7382066514756944
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7377709282769097
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7383715311686199
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7381990220811632
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7381727430555556
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7387949625651042
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7382032606336806
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7383761935763888
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7380544874403212
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7381320529513888
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7375878228081597
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7390255398220487
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7375962999131944
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7389102511935763
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7380782233344184
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7382583618164062
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7374962700737847
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7388831244574653
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7384287516276042
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7371283637152778
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7383045620388455
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7383045620388455
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7372097439236112
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7384982638888888
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7385592990451388
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.738311767578125
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7377611796061199
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7383282979329426
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.737823486328125
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7374742296006944
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7384965684678819
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7384236653645833
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7380167643229167
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7378374735514324
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7378607855902778
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7382795545789931
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7387271457248263
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.736846923828125
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7379540337456597
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7378646002875434
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7387034098307292
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7382609049479167
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.737640380859375
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7381540934244792
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7375742594401042
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7381180657280816
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.7377539740668403
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.7381203969319661
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.7380962371826172
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.7377395629882812
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.7374553680419922
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.7384529113769531
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.7382574081420898
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.7380995750427246
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.7375402450561523
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.7380828857421875
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.7377243041992188
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.73834228515625
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.73828125
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.74462890625
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.7381343841552734
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.740234375
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.7389488220214844
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.71875
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.73828125
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.7383357456752232
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.737140110560826
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.7377973284040178
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.739001682826451
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.737536839076451
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.7388992309570312
Final sparsity level of 0.738: 0.7380539883740403
Transfer numbers...
initialize by random pruning
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.738065052098876
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.7397226613164721
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 0.7434895833333333
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.73712158203125
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7381778293185763
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7373402913411458
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7373470730251737
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.738024393717448
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7374666002061632
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7374454074435763
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7384202745225694
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7384694417317708
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7375606960720487
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7379561530219184
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7378590901692708
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7386389838324653
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7362213134765625
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7385932074652778
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7386525472005208
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7378904554578993
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.738425784640842
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7374521891276042
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7381049262152778
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7368181016710069
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7385796440972222
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7382176717122395
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7380303276909722
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7379828559027778
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.73687744140625
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7382914225260417
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7389119466145833
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7380400763617622
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7377306620279949
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7368062337239583
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7373979356553819
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7379862467447917
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7391289605034722
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7377764383951824
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7381371392144097
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7374793158637153
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7380540635850694
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7377268473307292
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7377912733289931
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7377276950412326
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7379773457845051
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7379286024305556
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7382185194227431
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7382829454210069
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7378200954861112
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7380608452690972
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7375984191894531
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7376607259114583
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7376268174913194
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7386627197265625
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7379286024305556
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.737717522515191
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.738324483235677
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7374131944444444
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7384202745225694
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7376183403862847
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7373555501302083
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7382147047254775
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7380002339680989
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7370927598741319
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7389322916666667
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7386406792534722
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7387068006727431
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7382672627766926
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.737845950656467
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7389305962456597
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7386593288845487
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7384830050998263
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7373691134982638
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7378480699327257
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7376255459255643
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.7377166748046875
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.7378953297932942
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.7381706237792969
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.7381153106689453
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.7380800247192383
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.7378649711608887
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.7386178970336914
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.7377338409423828
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.7382345199584961
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.7374114990234375
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.7368717193603516
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.7376861572265625
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.7369041442871094
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.7392578125
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.7384529113769531
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.783203125
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.7375602722167969
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.72265625
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.7412109375
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.737656729561942
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.7381602696010044
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.737990243094308
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.7382801600864956
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.737701416015625
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.7372643607003349
Final sparsity level of 0.738: 0.7379750708762202
Transfer numbers...
initialize by random pruning
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.7379719258761895
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.7398467898832685
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 0.7669270833333334
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7380133734809028
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7379286024305556
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.73858642578125
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7379421657986112
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7379828559027778
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7378480699327257
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7377098931206597
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7376285129123263
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7377794053819444
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7378133138020833
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7384372287326388
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7378425598144531
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7376454671223958
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7383524576822917
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7375437418619792
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7379642062717013
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7381184895833333
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7371838887532551
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7381015353732638
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7383304172092013
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7382100423177083
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7383846706814237
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7381036546495225
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7381087409125434
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7394205729166667
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7384067111545138
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7381490071614583
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7370503743489583
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7382447984483507
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7383240593804253
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7372300889756944
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7381134033203125
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7371860080295138
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7380218505859375
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7380608452690972
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7376560635036893
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7374013264973958
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7391730414496528
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7377404106987847
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7382693820529513
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7381540934244792
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7385393778483074
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7381846110026042
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7391408284505208
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7373267279730903
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7381608751085069
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7375123765733507
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.737868414984809
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7384270562065972
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7377183702256944
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7386322021484375
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7378726535373263
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7373780144585503
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7381371392144097
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.739471435546875
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7376963297526042
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7375912136501737
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7375149197048612
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7384609646267362
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7385275099012587
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7384253607855903
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7373335096571181
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7382337782118056
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7379387749565972
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7381337483723958
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7382710774739583
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7382354736328125
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7382948133680556
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7390916612413194
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7374793158637153
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7381629943847656
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7381002638075087
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.7384457058376737
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.7377440134684246
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.7383241653442383
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.7375653584798176
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.7377891540527344
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.7381105422973633
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.7377729415893555
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.7380504608154297
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.7389669418334961
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.7377510070800781
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.7386817932128906
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.7381820678710938
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.7386264801025391
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.7314453125
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.7386589050292969
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.744140625
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.7379112243652344
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.712890625
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.7473958333333333
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.7379444667271206
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.7376382010323661
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.7374616350446428
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.737685067313058
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.7388033185686385
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.7385482788085938
Final sparsity level of 0.738: 0.7380381287260135
Transfer numbers...
initialize by random pruning
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.7379435605872211
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.7382255188067445
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 0.7200520833333333
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7374979654947917
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7377997504340278
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7361687554253472
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7383168538411458
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.738144768608941
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7381337483723958
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7384457058376737
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.738250732421875
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7381710476345487
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7380116780598958
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7382312350802951
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7378887600368924
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7381201850043403
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7387017144097222
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7377065022786458
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7376454671223958
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7377077738444011
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7377798292371962
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7366282145182292
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7380523681640625
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7383134629991319
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7375606960720487
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7379654778374566
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7378349304199219
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7376352945963542
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7383880615234375
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7376200358072917
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7385982937282987
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.73786375257704
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7378480699327257
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7375200059678819
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7377115885416667
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7383795844184028
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7386796739366319
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7381812201605903
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7376420762803819
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7371893988715278
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7372300889756944
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7375064425998263
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7382880316840278
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7382977803548176
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7378586663140191
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7375200059678819
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7377048068576388
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7389000786675347
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7384151882595487
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7377946641710069
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7378646002875434
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7382473415798612
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7379862467447917
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7380421956380208
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7374182807074653
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7379595438639324
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7382689581976997
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7384931776258681
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7381778293185763
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7385355631510417
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7388458251953125
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7384698655870225
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7375640869140625
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7384050157335069
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7380998399522569
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.73876953125
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7375962999131944
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7383253309461806
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7374725341796875
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7390458848741319
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7380489773220487
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7389882405598958
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7373979356553819
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7382931179470487
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7378370496961806
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.7377065022786458
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.7386112213134766
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.7378959655761719
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.7382113138834636
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.7382583618164062
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.7376017570495605
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.7373771667480469
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.7381744384765625
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.7382392883300781
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.7368087768554688
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.7389240264892578
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.7384681701660156
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.7393436431884766
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.7412109375
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.7386455535888672
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.703125
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.7383346557617188
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.76171875
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.7428385416666667
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.7383063180106026
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.7378082275390625
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.7381057739257812
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.7375292096819197
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.7382943289620536
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.738083975655692
Final sparsity level of 0.738: 0.7379908889887219
wandb: WARNING W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.
2022-09-10 07:29:31,912 | INFO | main_after.py: 96 : main() ::	 Experiment Name: run_cv_asdiv-a
2022-09-10 07:29:31,912 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:29:31,912 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:29:32,281 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:29:32,281 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 979
2022-09-10 07:29:32,281 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 238
2022-09-10 07:29:32,281 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: []
2022-09-10 07:29:32,281 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:29:32,281 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:29:32,460 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 2882 / 2882 = 1.0
2022-09-10 07:29:32,491 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 2885 words in input language, 12 words in output
2022-09-10 07:29:32,499 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:29:35,558 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:29:35,558 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:29:39,030 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:29:39,030 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/asdiv/dense/models/run_cv_asdiv-a_fold0.
2022-09-10 07:29:44,416 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:31:04,699 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.029411764705882353	
 equation acc epoch: 0.0	
 max val acc: 0.029411764705882353	
 equation acc: 0.0	
2022-09-10 07:31:04,699 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 1m 20s
2022-09-10 07:31:04,702 | INFO | main_after.py: 96 : main() ::	 Experiment Name: run_cv_asdiv-a
2022-09-10 07:31:04,702 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:31:04,702 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:31:05,095 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:31:05,095 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 979
2022-09-10 07:31:05,095 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 238
2022-09-10 07:31:05,095 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: []
2022-09-10 07:31:05,095 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:31:05,095 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:31:05,269 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 2882 / 2882 = 1.0
2022-09-10 07:31:05,301 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 2885 words in input language, 12 words in output
2022-09-10 07:31:05,308 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:31:08,652 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:31:08,652 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:31:08,735 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:31:08,735 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/asdiv/dense/models/run_cv_asdiv-a_fold1.
2022-09-10 07:31:13,798 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:31:20,255 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.004201680672268907	
 equation acc epoch: 0.0	
 max val acc: 0.004201680672268907	
 equation acc: 0.0	
2022-09-10 07:31:20,255 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 6s
2022-09-10 07:31:20,257 | INFO | main_after.py: 96 : main() ::	 Experiment Name: run_cv_asdiv-a
2022-09-10 07:31:20,257 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:31:20,257 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:31:20,545 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:31:20,545 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 979
2022-09-10 07:31:20,545 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 238
2022-09-10 07:31:20,545 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: []
2022-09-10 07:31:20,546 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:31:20,546 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:31:20,719 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 2882 / 2882 = 1.0
2022-09-10 07:31:20,749 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 2885 words in input language, 12 words in output
2022-09-10 07:31:20,757 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:31:23,611 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:31:23,611 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:31:23,689 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:31:23,689 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/asdiv/dense/models/run_cv_asdiv-a_fold2.
2022-09-10 07:31:28,873 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:31:34,865 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.012605042016806723	
 equation acc epoch: 0.0	
 max val acc: 0.012605042016806723	
 equation acc: 0.0	
2022-09-10 07:31:34,865 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 5s
2022-09-10 07:31:34,867 | INFO | main_after.py: 96 : main() ::	 Experiment Name: run_cv_asdiv-a
2022-09-10 07:31:34,867 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:31:34,867 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:31:35,155 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:31:35,155 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 980
2022-09-10 07:31:35,156 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 237
2022-09-10 07:31:35,156 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: []
2022-09-10 07:31:35,156 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:31:35,156 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:31:35,330 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 2882 / 2882 = 1.0
2022-09-10 07:31:35,363 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 2885 words in input language, 12 words in output
2022-09-10 07:31:35,371 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:31:38,171 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:31:38,171 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:31:38,250 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:31:38,250 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/asdiv/dense/models/run_cv_asdiv-a_fold3.
2022-09-10 07:31:43,542 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:32:01,767 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.0	
 equation acc epoch: 0.0	
 max val acc: 0.0	
 equation acc: 0.0	
2022-09-10 07:32:01,767 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 18s
2022-09-10 07:32:01,770 | INFO | main_after.py: 96 : main() ::	 Experiment Name: run_cv_asdiv-a
2022-09-10 07:32:01,770 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:32:01,770 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:32:02,061 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:32:02,061 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 951
2022-09-10 07:32:02,061 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 266
2022-09-10 07:32:02,061 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: []
2022-09-10 07:32:02,062 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:32:02,062 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:32:02,237 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 2882 / 2882 = 1.0
2022-09-10 07:32:02,267 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 2885 words in input language, 12 words in output
2022-09-10 07:32:02,276 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:32:05,181 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:32:05,182 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:32:05,261 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:32:05,262 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/asdiv/dense/models/run_cv_asdiv-a_fold4.
2022-09-10 07:32:10,583 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:32:17,206 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.0037593984962406013	
 equation acc epoch: 0.0	
 max val acc: 0.0037593984962406013	
 equation acc: 0.0	
2022-09-10 07:32:17,206 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 6s
Traceback (most recent call last):
  File "/home/sliu/miniconda3/envs/prune_cry/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/sliu/miniconda3/envs/prune_cry/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/gpfs/home6/sliu/Projects/SVAMP/code/gts/src/main_after.py", line 276, in <module>
    main()
  File "/gpfs/home6/sliu/Projects/SVAMP/code/gts/src/main_after.py", line 266, in main
    folds_scores.append(float(best_acc[w][0])/best_acc[w][1])
ZeroDivisionError: float division by zero
Transfer numbers...
initialize by random pruning
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.7909565500762625
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.7911574862191959
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 0.81640625
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7916514078776041
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7907392713758681
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.791534423828125
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7909817165798612
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7910982767740885
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.79100587632921
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7913953993055556
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7902374267578125
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7908884684244791
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7910580105251737
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7907947964138455
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7906205919053819
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7916395399305556
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7911292182074653
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7908749050564237
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7910274929470487
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7906909518771701
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7914386325412326
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7916090223524306
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7902475992838541
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7908664279513888
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7906833224826388
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.791145748562283
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.790914323594835
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7905680338541666
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7907477484809028
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7912139892578125
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7899915907118056
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7909969753689237
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7907392713758681
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7914903428819444
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7912733289930556
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7912648518880209
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7921176486545138
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7909906175401475
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7911275227864584
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7909223768446181
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7911665174696181
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7906460232204862
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7903120252821181
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.790962643093533
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7909075419108073
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7907528347439237
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7910427517361112
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7914242214626737
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7911800808376737
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7908329433865018
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7910376654730903
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7914089626736112
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7910868326822916
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7911953396267362
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7904476589626737
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7912610371907552
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7910105387369791
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7908664279513888
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7908732096354166
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7908460828993056
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7901255289713541
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7909287346733941
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7912428114149306
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7918073866102431
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7916259765625
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7921651204427084
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7916802300347222
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7913267347547743
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.791300031873915
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7909393310546875
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7905171712239584
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7920125325520834
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7904510498046875
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7909554375542535
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7911966111924913
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.7911800808376737
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.7911669413248698
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.790837287902832
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.7902730305989584
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.7902393341064453
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.7909126281738281
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.7909612655639648
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.7908225059509277
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.791499137878418
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.7898330688476562
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.791107177734375
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.7895431518554688
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.7910346984863281
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.79736328125
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.7916469573974609
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.78515625
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.790960947672526
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.806640625
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.7861328125
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.7903812953404018
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.7911322457449776
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.7909872872488839
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.790114266531808
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.7917981828962053
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.790679931640625
Final sparsity level of 0.791: 0.7909883569277268
Transfer numbers...
initialize by random pruning
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.7910196531300773
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.7924798354409858
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 0.7786458333333334
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7906409369574653
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7916531032986112
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7900373670789931
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7911326090494791
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7907842000325521
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.790520985921224
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7920057508680556
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7906494140625
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7914564344618056
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7903951009114584
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7907625834147135
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7909893459743924
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7912055121527778
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7906646728515625
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7896609836154513
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7908732096354166
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7909376356336806
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7911080254448785
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7902865939670138
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7914699978298612
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7913377549913194
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7900407579210069
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7908914354112413
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7912237379286025
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7914225260416666
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7916056315104166
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7913174099392362
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7906646728515625
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7913763258192275
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.791084713406033
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7910987006293403
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7919057210286459
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7916191948784722
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7910970052083334
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7910414801703559
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7910749647352431
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.790740966796875
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7919057210286459
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.79132080078125
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7917361789279513
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7912089029947916
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7910804748535156
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7905069986979166
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7916174994574653
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7916937934027778
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7901102701822916
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7911932203504775
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.79108640882704
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7902933756510416
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7912546793619791
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7918684217664931
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7915445963541666
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7910567389594184
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7912551032172309
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7906612820095487
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7905069986979166
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7914564344618056
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7913174099392362
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7912631564670138
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.790724860297309
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7908189561631944
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7911665174696181
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7913191053602431
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7898288302951388
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7912851969401041
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7908842298719618
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7915412055121528
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7906578911675347
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7905748155381944
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7911478678385416
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7907307942708334
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7912368774414062
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.7908036973741319
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.7909539540608724
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.7912673950195312
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.7907562255859375
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.790562629699707
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.7913131713867188
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.7912740707397461
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.7910366058349609
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.7906312942504883
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.7912216186523438
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.7906532287597656
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.791351318359375
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.7915019989013672
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.795166015625
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.7913246154785156
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.794921875
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.7916221618652344
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.7734375
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.7939453125
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.791062491280692
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.7904564993722099
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.7906526838030135
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.792067391531808
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.7903376988002232
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.7917578560965401
Final sparsity level of 0.791: 0.791037424228126
Transfer numbers...
initialize by random pruning
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.7910669286116914
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.7921859800583657
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 0.7942708333333334
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7902713351779513
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7912495930989584
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7904290093315972
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7904680040147569
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7910953097873263
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7906116909450955
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7904764811197916
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7912105984157987
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7914072672526041
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7901865641276041
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7910194396972656
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7908846537272135
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7911648220486112
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7897288004557291
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7916141086154513
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7917022705078125
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7909732394748263
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7914148966471354
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7908223470052084
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7917192247178819
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7897474500868056
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7910274929470487
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7914301554361979
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7909863789876302
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7909376356336806
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7905019124348959
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7908782958984375
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7920939127604166
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7911449008517795
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7907439337836372
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7900577121310763
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7905951605902778
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7907901340060763
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7919837103949653
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7906909518771701
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.791144053141276
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7907528347439237
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7911105685763888
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7909630669487847
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7911003960503472
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7910291883680556
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7907621595594618
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7913411458333334
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7912750244140625
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7913174099392362
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7912224663628472
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7908253139919705
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7906460232204862
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7910088433159722
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7904273139105903
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7914818657769097
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7908986409505209
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.79063966539171
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7912118699815538
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7906019422743056
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7916090223524306
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7908613416883681
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7901034884982638
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7911686367458768
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7907536824544271
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7898339165581597
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7916429307725694
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7917243109809028
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7916107177734375
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7916107177734375
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7908816867404513
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7910614013671875
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7915683322482638
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7912682427300347
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7905036078559028
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7909702724880643
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7906515333387587
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.7910207112630209
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.7907950083414713
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.7910366058349609
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.7910950978597006
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.7910175323486328
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.7907466888427734
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.7917232513427734
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.7906899452209473
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.7912616729736328
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.7916374206542969
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.789581298828125
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.7904510498046875
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.7899837493896484
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.795166015625
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.7911434173583984
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.841796875
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.7905235290527344
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.7890625
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.7923177083333334
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.7903355189732143
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.7910374232700893
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.7911322457449776
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.791003635951451
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.791046142578125
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.7903453281947544
Final sparsity level of 0.791: 0.7909836634133828
Transfer numbers...
initialize by random pruning
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.7910289268957856
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.7920314526588845
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 0.7981770833333334
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7910478379991319
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7909664577907987
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.791778564453125
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7910936143663194
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7911694844563802
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.79070070054796
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7905799018012153
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7907782660590278
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7900390625
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7909596761067709
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7915000915527344
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7910902235243056
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7906138102213541
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7916412353515625
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7905849880642362
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7903832329644097
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7910931905110677
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7904684278700087
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7913733588324653
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7914716932508681
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7904561360677084
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7910732693142362
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7909350925021701
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7910474141438802
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7922702365451388
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7913631863064237
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7909511990017362
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7902119954427084
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7909677293565538
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.791359371609158
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7903171115451388
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7910970052083334
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7903221978081597
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7906477186414931
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.791115230984158
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7908503214518229
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7906172010633681
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7919701470269097
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7910800509982638
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7910953097873263
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.790994856092665
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7915941874186198
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7911597357855903
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7918006049262153
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.79095458984375
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7908528645833334
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7907396952311198
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7907452053493924
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7912919786241319
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7909495035807291
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7916327582465278
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7909698486328125
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7904112074110243
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7909499274359809
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.792205810546875
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7903832329644097
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7906273735894097
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7909834120008681
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7915149264865451
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7911758422851562
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7911207411024306
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7904798719618056
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7910647922092013
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7907460530598959
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.790917714436849
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7911402384440104
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7911105685763888
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7911207411024306
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7922956678602431
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7906765407986112
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7911673651801215
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.791100819905599
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.7912139892578125
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.790899912516276
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.7912588119506836
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.7904491424560547
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.7908668518066406
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.7911806106567383
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.7913570404052734
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.791205883026123
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.7921152114868164
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.7903594970703125
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.791656494140625
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.7907066345214844
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.7915935516357422
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.7861328125
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.7917823791503906
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.802734375
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.7911249796549479
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.791015625
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.8053385416666666
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.7908248901367188
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.7909327915736607
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.7902930123465401
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.7907812935965401
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.7915758405412947
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.7913109915597099
Final sparsity level of 0.791: 0.7910435507180883
Transfer numbers...
initialize by random pruning
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.7909634147352366
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.7912714818417639
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 0.7760416666666666
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7909884982638888
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7911970350477431
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7897084554036459
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7911597357855903
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7912326388888888
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7910940382215712
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7916446261935763
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7914479573567709
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7910325792100694
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7912750244140625
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7912419637044271
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7908024258083768
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7911766899956597
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7916327582465278
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7908952501085069
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7907969156901041
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7907892862955729
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7906494140625
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7903730604383681
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7909071180555556
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7914377848307291
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7906358506944444
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7911262512207031
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7907825046115451
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7905358208550347
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7906646728515625
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7905561659071181
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7913564046223959
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7908994886610243
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7908702426486545
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7907613118489584
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7908053927951388
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7910749647352431
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7917192247178819
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.791222890218099
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7907155354817709
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7901645236545138
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7904730902777778
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7910410563151041
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7914496527777778
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7912877400716146
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7908944023980035
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7902950710720487
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7912072075737847
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.791778564453125
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7906324598524306
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.790763431125217
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7908185323079427
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7911393907335069
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7908053927951388
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7902645534939237
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7905341254340278
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7910029093424479
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7912343343098959
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7913275824652778
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7914496527777778
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.791229248046875
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7916802300347222
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7913356357150607
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7906934950086806
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7912495930989584
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7909969753689237
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7915157741970487
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7904425726996528
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7911961873372396
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7904930114746094
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7919277615017362
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7909274631076388
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7919260660807291
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7905629475911459
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7912368774414062
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7909961276584201
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.7908291286892362
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.7917315165201823
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.7912607192993164
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.7912559509277344
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.7913141250610352
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.7905058860778809
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.7903900146484375
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.7911643981933594
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.7914361953735352
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.7902488708496094
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.7920436859130859
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.7911605834960938
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.7918910980224609
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.79052734375
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.7920856475830078
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.763671875
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.7913983662923177
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.8046875
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.7955729166666666
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.7911584036690849
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.7906908307756697
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.7910940987723214
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.7904652186802456
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.7912532261439732
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.7912194388253349
Final sparsity level of 0.791: 0.7910035520073511
wandb: WARNING W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.
2022-09-10 07:32:20,466 | INFO | main_after.py: 96 : main() ::	 Experiment Name: run_cv_asdiv-a
2022-09-10 07:32:20,466 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:32:20,466 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:32:20,843 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:32:20,843 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 979
2022-09-10 07:32:20,843 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 238
2022-09-10 07:32:20,843 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: []
2022-09-10 07:32:20,843 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:32:20,843 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:32:21,024 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 2882 / 2882 = 1.0
2022-09-10 07:32:21,054 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 2885 words in input language, 12 words in output
2022-09-10 07:32:21,062 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:32:24,131 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:32:24,132 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:32:27,579 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:32:27,579 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/asdiv/dense/models/run_cv_asdiv-a_fold0.
2022-09-10 07:32:32,806 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:33:40,047 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.01680672268907563	
 equation acc epoch: 0.0	
 max val acc: 0.01680672268907563	
 equation acc: 0.0	
2022-09-10 07:33:40,047 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 1m 7s
2022-09-10 07:33:40,049 | INFO | main_after.py: 96 : main() ::	 Experiment Name: run_cv_asdiv-a
2022-09-10 07:33:40,050 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:33:40,050 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:33:40,338 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:33:40,338 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 979
2022-09-10 07:33:40,338 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 238
2022-09-10 07:33:40,338 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: []
2022-09-10 07:33:40,338 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:33:40,338 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:33:40,513 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 2882 / 2882 = 1.0
2022-09-10 07:33:40,543 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 2885 words in input language, 12 words in output
2022-09-10 07:33:40,551 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:33:43,817 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:33:43,817 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:33:43,908 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:33:43,908 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/asdiv/dense/models/run_cv_asdiv-a_fold1.
2022-09-10 07:33:47,694 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:34:26,352 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.008403361344537815	
 equation acc epoch: 0.0	
 max val acc: 0.008403361344537815	
 equation acc: 0.0	
2022-09-10 07:34:26,353 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 38s
2022-09-10 07:34:26,355 | INFO | main_after.py: 96 : main() ::	 Experiment Name: run_cv_asdiv-a
2022-09-10 07:34:26,355 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:34:26,355 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:34:26,646 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:34:26,646 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 979
2022-09-10 07:34:26,646 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 238
2022-09-10 07:34:26,646 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: []
2022-09-10 07:34:26,646 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:34:26,646 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:34:26,821 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 2882 / 2882 = 1.0
2022-09-10 07:34:26,852 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 2885 words in input language, 12 words in output
2022-09-10 07:34:26,861 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:34:29,747 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:34:29,747 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:34:29,826 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:34:29,826 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/asdiv/dense/models/run_cv_asdiv-a_fold2.
2022-09-10 07:34:34,628 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:35:07,989 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.012605042016806723	
 equation acc epoch: 0.0	
 max val acc: 0.012605042016806723	
 equation acc: 0.0	
2022-09-10 07:35:07,990 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 33s
2022-09-10 07:35:07,991 | INFO | main_after.py: 96 : main() ::	 Experiment Name: run_cv_asdiv-a
2022-09-10 07:35:07,991 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:35:07,992 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:35:08,282 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:35:08,282 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 980
2022-09-10 07:35:08,282 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 237
2022-09-10 07:35:08,282 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: []
2022-09-10 07:35:08,282 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:35:08,282 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:35:08,457 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 2882 / 2882 = 1.0
2022-09-10 07:35:08,489 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 2885 words in input language, 12 words in output
2022-09-10 07:35:08,497 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:35:11,644 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:35:11,644 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:35:11,728 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:35:11,729 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/asdiv/dense/models/run_cv_asdiv-a_fold3.
2022-09-10 07:35:17,106 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:35:48,556 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.0	
 equation acc epoch: 0.0	
 max val acc: 0.0	
 equation acc: 0.0	
2022-09-10 07:35:48,556 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 31s
2022-09-10 07:35:48,558 | INFO | main_after.py: 96 : main() ::	 Experiment Name: run_cv_asdiv-a
2022-09-10 07:35:48,558 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:35:48,558 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:35:48,848 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:35:48,848 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 951
2022-09-10 07:35:48,848 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 266
2022-09-10 07:35:48,848 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: []
2022-09-10 07:35:48,848 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:35:48,848 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:35:49,024 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 2882 / 2882 = 1.0
2022-09-10 07:35:49,055 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 2885 words in input language, 12 words in output
2022-09-10 07:35:49,064 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:35:51,935 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:35:51,935 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:35:52,012 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:35:52,012 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/asdiv/dense/models/run_cv_asdiv-a_fold4.
2022-09-10 07:35:57,397 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:36:03,746 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.011278195488721804	
 equation acc epoch: 0.0	
 max val acc: 0.011278195488721804	
 equation acc: 0.0	
2022-09-10 07:36:03,747 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 6s
Traceback (most recent call last):
  File "/home/sliu/miniconda3/envs/prune_cry/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/sliu/miniconda3/envs/prune_cry/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/gpfs/home6/sliu/Projects/SVAMP/code/gts/src/main_after.py", line 276, in <module>
    main()
  File "/gpfs/home6/sliu/Projects/SVAMP/code/gts/src/main_after.py", line 266, in main
    folds_scores.append(float(best_acc[w][0])/best_acc[w][1])
ZeroDivisionError: float division by zero
Transfer numbers...
initialize by random pruning
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.8324389848386883
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.8330192120622568
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 0.8502604166666666
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8330400254991319
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8321940104166666
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.832427978515625
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8323194715711806
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8325432671440972
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8324788411458334
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8327501085069444
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.832427978515625
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.832305908203125
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8327433268229166
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.832344479031033
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8320727878146701
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8332672119140625
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8325466579861112
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8324703640407987
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8328620062934028
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8322804768880209
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8329052395290799
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8328687879774306
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8320261637369791
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8320939805772569
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8317684597439237
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8326102362738715
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8321783277723525
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8318498399522569
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8323754204644097
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8326144748263888
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8317548963758681
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8324207729763455
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8322372436523438
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8327772352430556
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8320939805772569
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8325551350911459
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.83349609375
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8324076334635416
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8326377868652344
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8321295844184028
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8330298529730903
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8320363362630209
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8322601318359375
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8323326110839844
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.83240720960829
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8322669135199653
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8326365152994791
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8327958848741319
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8328840467664931
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8326148986816406
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8325780232747396
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8326212565104166
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8325839572482638
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8325364854600694
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8324093288845487
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8328081766764323
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8325797186957465
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8324330647786459
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8323482937282987
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8326076931423612
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8319939507378472
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8323716057671441
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.832656012641059
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8332892523871528
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8332095675998263
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8336096869574653
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8330315483940972
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8329493204752604
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8328912523057725
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8326161702473959
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8317057291666666
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8333011203342013
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8322092692057291
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8325017293294271
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.83252927992079
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.8329722086588541
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.8325430552164713
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.8325090408325195
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.8317616780598959
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.8320465087890625
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.8324227333068848
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.8330116271972656
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.83233642578125
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.8331079483032227
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.8314552307128906
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.8327884674072266
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.8313446044921875
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.8323307037353516
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.838623046875
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.8336601257324219
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.837890625
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.8323376973470052
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.837890625
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.8291015625
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.8321435110909599
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.832472664969308
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.8325086321149553
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.8319386073521206
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.8329315185546875
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.8323767525809151
Final sparsity level of 0.8325: 0.8324912697864166
Transfer numbers...
initialize by random pruning
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.8325530158907789
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.8336119892996109
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 0.8255208333333334
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8321211073133681
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8332756890190972
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8318532307942709
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8328823513454862
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8323389689127604
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8320274353027344
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.833404541015625
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8321431477864584
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8330128987630209
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8316650390625
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8324737548828125
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8325000339084201
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8324008517795138
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8319108751085069
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8313276502821181
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8322363959418403
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8323258293999566
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8326797485351562
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8315412733289931
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8331926133897569
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8328687879774306
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8315751817491319
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8325542873806424
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8325475056966146
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8328382703993056
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8326822916666666
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8328755696614584
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8319871690538194
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8327513800726997
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8326903449164497
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8320685492621528
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8333570692274306
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8328348795572916
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8327145046657987
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8326594034830729
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.832708994547526
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8324957953559028
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8332197401258681
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.83245849609375
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8329399956597222
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8326386345757378
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8325898912217882
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8324754503038194
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8331129286024306
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8332400851779513
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8319346110026041
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8326822916666666
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8326793246799045
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8320210774739584
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8325364854600694
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8331400553385416
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8323313395182291
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8323690626356337
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.832498762342665
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8324703640407987
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8320024278428819
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8328569200303819
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8324127197265625
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8325860765245225
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8323944939507378
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8322736952039931
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8325941297743056
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8328111436631944
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8319447835286459
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8327022128634982
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8325055440266927
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8327704535590278
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8320871988932291
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8321092393663194
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8327518039279513
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8323071797688802
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8326242234971788
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.8320804172092013
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.8323720296223959
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.8325052261352539
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.8323014577229818
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.8321352005004883
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.8330230712890625
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.8324413299560547
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.832430362701416
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.8324174880981445
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.8328094482421875
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.8322334289550781
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.8326835632324219
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.8329830169677734
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.83447265625
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.8326969146728516
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.84765625
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.832922617594401
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.822265625
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.8375651041666666
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.8324683053152901
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.8320563180106026
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.8327320643833706
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.8332846505301339
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.8319222586495536
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.8328584943498885
Final sparsity level of 0.8325: 0.8325344390422418
Transfer numbers...
initialize by random pruning
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.8325512284890082
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.8335309257457847
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 0.8255208333333334
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8320448133680556
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8326212565104166
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8316701253255209
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8319481743706597
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8326021830240885
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8322647942437066
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8319617377387153
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8326822916666666
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8329010009765625
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.83160400390625
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8322347005208334
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.832472907172309
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8327687581380209
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8313564724392362
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8330569797092013
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8328314887152778
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8323241339789497
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8329857720269097
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8324500189887153
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8330705430772569
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8314700656467013
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8323872884114584
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8328276740180122
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8323563469780816
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8326076931423612
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8320109049479166
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8327789306640625
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8331485324435763
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.832601335313585
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8323881361219618
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8315463595920138
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8321601019965278
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8324466281467013
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8332299126519097
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8323949178059896
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8325152926974826
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8325449625651041
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8327382405598959
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8327551947699653
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8325127495659722
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8325945536295573
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8324436611599393
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8323635525173612
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8326483832465278
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8325737847222222
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8324907090928819
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.832106696234809
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8322660658094618
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8326738145616319
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8318515353732638
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8330179850260416
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8326212565104166
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8322550455729166
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.832472907172309
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8321685791015625
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8325754801432291
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8324381510416666
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8318057590060763
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8327242533365885
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.832206302218967
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8314107259114584
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8333062065972222
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8327874077690972
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8331705729166666
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8328865898980035
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8323864407009549
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8326839870876737
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8331790500217013
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8326822916666666
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8316836886935763
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8325407240125868
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.832434336344401
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.8320227728949653
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.8322219848632812
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.8328313827514648
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.8325735727945963
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.8323965072631836
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.832451343536377
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.8331918716430664
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.8323721885681152
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.8328027725219727
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.8334808349609375
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.8315696716308594
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.8311805725097656
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.8317413330078125
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.830810546875
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.8324260711669922
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.876953125
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.8324368794759115
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.826171875
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.833984375
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.832171848842076
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.8326078142438615
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.8325206211635044
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.832599094935826
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.8326982770647322
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.8324672154017857
Final sparsity level of 0.8325: 0.8324896568235225
Transfer numbers...
initialize by random pruning
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.8325480681554428
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.8336803866731517
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 0.8333333333333334
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8326585557725694
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.832855224609375
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8332672119140625
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8330620659722222
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8327149285210503
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8324406941731771
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8320515950520834
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8327297634548612
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8316786024305556
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8327501085069444
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8328170776367188
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8324237399631076
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8322109646267362
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8331993950737847
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8321261935763888
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8319600423177084
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8324720594618056
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8320541381835938
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8325280083550347
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8326263427734375
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8323313395182291
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8324347601996528
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8324979146321615
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8325576782226562
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8333706325954862
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8327653672960069
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8329247368706597
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8318345811631944
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8324534098307291
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8327361212836372
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.83245849609375
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8328518337673612
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8317125108506944
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8323991563585069
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8328420850965712
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8325682746039497
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8319040934244791
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8331468370225694
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8325483534071181
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8325008816189237
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8326610989040799
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8329395718044705
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8325059678819444
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8332248263888888
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8324788411458334
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8327653672960069
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8322177463107638
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8323347303602431
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8326178656684028
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8324127197265625
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8329739040798612
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8320821126302084
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8318277994791666
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8322003682454427
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8335028754340278
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8321719699435763
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8319956461588541
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8323398166232638
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8327823215060763
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8326373630099826
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.832122802734375
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8320482042100694
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8323584662543403
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8323279486762153
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8323618570963541
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8325589497884115
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8320346408420138
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8329603407118056
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8339894612630209
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8323008219401041
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8325937059190538
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.832558102077908
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.8328620062934028
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.8323078155517578
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.8327779769897461
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.8319746653238932
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.8323688507080078
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.8326964378356934
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.8328170776367188
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.8328380584716797
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.833404541015625
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.8319778442382812
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.8328285217285156
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.8322639465332031
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.8327350616455078
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.830810546875
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.8331794738769531
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.8515625
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.8325551350911459
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.837890625
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.8463541666666666
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.8324105398995536
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.8324290684291294
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.8320181710379464
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.8321293422154018
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.832625252859933
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.8324214390345982
Final sparsity level of 0.8325: 0.8325377618842552
Transfer numbers...
initialize by random pruning
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.8324982540452269
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.8329102829118029
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 0.8229166666666666
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8322075737847222
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.83282470703125
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8309953477647569
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8326653374565972
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.832723405626085
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8326140509711372
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8332146538628472
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8329637315538194
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8327060275607638
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8323025173611112
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8326716952853732
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8323292202419705
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8323432074652778
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8330112033420138
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8322669135199653
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8323194715711806
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8324250115288628
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8321876525878906
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8318667941623263
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8330213758680556
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8331976996527778
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8316887749565972
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8325076633029513
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8322965833875868
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8318159315321181
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8322414822048612
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8316599527994791
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8330586751302084
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8325352138943143
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8322160508897569
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.832550048828125
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8327416314019097
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8324720594618056
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8327009412977431
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8327259487575955
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8324131435818143
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8317735460069444
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8321194118923612
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8329179551866319
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8336622450086806
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8327301873101128
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8323949178059896
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8315633138020834
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8325364854600694
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8332604302300347
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8325602213541666
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.832389407687717
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8321711222330729
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8327382405598959
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8327077229817709
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8316616482204862
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8319583468967013
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8325716654459635
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8327462938096788
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8327551947699653
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8329671223958334
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8331841362847222
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8331078423394097
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8327975802951388
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8322495354546441
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8324839274088541
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8327162000868056
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8326127794053819
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8314802381727431
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.832725101047092
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8322745429144965
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8332383897569444
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8325771755642362
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8330959743923612
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8319820827907987
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.83294677734375
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8323597378200955
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.832672119140625
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.8332029978434244
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.8329801559448242
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.8327414194742838
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.8326883316040039
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.8321847915649414
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.8318872451782227
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.8328208923339844
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.8327741622924805
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.8315086364746094
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.8334712982177734
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.8323860168457031
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.8328571319580078
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.83154296875
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.8335666656494141
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.8125
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.8328806559244791
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.849609375
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.8382161458333334
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.8324552263532365
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.8320868355887276
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.8327037266322544
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.8321249825613839
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.8327505929129464
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.8326012747628349
Final sparsity level of 0.8325: 0.8325232175278587
wandb: WARNING W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.
2022-09-10 07:36:06,677 | INFO | main_after.py: 96 : main() ::	 Experiment Name: run_cv_asdiv-a
2022-09-10 07:36:06,678 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:36:06,678 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:36:07,051 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:36:07,051 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 979
2022-09-10 07:36:07,051 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 238
2022-09-10 07:36:07,051 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: []
2022-09-10 07:36:07,051 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:36:07,051 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:36:07,230 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 2882 / 2882 = 1.0
2022-09-10 07:36:07,261 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 2885 words in input language, 12 words in output
2022-09-10 07:36:07,268 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:36:10,560 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:36:10,560 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:36:14,204 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:36:14,204 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/asdiv/dense/models/run_cv_asdiv-a_fold0.
2022-09-10 07:36:19,444 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:37:21,624 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.01680672268907563	
 equation acc epoch: 0.0	
 max val acc: 0.01680672268907563	
 equation acc: 0.0	
2022-09-10 07:37:21,624 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 1m 2s
2022-09-10 07:37:21,627 | INFO | main_after.py: 96 : main() ::	 Experiment Name: run_cv_asdiv-a
2022-09-10 07:37:21,627 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:37:21,627 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:37:21,919 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:37:21,919 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 979
2022-09-10 07:37:21,919 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 238
2022-09-10 07:37:21,919 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: []
2022-09-10 07:37:21,919 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:37:21,919 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:37:22,093 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 2882 / 2882 = 1.0
2022-09-10 07:37:22,125 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 2885 words in input language, 12 words in output
2022-09-10 07:37:22,132 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:37:25,170 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:37:25,171 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:37:25,249 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:37:25,249 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/asdiv/dense/models/run_cv_asdiv-a_fold1.
2022-09-10 07:37:27,284 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:37:48,388 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.004201680672268907	
 equation acc epoch: 0.0	
 max val acc: 0.004201680672268907	
 equation acc: 0.0	
2022-09-10 07:37:48,388 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 21s
2022-09-10 07:37:48,391 | INFO | main_after.py: 96 : main() ::	 Experiment Name: run_cv_asdiv-a
2022-09-10 07:37:48,391 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:37:48,391 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:37:48,683 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:37:48,683 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 979
2022-09-10 07:37:48,683 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 238
2022-09-10 07:37:48,683 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: []
2022-09-10 07:37:48,683 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:37:48,683 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:37:48,859 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 2882 / 2882 = 1.0
2022-09-10 07:37:48,890 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 2885 words in input language, 12 words in output
2022-09-10 07:37:48,898 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:37:51,920 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:37:51,920 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:37:51,999 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:37:51,999 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/asdiv/dense/models/run_cv_asdiv-a_fold2.
2022-09-10 07:37:55,801 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:38:22,804 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.012605042016806723	
 equation acc epoch: 0.0	
 max val acc: 0.012605042016806723	
 equation acc: 0.0	
2022-09-10 07:38:22,805 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 27s
2022-09-10 07:38:22,807 | INFO | main_after.py: 96 : main() ::	 Experiment Name: run_cv_asdiv-a
2022-09-10 07:38:22,807 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:38:22,807 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:38:23,101 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:38:23,101 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 980
2022-09-10 07:38:23,101 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 237
2022-09-10 07:38:23,101 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: []
2022-09-10 07:38:23,101 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:38:23,101 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:38:23,278 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 2882 / 2882 = 1.0
2022-09-10 07:38:23,308 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 2885 words in input language, 12 words in output
2022-09-10 07:38:23,316 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:38:26,176 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:38:26,176 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:38:26,255 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:38:26,256 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/asdiv/dense/models/run_cv_asdiv-a_fold3.
2022-09-10 07:38:30,892 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:39:54,665 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.008438818565400843	
 equation acc epoch: 0.0	
 max val acc: 0.008438818565400843	
 equation acc: 0.0	
2022-09-10 07:39:54,665 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 1m 23s
2022-09-10 07:39:54,668 | INFO | main_after.py: 96 : main() ::	 Experiment Name: run_cv_asdiv-a
2022-09-10 07:39:54,668 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:39:54,668 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:39:54,960 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:39:54,960 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 951
2022-09-10 07:39:54,960 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 266
2022-09-10 07:39:54,960 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: []
2022-09-10 07:39:54,960 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:39:54,960 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:39:55,138 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 2882 / 2882 = 1.0
2022-09-10 07:39:55,169 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 2885 words in input language, 12 words in output
2022-09-10 07:39:55,177 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:39:58,129 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:39:58,129 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:39:58,205 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:39:58,205 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/asdiv/dense/models/run_cv_asdiv-a_fold4.
2022-09-10 07:40:03,488 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:40:09,840 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.007518796992481203	
 equation acc epoch: 0.0	
 max val acc: 0.007518796992481203	
 equation acc: 0.0	
2022-09-10 07:40:09,840 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 6s
2022-09-10 07:40:09,840 | INFO | main_after.py: 272 : main() ::	 Final Val score: 0.009860312243221035
Transfer numbers...
initialize by random pruning
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.8659517577671674
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.8667391172178989
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 0.8815104166666666
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8660990397135416
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8655225965711806
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8659752739800347
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8657277425130209
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8661426968044705
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8660587734646268
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8663770887586806
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8656463623046875
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8657023111979166
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8666025797526041
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8658794826931424
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8657095167371962
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.866943359375
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8658972846137153
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8660973442925347
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8661142985026041
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.865889655219184
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.866278330485026
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8659515380859375
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8653733995225694
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8653784857855903
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8653835720486112
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8661778767903646
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8657887776692709
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8652530246310763
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8657633463541666
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8660549587673612
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8648800320095487
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8660121493869357
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8657921685112847
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.86627197265625
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8654988606770834
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8658633761935763
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8663923475477431
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8659833272298177
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8661638895670573
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8657904730902778
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8662787543402778
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8662041558159722
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8661346435546875
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8659549289279513
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8658519321017795
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8659566243489584
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8658362494574653
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8663567437065972
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8658921983506944
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8658370971679688
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8661024305555556
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8661990695529513
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8661668565538194
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8661939832899306
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8657633463541666
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8661164177788628
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8660028245713975
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8655293782552084
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8660176595052084
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8660464816623263
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8655361599392362
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8660651312934028
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8658680386013455
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8665093315972222
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8663872612847222
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8667212592230903
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8661465115017362
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8661927117241753
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8663656446668837
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8663007948133681
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8651038275824653
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8666212293836806
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8655276828342013
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8657819959852431
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8659167819552951
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.8660142686631944
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.8660907745361328
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.8658790588378906
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.8654867808024088
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.8657417297363281
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.8660974502563477
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.866307258605957
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.8658747673034668
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.8662195205688477
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.8647956848144531
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.866546630859375
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.8642463684082031
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.8656978607177734
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.87060546875
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.8671703338623047
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.873046875
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.8659375508626302
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.880859375
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.8639322916666666
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.8660899571010044
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.8663439069475447
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.865936279296875
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.8654556274414062
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.8661564418247768
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.8659624372209822
Final sparsity level of 0.866: 0.8659752718527812
Transfer numbers...
initialize by random pruning
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.8660287714695447
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.8668860449092088
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 0.8684895833333334
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8659278021918403
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8664889865451388
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8653971354166666
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8662228054470487
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.86583497789171
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8657497829861112
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8667093912760416
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8656667073567709
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8662923177083334
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8653717041015625
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8659947713216146
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8659027947319878
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8658108181423612
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8654802110460069
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8652903238932291
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8656599256727431
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8657841152615018
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8661812676323785
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8657972547743056
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8668297661675347
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8662821451822916
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8653293185763888
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8662401835123698
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8660350375705295
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8663533528645834
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8663041856553819
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8660261366102431
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8653649224175347
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8662791781955295
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8662762112087674
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8655056423611112
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8665720621744791
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8662567138671875
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8659600151909722
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.866166008843316
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8661888970269097
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8658667670355903
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8666110568576388
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8660193549262153
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8664109971788194
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8661215040418837
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8664207458496094
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8657430013020834
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8664771185980903
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8663974338107638
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8653954399956597
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8662787543402778
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8662715488009982
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8658718532986112
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8660668267144097
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8666144476996528
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8660396999782987
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8658748202853732
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8661024305555556
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8657701280381944
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8655192057291666
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8660380045572916
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8659803602430556
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8659803602430556
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8657794528537326
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8658955891927084
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8661939832899306
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8665720621744791
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8652733696831597
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8661321004231771
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8659489949544271
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.866546630859375
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8659396701388888
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8653479682074653
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8661549886067709
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.865852779812283
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8659735785590278
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.8653632269965278
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.8658987681070963
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.8660268783569336
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.8658809661865234
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.8657894134521484
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.8663806915283203
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.8659687042236328
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.86572265625
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.8660564422607422
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.866363525390625
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.8656196594238281
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.8662986755371094
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.8664798736572266
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.869384765625
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.865875244140625
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.88671875
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.8662821451822916
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.849609375
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.8671875
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.8659755161830357
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.8656964983258928
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.8661215645926339
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.8666054861886161
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.8656834193638393
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.8661488124302456
Final sparsity level of 0.866: 0.8660306594755935
Transfer numbers...
initialize by random pruning
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.8660063382821048
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.8667745825226978
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 0.8567708333333334
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8657819959852431
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8664177788628472
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8652801513671875
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8657480875651041
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8659392462836372
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8658091227213541
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.865509033203125
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8660464816623263
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8666788736979166
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8651902940538194
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8656798468695747
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.866068098280165
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8663245307074653
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8648325602213541
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8662702772352431
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8660651312934028
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8657031589084201
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.866438971625434
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8657870822482638
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8662957085503472
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8652123345269097
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.866058349609375
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8662906222873263
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8659070332845052
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8665551079644097
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8655022515190972
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8659837510850694
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8668399386935763
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8661028544108073
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8658782111273872
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8654446072048612
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8658599853515625
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8661515977647569
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8667432996961806
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8659294976128472
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8659142388237847
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8659311930338541
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8662245008680556
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.866607666015625
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8664160834418403
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8659845987955729
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.866043938530816
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8662567138671875
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8661329481336806
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8655565049913194
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8656531439887153
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.86566162109375
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8657582600911459
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.866668701171875
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8653835720486112
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8662685818142362
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8662194146050347
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8658091227213541
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8660320705837674
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8659311930338541
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8662058512369791
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8657684326171875
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8651936848958334
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8661448160807291
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8658269246419271
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8651529947916666
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8666788736979166
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8664093017578125
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8665873209635416
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8662117852105035
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8658171759711372
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8660244411892362
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8663363986545138
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8664889865451388
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8655022515190972
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8660630120171441
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8659545050726997
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.8656005859375
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.8659833272298177
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.8662014007568359
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.8660742441813151
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.865818977355957
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.8660359382629395
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.8664312362670898
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.8660416603088379
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.8663511276245117
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.8667144775390625
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.8654880523681641
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.8648338317871094
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.865203857421875
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.8642578125
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.8662204742431641
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.90625
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.86578369140625
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.865234375
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.8701171875
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.8655766078404018
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.8663657052176339
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.8661869594029018
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.8663046700613839
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.8659101213727678
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.8659286499023438
Final sparsity level of 0.866: 0.8659895600734393
Transfer numbers...
initialize by random pruning
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.8660634056168971
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.86689364461738
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 0.8658854166666666
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8660193549262153
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8663652208116319
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8668009440104166
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8662245008680556
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8662423027886285
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8661015828450521
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8657887776692709
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8662058512369791
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8655463324652778
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8660566541883681
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8662359449598525
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.865875244140625
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8656938340928819
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8661465115017362
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.86578369140625
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8654734293619791
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8660761515299479
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8655734592013888
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8660329182942709
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8662872314453125
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8661142985026041
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8660125732421875
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8661159939236112
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8658989800347222
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8669399685329862
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8667178683810763
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8661312527126737
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8655039469401041
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8659617106119791
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8661452399359809
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8659871419270834
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8661092122395834
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8654903835720487
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8662075466579862
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8660511440700955
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8658697340223525
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.865386962890625
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8663194444444444
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8659447564019097
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8658871120876737
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.866119384765625
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8664495680067275
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8658905029296875
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8665195041232638
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8661261664496528
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8662228054470487
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.86585447523329
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8657133314344618
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8660532633463541
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8655361599392362
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8662295871310763
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8657090928819444
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8654674953884549
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8657455444335938
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8664313422309028
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8656277126736112
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8657582600911459
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8656972249348959
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8663609822591146
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8660939534505209
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8656887478298612
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8654598659939237
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8657023111979166
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8652394612630209
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8659782409667969
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8658883836534288
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8659142388237847
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8662058512369791
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8672671847873263
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8657599555121528
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8660129970974393
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8661066691080729
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.8660736083984375
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.8659674326578776
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.8663702011108398
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.865447998046875
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.8655376434326172
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.8664135932922363
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.8662118911743164
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.8662924766540527
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.8667869567871094
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.8657989501953125
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.8664264678955078
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.8659477233886719
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.8662166595458984
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.867431640625
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.8667449951171875
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.888671875
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.8661422729492188
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.8671875
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.8776041666666666
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.8658360072544643
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.865797860281808
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.865509033203125
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.8656256539481026
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.8658490862165178
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.8659896850585938
Final sparsity level of 0.866: 0.8660269143557409
Transfer numbers...
initialize by random pruning
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.8660406874813489
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.8662046044098574
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 0.859375
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8660634358723959
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8662634955512153
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8648240831163194
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8661363389756944
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8660591973198785
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8659532335069444
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8667195638020834
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8658837212456597
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8660922580295138
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8658091227213541
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8659405178493924
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8658667670355903
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8656480577256944
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8664398193359375
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8658955891927084
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8657480875651041
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.866096920437283
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8655446370442709
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8653835720486112
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.866729736328125
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8667500813802084
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8649529351128472
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8661312527126737
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8658481174045138
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8654836018880209
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8659752739800347
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8647935655381944
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8663211398654513
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8660524156358507
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8657913208007812
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8662584092881944
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8661973741319444
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8659769694010416
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8662295871310763
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8660803900824653
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8659006754557291
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8656684027777778
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8659956190321181
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8665228949652778
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8668314615885416
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8661685519748263
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8658854166666666
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8651902940538194
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.866119384765625
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8666364881727431
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8659583197699653
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8658943176269531
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8657353719075521
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8660091824001737
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8662872314453125
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8653072781032987
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8655429416232638
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8661541408962674
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8661198086208768
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8663279215494791
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8663855658637153
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8665313720703125
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8662770589192709
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.866251203748915
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8657328287760416
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8658447265625
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8660481770833334
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8662329779730903
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8646511501736112
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8662198384602865
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8658443027072482
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8664703369140625
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.865997314453125
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8663465711805556
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8656175401475694
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8663334316677518
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8658794826931424
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.8661448160807291
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.8664531707763672
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.8661966323852539
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.8662185668945312
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.8658962249755859
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.8658456802368164
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.865574836730957
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.8663229942321777
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.8660993576049805
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.8654594421386719
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.8669395446777344
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.8657875061035156
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.8659992218017578
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.86669921875
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.8667411804199219
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.85546875
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.8666280110677084
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.890625
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.8678385416666666
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.8659330095563615
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.8655678885323661
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.8661793300083706
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.866100856236049
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.8662523542131697
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.8659918648856026
Final sparsity level of 0.866: 0.8660175065592901
wandb: WARNING W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.
2022-09-10 07:40:12,986 | INFO | main_after.py: 96 : main() ::	 Experiment Name: run_cv_asdiv-a
2022-09-10 07:40:12,986 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:40:12,986 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:40:13,368 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:40:13,368 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 979
2022-09-10 07:40:13,368 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 238
2022-09-10 07:40:13,368 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: []
2022-09-10 07:40:13,368 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:40:13,368 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:40:13,548 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 2882 / 2882 = 1.0
2022-09-10 07:40:13,579 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 2885 words in input language, 12 words in output
2022-09-10 07:40:13,586 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:40:16,609 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:40:16,609 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:40:20,051 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:40:20,052 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/asdiv/dense/models/run_cv_asdiv-a_fold0.
2022-09-10 07:40:25,440 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:41:22,609 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.025210084033613446	
 equation acc epoch: 0.0	
 max val acc: 0.025210084033613446	
 equation acc: 0.0	
2022-09-10 07:41:22,609 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 57s
2022-09-10 07:41:22,611 | INFO | main_after.py: 96 : main() ::	 Experiment Name: run_cv_asdiv-a
2022-09-10 07:41:22,611 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:41:22,612 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:41:22,996 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:41:22,996 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 979
2022-09-10 07:41:22,996 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 238
2022-09-10 07:41:22,997 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: []
2022-09-10 07:41:22,997 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:41:22,997 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:41:23,170 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 2882 / 2882 = 1.0
2022-09-10 07:41:23,200 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 2885 words in input language, 12 words in output
2022-09-10 07:41:23,208 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:41:26,223 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:41:26,223 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:41:26,303 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:41:26,303 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/asdiv/dense/models/run_cv_asdiv-a_fold1.
2022-09-10 07:41:28,608 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:42:26,581 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.02100840336134454	
 equation acc epoch: 0.0	
 max val acc: 0.02100840336134454	
 equation acc: 0.0	
2022-09-10 07:42:26,581 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 57s
2022-09-10 07:42:26,584 | INFO | main_after.py: 96 : main() ::	 Experiment Name: run_cv_asdiv-a
2022-09-10 07:42:26,584 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:42:26,584 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:42:26,877 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:42:26,877 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 979
2022-09-10 07:42:26,877 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 238
2022-09-10 07:42:26,877 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: []
2022-09-10 07:42:26,877 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:42:26,877 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:42:27,050 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 2882 / 2882 = 1.0
2022-09-10 07:42:27,081 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 2885 words in input language, 12 words in output
2022-09-10 07:42:27,090 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:42:29,973 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:42:29,974 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:42:30,053 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:42:30,053 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/asdiv/dense/models/run_cv_asdiv-a_fold2.
2022-09-10 07:42:31,867 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:43:27,774 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.008403361344537815	
 equation acc epoch: 0.0	
 max val acc: 0.008403361344537815	
 equation acc: 0.0	
2022-09-10 07:43:27,775 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 55s
2022-09-10 07:43:27,778 | INFO | main_after.py: 96 : main() ::	 Experiment Name: run_cv_asdiv-a
2022-09-10 07:43:27,778 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:43:27,778 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:43:28,069 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:43:28,070 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 980
2022-09-10 07:43:28,070 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 237
2022-09-10 07:43:28,070 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: []
2022-09-10 07:43:28,070 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:43:28,070 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:43:28,244 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 2882 / 2882 = 1.0
2022-09-10 07:43:28,356 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 2885 words in input language, 12 words in output
2022-09-10 07:43:28,364 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:43:31,264 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:43:31,264 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:43:31,341 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:43:31,341 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/asdiv/dense/models/run_cv_asdiv-a_fold3.
2022-09-10 07:43:33,872 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:44:38,790 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.004219409282700422	
 equation acc epoch: 0.0	
 max val acc: 0.004219409282700422	
 equation acc: 0.0	
2022-09-10 07:44:38,790 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 1m 4s
2022-09-10 07:44:38,794 | INFO | main_after.py: 96 : main() ::	 Experiment Name: run_cv_asdiv-a
2022-09-10 07:44:38,794 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:44:38,794 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:44:39,086 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:44:39,086 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 951
2022-09-10 07:44:39,086 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 266
2022-09-10 07:44:39,086 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: []
2022-09-10 07:44:39,086 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:44:39,086 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:44:39,261 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 2882 / 2882 = 1.0
2022-09-10 07:44:39,291 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 2885 words in input language, 12 words in output
2022-09-10 07:44:39,299 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:44:42,174 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:44:42,175 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:44:42,251 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:44:42,251 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/asdiv/dense/models/run_cv_asdiv-a_fold4.
2022-09-10 07:44:45,951 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:44:52,375 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.007518796992481203	
 equation acc epoch: 0.0	
 max val acc: 0.007518796992481203	
 equation acc: 0.0	
2022-09-10 07:44:52,375 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 6s
2022-09-10 07:44:52,375 | INFO | main_after.py: 272 : main() ::	 Final Val score: 0.01314708299096138
Transfer numbers...
initialize by random pruning
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.8929560827613647
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.8935838197146563
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 0.9036458333333334
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8929426405164931
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8923085530598959
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8929426405164931
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8928205702039931
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8929757012261285
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8931117587619357
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8937717013888888
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8926764594184028
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8927035861545138
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8938344319661459
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8927353752983941
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8928176032172309
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8935869004991319
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8927781846788194
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8928409152560763
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8930155436197916
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.892904069688585
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8932707044813368
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8930765787760416
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8924916585286459
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8926255967881944
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8928748236762153
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.893013424343533
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8927010430230035
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8924051920572916
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8927629258897569
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8931596544053819
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8924577501085069
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8929426405164931
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.892843034532335
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8931325276692709
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8925526936848959
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8928646511501737
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8931952582465278
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8929532368977865
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.893021477593316
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8927866617838541
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8929290771484375
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8935004340277778
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8929799397786459
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8930045233832465
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8924577501085069
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8932664659288194
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8927849663628472
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8934326171875
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8929443359375
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8929642571343316
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8929723103841146
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8934665256076388
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8932545979817709
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8932206895616319
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8929019504123263
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8932274712456597
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8928799099392362
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.892730712890625
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8929104275173612
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8931833902994791
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.892669677734375
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.893133799235026
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8928667704264323
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8935665554470487
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8931494818793403
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8936394585503472
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8930460611979166
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.893113030327691
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8933982849121094
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8931443956163194
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8922593858506944
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8938496907552084
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8926984998914931
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8929265340169271
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8929608662923177
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.8929358588324653
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.8930002848307291
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.8929653167724609
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.8926499684651693
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.8925094604492188
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.8931937217712402
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.8931140899658203
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.8930511474609375
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.8931989669799805
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.8923187255859375
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.8933086395263672
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.8918724060058594
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.8929615020751953
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.897705078125
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.8941097259521484
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.90234375
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.892907460530599
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.90234375
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.8922526041666666
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.8928571428571429
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.893385750906808
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.8931590488978794
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.8922991071428571
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.893202645438058
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.8931644984654018
Final sparsity level of 0.893: 0.8929771221773565
Transfer numbers...
initialize by random pruning
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.8930474474866541
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.89364968385214
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 0.8932291666666666
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8928731282552084
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8934851752387153
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8923441569010416
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8931833902994791
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8927900526258681
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8927417331271701
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.89349365234375
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8923729790581597
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8930138481987847
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8922966851128472
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8930617438422309
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8930032518174913
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8926628960503472
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8926713731553819
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8924730088975694
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8926815456814237
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8929977416992188
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8932083977593316
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8926883273654513
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8933444552951388
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8928799099392362
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8924068874782987
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8932105170355903
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8932050069173177
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8933749728732638
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8933054606119791
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8930223253038194
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8923441569010416
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8933364020453559
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8930011325412326
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8925001356336806
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8933648003472222
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8931783040364584
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8927781846788194
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.893149905734592
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8930286831325955
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8926222059461806
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8934614393446181
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8932545979817709
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8933664957682291
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8929875691731771
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8933071560329862
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.892791748046875
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8932817247178819
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8933953179253472
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8923221164279513
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8932923210991753
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.893304189046224
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8929070366753472
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8932952880859375
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8933885362413194
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8930070665147569
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8927900526258681
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8931456671820747
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8929307725694444
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8924577501085069
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8934071858723959
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8929799397786459
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8931083679199219
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8929163614908854
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8923848470052084
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.892791748046875
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8937106662326388
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8924814860026041
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8931901719835069
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8930837843153212
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8933139377170138
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8929697672526041
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8926561143663194
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8930545383029513
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8928909301757812
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8929269578721788
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.8928595648871528
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.8929341634114584
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.8933343887329102
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.8929144541422526
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.8930187225341797
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.8935532569885254
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.893315315246582
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.8927512168884277
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.8929147720336914
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.8936843872070312
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.89276123046875
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.8933181762695312
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.8932876586914062
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.891845703125
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.89312744140625
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.90625
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.8931083679199219
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.8828125
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.8987630208333334
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.8930108206612724
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.8928658621651786
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.8929192679268974
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.8933149065290179
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.8925552368164062
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.893324715750558
Final sparsity level of 0.893: 0.8930390239550327
Transfer numbers...
initialize by random pruning
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.8929913645180543
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.8934394252594033
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 0.890625
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8927476671006944
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8933054606119791
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8925069173177084
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8931054009331597
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8930024041069878
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8928137885199653
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8925221761067709
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8930884467230903
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8936530219184028
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8922492133246528
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8925696478949653
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8929528130425347
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.893463134765625
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.892364501953125
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8933393690321181
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8930647108289931
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8926908704969618
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8932813008626302
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8926984998914931
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8933444552951388
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8921695285373263
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8930341932508681
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8932274712456597
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8928116692437066
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8934750027126737
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8928358289930556
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8930714925130209
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8936852349175347
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8931138780381944
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8929142422146268
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8924018012152778
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8927086724175347
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8931477864583334
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8937242296006944
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8930528428819444
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8928197224934896
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8925679524739584
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8930341932508681
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8934563530815972
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8933258056640625
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8929311964246962
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8929638332790799
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8930494520399306
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8934444851345487
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8925306532118056
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8928036159939237
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8926450941297743
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8928938971625434
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8934529622395834
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8924696180555556
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8930341932508681
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8931528727213541
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8929617140028212
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8930948045518663
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8927883572048612
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.893463134765625
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8925391303168403
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8923068576388888
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8930981953938802
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8929519653320312
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8924696180555556
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8935207790798612
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8931799994574653
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8935631646050347
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8931872049967448
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8929273817274306
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8925238715277778
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8932257758246528
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8933936225043403
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8924679226345487
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8930435180664062
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8930625915527344
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.8930036756727431
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.8929297129313151
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.8932199478149414
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.8931090037027994
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.8928461074829102
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.8931894302368164
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.893488883972168
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.8929104804992676
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.8929824829101562
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.8928489685058594
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.8923549652099609
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.8917694091796875
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.8921222686767578
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.894775390625
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.8930683135986328
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.93359375
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.8922742207845052
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.896484375
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.8977864583333334
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.8926718575613839
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.8934969220842633
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.8933149065290179
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.8931121826171875
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.8930140904017857
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.8930489676339286
Final sparsity level of 0.893: 0.8929791297277396
Transfer numbers...
initialize by random pruning
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.893081796685898
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.893791545071336
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 0.9049479166666666
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8927527533637153
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8931393093532987
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8937106662326388
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8931172688802084
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8932122124565972
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8931744893391927
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8928239610460069
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8932918972439237
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8927595350477431
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8930528428819444
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.89325926038954
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8930159674750434
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8930562337239584
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.893096923828125
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8926595052083334
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8927527533637153
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.892960442437066
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.892785390218099
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8929799397786459
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8933851453993056
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8931087917751737
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8927680121527778
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8932537502712674
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8929591708713107
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8939954969618056
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8935258653428819
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8928697374131944
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8927442762586806
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8929689195421007
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8932016160753038
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8929646809895834
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8934970431857638
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8926459418402778
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8931494818793403
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8930146959092882
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8927332560221354
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8922949896918403
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8935801188151041
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8929816351996528
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8929222954644097
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8931448194715712
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.893270280626085
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8931155734592013
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8938073052300347
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8929646809895834
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8933987087673612
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8928837246365018
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8927158779568143
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8927849663628472
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8926035563151041
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8932037353515625
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8927544487847222
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8924869961208768
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8924806382921007
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8929985894097222
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8925764295789931
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8926628960503472
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8926171196831597
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8932885064019097
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8930490281846788
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8927934434678819
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8926781548394097
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8928680419921875
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8921966552734375
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8929417928059896
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8930863274468316
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8927188449435763
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8933631049262153
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8939887152777778
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8928782145182291
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8927489386664497
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8931236267089844
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.8932461208767362
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.8931598663330078
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.8933696746826172
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.8926722208658854
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.8926477432250977
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.8934116363525391
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.8934831619262695
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.8934359550476074
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.8937444686889648
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.8926773071289062
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.8933067321777344
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.8926773071289062
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.8934669494628906
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.8935546875
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.89398193359375
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.90234375
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.8929036458333334
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.8984375
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.8997395833333334
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.8930119105747768
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.8931535993303571
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.8926598685128349
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.8926064627511161
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.892930167061942
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.8928680419921875
Final sparsity level of 0.893: 0.8930431290494368
Transfer numbers...
initialize by random pruning
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.8930284336765145
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.8931202375162127
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 0.89453125
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8928595648871528
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8930782741970487
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8920932345920138
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8931477864583334
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8929689195421007
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8930142720540365
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8936767578125
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8930291069878472
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8933020697699653
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8931257459852431
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.892928229437934
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.892767588297526
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8926968044704862
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8934563530815972
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8929206000434028
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8927171495225694
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8930990431043837
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8926124572753906
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8925052218967013
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8933241102430556
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8932986789279513
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8924848768446181
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.893021477593316
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.892876942952474
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8925018310546875
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8928307427300347
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.892181396484375
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8931715223524306
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8930625915527344
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8929850260416666
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8931020100911459
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.892852783203125
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8928900824652778
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8934088812934028
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8931935628255209
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8930049472384982
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.892913818359375
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8930511474609375
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8931630452473959
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8932478162977431
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8932613796657987
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8928849962022569
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8921881781684028
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8929477267795138
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8937615288628472
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8928426106770834
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8928557501898872
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8928989834255643
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8932834201388888
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8927442762586806
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8923560248480903
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8925238715277778
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8931244744194878
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8931956821017795
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8931901719835069
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8935038248697916
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8937327067057291
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8929307725694444
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8932783338758681
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8928743998209635
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8925984700520834
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8932461208767362
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8931342230902778
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8919186062282987
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.893074459499783
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8926790025499132
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8929239908854166
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8927849663628472
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8934088812934028
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8928205702039931
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8934330410427518
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8929371303982205
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.8930782741970487
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.8934892018636068
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.8929538726806641
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.8932425181070963
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.8929824829101562
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.8927273750305176
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.8925580978393555
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.8931694030761719
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.8929605484008789
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.8922538757324219
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.8938083648681641
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.8931655883789062
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.8932876586914062
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.893798828125
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.8937625885009766
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.884765625
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.8935534159342448
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.908203125
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.8922526041666666
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.8932429722377232
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.8927764892578125
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.8932255336216518
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.8932745797293526
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.8934729439871651
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.8931296212332589
Final sparsity level of 0.893: 0.8930184777152492

JOB STATISTICS
==============
Job ID: 1507122
Cluster: snellius
User/Group: sliu/sliu
State: RUNNING
Nodes: 1
Cores per node: 18
CPU Utilized: 00:00:00
CPU Efficiency: 0.00% of 07:26:06 core-walltime
Job Wall-clock time: 00:24:47
Memory Utilized: 0.00 MB (estimated maximum)
Memory Efficiency: 0.00% of 0.00 MB (0.00 MB/node)
WARNING: Efficiency statistics may be misleading for RUNNING jobs.
