wandb: WARNING W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.
2022-09-10 07:21:14,291 | INFO | main_after.py: 96 : main() ::	 Experiment Name: cv_svamp_augmented
2022-09-10 07:21:14,291 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:21:14,291 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:21:15,329 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:21:15,329 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 3932
2022-09-10 07:21:15,329 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 206
2022-09-10 07:21:15,329 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: ['0.01', '12.0', '1.0', '100.0', '0.1', '0.5', '3.0', '4.0', '7.0']
2022-09-10 07:21:15,329 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:21:15,329 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:21:15,911 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 4068 / 4068 = 1.0
2022-09-10 07:21:16,077 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 4071 words in input language, 21 words in output
2022-09-10 07:21:16,086 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:21:19,045 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:21:19,046 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:21:22,352 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:21:22,353 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/svamp/dense/models/cv_svamp_augmented_fold0.
2022-09-10 07:21:26,696 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:21:33,546 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.019417475728155338	
 equation acc epoch: 0.019417475728155338	
 max val acc: 0.019417475728155338	
 equation acc: 0.019417475728155338	
2022-09-10 07:21:33,546 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 6s
2022-09-10 07:21:33,548 | INFO | main_after.py: 96 : main() ::	 Experiment Name: cv_svamp_augmented
2022-09-10 07:21:33,548 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:21:33,548 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:21:34,506 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:21:34,507 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 3920
2022-09-10 07:21:34,507 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 218
2022-09-10 07:21:34,507 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: ['0.01', '12.0', '1.0', '100.0', '0.1', '0.5', '3.0', '4.0', '7.0']
2022-09-10 07:21:34,507 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:21:34,507 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:21:35,079 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 4068 / 4068 = 1.0
2022-09-10 07:21:35,241 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 4071 words in input language, 21 words in output
2022-09-10 07:21:35,252 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:21:38,197 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:21:38,197 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:21:38,276 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:21:38,276 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/svamp/dense/models/cv_svamp_augmented_fold1.
2022-09-10 07:21:43,925 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:21:50,861 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.06422018348623854	
 equation acc epoch: 0.04128440366972477	
 max val acc: 0.06422018348623854	
 equation acc: 0.04128440366972477	
2022-09-10 07:21:50,861 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 6s
2022-09-10 07:21:50,863 | INFO | main_after.py: 96 : main() ::	 Experiment Name: cv_svamp_augmented
2022-09-10 07:21:50,864 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:21:50,864 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:21:51,822 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:21:51,823 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 3911
2022-09-10 07:21:51,823 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 227
2022-09-10 07:21:51,823 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: ['0.01', '12.0', '1.0', '100.0', '0.1', '0.5', '3.0', '4.0', '7.0']
2022-09-10 07:21:51,823 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:21:51,823 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:21:52,406 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 4068 / 4068 = 1.0
2022-09-10 07:21:52,573 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 4071 words in input language, 21 words in output
2022-09-10 07:21:52,586 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:21:55,400 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:21:55,401 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:21:55,477 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:21:55,478 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/svamp/dense/models/cv_svamp_augmented_fold2.
2022-09-10 07:22:00,980 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:22:08,259 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.04405286343612335	
 equation acc epoch: 0.013215859030837005	
 max val acc: 0.04405286343612335	
 equation acc: 0.013215859030837005	
2022-09-10 07:22:08,259 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 7s
2022-09-10 07:22:08,261 | INFO | main_after.py: 96 : main() ::	 Experiment Name: cv_svamp_augmented
2022-09-10 07:22:08,261 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:22:08,261 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:22:09,325 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:22:09,325 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 3973
2022-09-10 07:22:09,325 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 165
2022-09-10 07:22:09,325 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: ['0.01', '12.0', '1.0', '100.0', '0.1', '0.5', '3.0', '4.0', '7.0']
2022-09-10 07:22:09,325 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:22:09,326 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:22:09,903 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 4068 / 4068 = 1.0
2022-09-10 07:22:10,070 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 4071 words in input language, 21 words in output
2022-09-10 07:22:10,080 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:22:12,927 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:22:12,927 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:22:13,004 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:22:13,004 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/svamp/dense/models/cv_svamp_augmented_fold3.
2022-09-10 07:22:18,348 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:22:23,486 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.030303030303030304	
 equation acc epoch: 0.024242424242424242	
 max val acc: 0.030303030303030304	
 equation acc: 0.024242424242424242	
2022-09-10 07:22:23,486 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 5s
2022-09-10 07:22:23,488 | INFO | main_after.py: 96 : main() ::	 Experiment Name: cv_svamp_augmented
2022-09-10 07:22:23,488 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:22:23,488 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:22:24,528 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:22:24,528 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 3954
2022-09-10 07:22:24,528 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 184
2022-09-10 07:22:24,529 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: ['0.01', '12.0', '1.0', '100.0', '0.1', '0.5', '3.0', '4.0', '7.0']
2022-09-10 07:22:24,529 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:22:24,529 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:22:25,112 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 4068 / 4068 = 1.0
2022-09-10 07:22:25,280 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 4071 words in input language, 21 words in output
2022-09-10 07:22:25,290 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:22:28,134 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:22:28,135 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:22:28,212 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:22:28,212 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/svamp/dense/models/cv_svamp_augmented_fold4.
2022-09-10 07:22:33,520 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:22:40,612 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.005434782608695652	
 equation acc epoch: 0.0	
 max val acc: 0.005434782608695652	
 equation acc: 0.0	
2022-09-10 07:22:40,612 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 7s
2022-09-10 07:22:40,612 | INFO | main_after.py: 272 : main() ::	 Final Val score: 0.034
Transfer numbers...
initialize by random pruning
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.19998085666799292
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.20218010295071331
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 0.21744791666666663
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.19980027940538192
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.19935268825954866
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.19986300998263884
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.2009735107421875
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.1998638576931424
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.19996134440104163
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.19958835177951384
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.19973246256510413
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.20011732313368058
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.20002237955729163
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.1998269822862413
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.19940524631076384
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.20061069064670134
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.19978841145833337
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.19970872667100692
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.1995459662543403
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.19988716973198783
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.20001941257052946
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.20003255208333337
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.20043606228298616
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.2000969780815972
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.20067342122395837
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.2001431783040365
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.20014699300130212
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.20010545518663192
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.19960361056857634
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.19945271809895837
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.19839138454861116
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.1999969482421875
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.2003631591796875
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.20012919108072913
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.20053270128038192
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.20042249891493058
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.2001410590277778
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.19986936781141496
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.19967608981662321
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.2003021240234375
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.2007836235894097
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.19945271809895837
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.19980367024739587
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.1999579535590278
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.20018598768446183
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.19931369357638884
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.19894239637586808
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.200103759765625
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.1996307373046875
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.20041656494140625
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.19990285237630212
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.2000579833984375
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.2001800537109375
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.19951375325520837
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.20023939344618058
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.20016437106662321
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.19958962334526908
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.19991895887586808
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.1996307373046875
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.1992730034722222
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.20039706759982634
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.2000753614637587
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.19969982571072054
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.19978502061631942
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.19961378309461808
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.2008446587456597
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.20048862033420134
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.19984393649631071
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.19980621337890625
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.1998443603515625
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.1999053955078125
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.19960530598958337
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.2000054253472222
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.2001258002387153
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.20022625393337679
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.2004174126519097
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.19933128356933594
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.1996622085571289
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.1999073028564453
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.20039653778076172
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.19957733154296875
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.19986248016357422
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.2002091407775879
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.2000885009765625
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.199615478515625
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.20110702514648438
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.19890975952148438
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.2004852294921875
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.204345703125
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.19979095458984375
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.201171875
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.19937260945638025
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.208984375
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.19140625
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.20076969691685265
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.20004163469587055
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.2001386369977679
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.19911302839006695
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.20067596435546875
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.19978877476283485
Final sparsity level of 0.2: 0.1999665279047842
Transfer numbers...
initialize by random pruning
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.19993368480387286
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.20138466682879375
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 0.22526041666666663
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.19979688856336808
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.20073276095920134
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.20050387912326384
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.2006751166449653
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.20004145304361975
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.19952307807074654
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.2001953125
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.2002800835503472
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.20126512315538192
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.2002020941840278
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.2003851996527778
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.1994760301378038
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.1997307671440972
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.20126512315538192
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.19906277126736116
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.19908650716145837
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.20003339979383683
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.19999143812391496
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.19921535915798616
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.20034450954861116
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.19973924424913192
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.1999274359809028
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.20002449883355033
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.19994608561197913
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.19987148708767366
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.1993187798394097
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.19986979166666663
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.19934251573350692
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.19983376397026908
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.19988928900824654
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.20000203450520837
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.1998070610894097
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.19908481174045134
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.20025804307725692
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.19993845621744788
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.200066884358724
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.19976298014322913
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.19910685221354163
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.19993082682291663
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.20013088650173616
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.19974221123589408
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.19988674587673616
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.19976806640625
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.20096842447916663
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.20136006673177087
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.20043436686197913
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.20004823472764754
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.19996388753255212
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.20013258192274308
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.20070054796006942
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.20007154676649308
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.19954935709635413
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.19958665635850692
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.19997617933485246
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.19994099934895837
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.200439453125
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.20008341471354163
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.1998680962456597
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.2000338236490885
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.19991260104709196
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.19987318250868058
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.2005072699652778
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.20056321885850692
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.19993252224392366
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.19996176825629342
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.2000495062934028
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.19971720377604163
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.2006072998046875
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.19982401529947913
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.20012240939670134
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.19957097371419275
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.20013682047526038
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.2006293402777778
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.2000446319580078
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.20032215118408203
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.19969050089518225
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.19946765899658203
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.2005758285522461
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.1998310089111328
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.2004685401916504
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.19952774047851562
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.1988372802734375
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.2006683349609375
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.19981765747070312
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.19987106323242188
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.203125
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.19981956481933594
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.205078125
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.19954172770182288
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.205078125
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.20149739583333337
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.20056697300502235
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.19944218226841515
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.1993756975446429
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.20047760009765625
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.19908687046595985
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.1994454520089286
Final sparsity level of 0.2: 0.19996097598958662
Transfer numbers...
initialize by random pruning
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.1999965288139527
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.20106801232166016
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 0.20442708333333337
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.19999864366319442
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.2002190483940972
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.20031568739149308
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.19999186197916663
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.20011096530490446
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.200127919514974
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.20035129123263884
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.20026991102430558
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.19988674587673616
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.19841512044270837
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.20021099514431429
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.2002347310384115
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.19955105251736116
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.19984775119357634
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.20013088650173616
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.2007225884331597
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.20000669691297746
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.2000927395290799
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.19994778103298616
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.20038859049479163
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.19896443684895837
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.20003255208333337
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.1999859280056424
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.19998253716362846
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.1986168755425347
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.19951036241319442
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.2009497748480903
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.20011393229166663
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.2002190483940972
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.19999567667643225
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.1999749077690972
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.20068020290798616
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.20007832845052087
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.20071919759114587
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.19967778523763025
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.2001516554090712
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.20075480143229163
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.19946628146701384
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.20046827528211808
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.20028856065538192
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.19965871175130212
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.2000948588053385
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.19982740614149308
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.20069546169704866
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.20042588975694442
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.20051066080729163
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.19951629638671875
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.19968244764539933
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.19957139756944442
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.19975789388020837
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.20045979817708337
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.1995459662543403
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.19972822401258683
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.20008892483181429
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.20009358723958337
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.19992574055989587
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.19988674587673616
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.2000885009765625
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.19977060953776038
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.19979137844509554
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.19964090983072913
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.20075480143229163
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.20121426052517366
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.19976976182725692
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.19983927408854163
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.20002661810980904
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.2007514105902778
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.1997918023003472
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.2000579833984375
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.20089213053385413
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.20019361707899308
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.20009019639756942
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.2000342475043403
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.2000255584716797
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.2000732421875
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.1995372772216797
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.1999969482421875
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.20042800903320312
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.19945240020751953
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.1995849609375
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.2002401351928711
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.20072174072265625
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.1994800567626953
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.19839859008789062
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.19901275634765625
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.191162109375
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.2003612518310547
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.171875
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.19975789388020837
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.177734375
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.19140625
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.1997157505580357
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.19988686697823665
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.20050811767578125
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.19968632289341515
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.1997353690011161
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.1994519914899554
Final sparsity level of 0.2: 0.19998755872569451
Transfer numbers...
initialize by random pruning
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.19992865935541626
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.2021446376459144
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 0.21484375
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.19950358072916663
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.20031907823350692
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.19969007703993058
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.19915771484375
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.20020675659179688
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.19991260104709196
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.20000712076822913
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.20041402180989587
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.1994696723090278
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.1996002197265625
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.19953621758355033
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.2002495659722222
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.20096842447916663
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.20039876302083337
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.19972059461805558
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.20003085666232634
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.19958453708224821
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.20005628797743058
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.1990271674262153
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.19949001736111116
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.20023939344618058
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.2002037896050347
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.20020718044704866
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.1999905904134115
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.20045810275607634
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.20026143391927087
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.2004547119140625
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.1998816596137153
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.20018259684244788
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.19999440511067712
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.19937472873263884
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.20026652018229163
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.2001885308159722
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.1997833251953125
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.20034535725911462
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.20000415378146696
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.19944254557291663
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.19977823893229163
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.19945102267795134
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.19911024305555558
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.20024490356445312
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.20034450954861116
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.2010277642144097
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.20028177897135413
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.20016649034288192
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.2010260687934028
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.20022667778862846
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.20013088650173616
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.2005767822265625
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.199493408203125
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.20035129123263884
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.19960191514756942
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.1998507181803385
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.1998269822862413
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.19910685221354163
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.19958835177951384
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.19955105251736116
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.2005225287543403
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.20011647542317712
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.20005035400390625
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.19997999403211808
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.20050218370225692
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.19985792371961808
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.1996849907769097
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.19989013671875
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.20019997490776908
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.200592041015625
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.2005988226996528
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.2002648247612847
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.19989352756076384
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.1999621921115451
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.19971169365776908
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.20005289713541663
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.20039049784342444
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.19945812225341797
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.19970130920410156
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.19979476928710938
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.20037412643432617
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.20006752014160156
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.1996474266052246
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.2007770538330078
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.20102310180664062
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.1994037628173828
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.20006179809570312
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.20031356811523438
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.18994140625
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.20028305053710938
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.23828125
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.20055643717447913
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.203125
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.19791666666666663
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.1998944963727679
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.1995871407645089
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.20022038051060265
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.1994759695870536
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.200103759765625
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.19983019147600445
Final sparsity level of 0.2: 0.19999916098239157
Transfer numbers...
initialize by random pruning
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.20006929419907826
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.20140746595330739
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 0.20572916666666663
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.2002648247612847
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.19958665635850692
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.19967990451388884
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.2001953125
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.20016352335611975
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.20009401109483504
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.20032755533854163
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.19996473524305558
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.19938998752170134
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.19941202799479163
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.1999291314019097
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.20008129543728304
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.1995391845703125
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.20008680555555558
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.19869825575086808
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.19968159993489587
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.19970364040798616
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.19985622829861116
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.1998375786675347
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.1986779106987847
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.2005700005425347
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.19959513346354163
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.19970321655273438
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.20018259684244788
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.19940694173177087
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.2005700005425347
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.19999864366319442
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.19956631130642366
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.19928953382703996
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.19978205362955725
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.1996002197265625
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.1992577446831597
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.19977823893229163
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.19999864366319442
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.20037333170572913
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.20007451375325525
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.19930860731336808
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.1998528374565972
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.1990288628472222
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.19985622829861116
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.1997812059190538
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.1998380025227865
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.20013597276475692
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.19920857747395837
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.2002326117621528
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.20032925075954866
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.20028177897135413
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.20019065009223092
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.20013427734375
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.2014007568359375
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.20029025607638884
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.19983927408854163
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.19983249240451384
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.1997595893012153
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.1997155083550347
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.2003173828125
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.19936116536458337
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.20001051161024308
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.20013682047526038
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.19990073310004342
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.2002410888671875
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.20074801974826384
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.2000359429253472
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.19991556803385413
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.20039325290256071
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.19987148708767366
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.199920654296875
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.19966295030381942
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.2005700005425347
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.19980027940538192
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.1999838087293837
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.19990412394205725
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.1998053656684028
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.20022646586100257
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.2000722885131836
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.20021820068359375
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.19992446899414062
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.19998884201049805
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.1999034881591797
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.20023298263549805
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.1998138427734375
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.19989776611328125
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.20041275024414062
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.20188522338867188
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.20086669921875
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.19677734375
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.20070648193359375
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.181640625
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.19999186197916663
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.197265625
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.1953125
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.20053427559988835
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.2001124790736607
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.20039040701729915
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.20031411307198665
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.20004817417689735
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.2001059395926339
Final sparsity level of 0.2: 0.20000656815104667
wandb: WARNING W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.
2022-09-10 07:22:43,637 | INFO | main_after.py: 96 : main() ::	 Experiment Name: cv_svamp_augmented
2022-09-10 07:22:43,638 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:22:43,638 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:22:44,676 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:22:44,676 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 3932
2022-09-10 07:22:44,676 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 206
2022-09-10 07:22:44,676 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: ['0.01', '12.0', '1.0', '100.0', '0.1', '0.5', '3.0', '4.0', '7.0']
2022-09-10 07:22:44,676 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:22:44,676 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:22:45,252 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 4068 / 4068 = 1.0
2022-09-10 07:22:45,416 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 4071 words in input language, 21 words in output
2022-09-10 07:22:45,425 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:22:48,407 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:22:48,407 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:22:51,766 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:22:51,766 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/svamp/dense/models/cv_svamp_augmented_fold0.
2022-09-10 07:22:54,737 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:23:09,831 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.009708737864077669	
 equation acc epoch: 0.0048543689320388345	
 max val acc: 0.009708737864077669	
 equation acc: 0.0048543689320388345	
2022-09-10 07:23:09,832 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 15s
2022-09-10 07:23:09,835 | INFO | main_after.py: 96 : main() ::	 Experiment Name: cv_svamp_augmented
2022-09-10 07:23:09,835 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:23:09,835 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:23:10,792 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:23:10,792 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 3920
2022-09-10 07:23:10,792 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 218
2022-09-10 07:23:10,792 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: ['0.01', '12.0', '1.0', '100.0', '0.1', '0.5', '3.0', '4.0', '7.0']
2022-09-10 07:23:10,792 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:23:10,792 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:23:11,360 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 4068 / 4068 = 1.0
2022-09-10 07:23:11,523 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 4071 words in input language, 21 words in output
2022-09-10 07:23:11,534 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:23:14,479 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:23:14,479 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:23:14,559 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:23:14,559 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/svamp/dense/models/cv_svamp_augmented_fold1.
2022-09-10 07:23:16,517 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:23:23,403 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.045871559633027525	
 equation acc epoch: 0.04128440366972477	
 max val acc: 0.045871559633027525	
 equation acc: 0.04128440366972477	
2022-09-10 07:23:23,403 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 6s
2022-09-10 07:23:23,406 | INFO | main_after.py: 96 : main() ::	 Experiment Name: cv_svamp_augmented
2022-09-10 07:23:23,406 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:23:23,406 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:23:24,364 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:23:24,364 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 3911
2022-09-10 07:23:24,364 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 227
2022-09-10 07:23:24,364 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: ['0.01', '12.0', '1.0', '100.0', '0.1', '0.5', '3.0', '4.0', '7.0']
2022-09-10 07:23:24,364 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:23:24,364 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:23:24,941 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 4068 / 4068 = 1.0
2022-09-10 07:23:25,109 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 4071 words in input language, 21 words in output
2022-09-10 07:23:25,121 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:23:27,992 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:23:27,993 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:23:28,069 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:23:28,070 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/svamp/dense/models/cv_svamp_augmented_fold2.
2022-09-10 07:23:29,995 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:23:39,166 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.04405286343612335	
 equation acc epoch: 0.01762114537444934	
 max val acc: 0.04405286343612335	
 equation acc: 0.01762114537444934	
2022-09-10 07:23:39,167 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 9s
2022-09-10 07:23:39,170 | INFO | main_after.py: 96 : main() ::	 Experiment Name: cv_svamp_augmented
2022-09-10 07:23:39,170 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:23:39,170 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:23:40,129 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:23:40,129 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 3973
2022-09-10 07:23:40,129 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 165
2022-09-10 07:23:40,129 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: ['0.01', '12.0', '1.0', '100.0', '0.1', '0.5', '3.0', '4.0', '7.0']
2022-09-10 07:23:40,129 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:23:40,129 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:23:40,704 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 4068 / 4068 = 1.0
2022-09-10 07:23:40,880 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 4071 words in input language, 21 words in output
2022-09-10 07:23:40,890 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:23:43,729 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:23:43,730 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:23:43,807 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:23:43,807 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/svamp/dense/models/cv_svamp_augmented_fold3.
2022-09-10 07:23:45,538 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:23:50,536 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.04242424242424243	
 equation acc epoch: 0.012121212121212121	
 max val acc: 0.04242424242424243	
 equation acc: 0.012121212121212121	
2022-09-10 07:23:50,536 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 4s
2022-09-10 07:23:50,539 | INFO | main_after.py: 96 : main() ::	 Experiment Name: cv_svamp_augmented
2022-09-10 07:23:50,539 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:23:50,539 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:23:51,493 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:23:51,493 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 3954
2022-09-10 07:23:51,493 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 184
2022-09-10 07:23:51,493 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: ['0.01', '12.0', '1.0', '100.0', '0.1', '0.5', '3.0', '4.0', '7.0']
2022-09-10 07:23:51,493 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:23:51,494 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:23:52,073 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 4068 / 4068 = 1.0
2022-09-10 07:23:52,244 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 4071 words in input language, 21 words in output
2022-09-10 07:23:52,254 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:23:55,121 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:23:55,121 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:23:55,198 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:23:55,198 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/svamp/dense/models/cv_svamp_augmented_fold4.
2022-09-10 07:23:59,517 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:24:05,863 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.005434782608695652	
 equation acc epoch: 0.0	
 max val acc: 0.005434782608695652	
 equation acc: 0.0	
2022-09-10 07:24:05,863 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 6s
2022-09-10 07:24:05,863 | INFO | main_after.py: 272 : main() ::	 Final Val score: 0.03
Transfer numbers...
initialize by random pruning
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.3599382646971053
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.3620703631647212
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 0.36328125
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.3597123887803819
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.35982089572482634
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.35990566677517366
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.3606940375434028
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.3597123887803819
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.3601269192165799
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.35969882541232634
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.35921732584635413
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.35980563693576384
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.3605584038628472
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.35986879136827254
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.3599349127875434
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.36016337076822913
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.35945468478732634
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.3600921630859375
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.35979885525173616
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.35993152194552946
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.35981199476453996
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.3596716986762153
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.3598192003038194
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.36037699381510413
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.35998196072048616
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.3600989447699653
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.3597827487521701
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.3602294921875
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.3593902587890625
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.35958353678385413
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.3592003716362847
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.360144297281901
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.3601663377549913
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.3602990044487847
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.3611314561631944
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.36102634006076384
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.3600226508246528
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.3597284952799479
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.35951995849609375
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.3606855604383681
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.36083645290798616
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.35957845052083337
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.359649658203125
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.35967932807074654
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.35995610555013025
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.35927327473958337
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.3587866889105903
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.359954833984375
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.35962931315104163
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.3606236775716146
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.3602095709906684
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.36002604166666663
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.3599921332465278
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.3595648871527778
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.35992770724826384
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.36019770304361975
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.3599569532606337
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.3593275282118056
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.36062961154513884
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.35913425021701384
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.3597259521484375
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.3602125379774306
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.3597564697265625
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.3608161078559028
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.35994974772135413
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.36098225911458337
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.3607805040147569
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.3601112365722656
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.35947587754991317
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.3598870171440972
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.35975307888454866
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.35950046115451384
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.359893798828125
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.36020236545138884
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.3600807189941406
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.36031087239583337
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.3593184153238932
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.3595294952392578
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.3592878977457682
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.35999393463134766
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.3596057891845703
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.3598289489746094
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.3600311279296875
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.36000728607177734
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.36011505126953125
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.3611412048339844
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.35942840576171875
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.35993385314941406
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.366943359375
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.3595161437988281
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.400390625
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.359839121500651
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.369140625
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.35904947916666663
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.360504150390625
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.35940878731863835
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.36051613943917415
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.3591809953962054
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.3602709089006696
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.3599025181361607
Final sparsity level of 0.36: 0.3599447635664994
Transfer numbers...
initialize by random pruning
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.3599340681886004
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.3607150818741893
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 0.390625
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.36024814181857634
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.3615095350477431
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.3599836561414931
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.36058553059895837
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.36010148790147567
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.35956319173177087
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.36108907063802087
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.36016845703125
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.3601091172960069
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.3605211046006944
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.3600417243109809
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.3595386081271701
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.3598921034071181
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.36053466796875
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.3596716986762153
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.35940043131510413
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.36037614610460067
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.35958268907335067
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.3587103949652778
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.36007181803385413
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.3599090576171875
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.3602006700303819
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.3595576816134982
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.3596055772569444
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.36023457845052087
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.3592698838975694
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.35925801595052087
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.36023288302951384
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.3600010342068143
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.3598331875271268
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.36060587565104163
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.36012776692708337
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.3591783311631944
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.36053466796875
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.3598459031846788
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.35973782009548616
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.3603600396050347
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.3593902587890625
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.3597564697265625
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.36052958170572913
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.36030493842230904
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.3598323398166232
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.359832763671875
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.3612535264756944
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.36124165852864587
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.3598717583550347
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.3600985209147135
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.3599594963921441
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.3598954942491319
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.36078389485677087
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.36014133029513884
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.3592919243706597
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.3599353366427951
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.35999552408854163
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.3595716688368056
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.36092800564236116
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.3602447509765625
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.3604515923394097
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.3598590426974826
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.36013242933485246
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.35990397135416663
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.3606957329644097
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.3606414794921875
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.359588623046875
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.35969670613606775
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.36016337076822913
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.36068216959635413
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.36132303873697913
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.36060757107204866
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.35992431640625
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.3597089979383681
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.35987260606553817
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.3602074517144097
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.35962041219075525
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.3603487014770508
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.359411875406901
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.3594179153442383
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.36034250259399414
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.36011505126953125
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.3602581024169922
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.3592653274536133
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.3588142395019531
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.36031150817871094
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.3600006103515625
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.3604583740234375
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.351806640625
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.35985755920410156
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.38671875
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.3596979777018229
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.3828125
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.3642578125
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.36022840227399555
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.35955265590122765
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.35894666399274555
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.36097608293805805
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.3593706403459821
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.3597281319754464
Final sparsity level of 0.36: 0.35996145392502965
Transfer numbers...
initialize by random pruning
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.35994373051991113
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.3608746757457847
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 0.38020833333333337
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.3600073920355903
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.36002095540364587
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.36027865939670134
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.3593919542100694
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.3605363633897569
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.3597564697265625
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.3592851426866319
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.36037868923611116
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.3596818712022569
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.35817972819010413
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.36029349433051217
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.359839121500651
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.3596055772569444
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.3596106635199653
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.3596208360460069
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.3609076605902778
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.35982047186957467
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.3599654303656684
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.3597242567274306
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.36032443576388884
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.3585747612847222
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.36027187771267366
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.35976621839735246
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.3600701226128472
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.3588019476996528
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.35899861653645837
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.36111789279513884
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.36011759440104163
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.3603562249077691
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.3597450256347656
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.3596886528862847
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.3604363335503472
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.3602973090277778
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.36129421657986116
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.35978359646267366
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.3600353664822049
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.36069742838541663
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.3599260118272569
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.35992940266927087
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.3607652452256944
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.3595407274034288
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.3603990342881944
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.3592681884765625
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.36054484049479163
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.3595733642578125
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.3601854112413194
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.3598670959472656
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.3594928317599826
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.3592546251085069
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.3599073621961806
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.3602447509765625
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.359527587890625
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.35985141330295134
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.36008453369140625
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.3599005805121528
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.3599310980902778
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.36007181803385413
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.35960727267795134
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.359887440999349
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.3596958584255643
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.35982089572482634
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.3612382676866319
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.36110263400607634
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.36045498318142366
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.3597297668457031
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.3597323099772135
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.36060078938802087
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.35980224609375
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.36052958170572913
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.36029391818576384
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.35991923014322913
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.3592397901746962
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.35935635036892366
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.35992495218912757
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.3605031967163086
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.35995610555013025
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.3597564697265625
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.3600502014160156
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.359710693359375
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.3594856262207031
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.3602123260498047
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.36065673828125
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.3591575622558594
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.35887908935546875
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.3585090637207031
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.355712890625
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.36069297790527344
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.3515625
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.3599090576171875
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.30078125
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.3515625
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.3589564732142857
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.3597869873046875
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.3608986990792411
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.36019788469587055
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.3596147809709821
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.3590480259486607
Final sparsity level of 0.36: 0.35991761317821425
Transfer numbers...
initialize by random pruning
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.3598701361948341
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.3632001864461738
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 0.35677083333333337
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.35934956868489587
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.36022610134548616
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.3599836561414931
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.35935465494791663
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.3606135050455729
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.3597390916612413
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.35946316189236116
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.36087544759114587
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.36000569661458337
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.35900539822048616
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.35995610555013025
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.3602078755696615
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.3608466254340278
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.36107381184895837
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.35957845052083337
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.3601243760850694
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.35970984564887154
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.36020660400390625
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.3591393364800347
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.35951741536458337
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.3602752685546875
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.35899861653645837
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.3600578308105469
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.3600285847981771
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.361053466796875
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.3611229790581597
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.3601091172960069
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.3598564995659722
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.36002561781141496
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.3601726955837674
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.3582238091362847
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.3601972791883681
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.36008199055989587
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.3601294623480903
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.35983149210611975
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.3599633110894097
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.35922580295138884
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.3604956732855903
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.3595716688368056
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.3588850233289931
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.36009089152018225
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.36006037394205725
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.3611738416883681
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.36025661892361116
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.3602006700303819
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.3609246148003472
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.3599260118272569
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.35997729831271696
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.359649658203125
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.36025492350260413
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.36078219943576384
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.3601548936631944
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.3599022759331597
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.35967932807074654
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.35926479763454866
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.3589307996961806
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.3594275580512153
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.3599921332465278
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.3601879543728299
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.3603604634602865
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.3597564697265625
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.36075168185763884
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.36068386501736116
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.3602617051866319
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.36036766899956596
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.36056433783637154
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.3603379991319444
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.3604346381293403
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.3603074815538194
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.3596581353081597
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.3599238925509982
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.35979249742296004
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.3601837158203125
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.36022631327311194
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.35959720611572266
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.35947227478027344
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.3592824935913086
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.3607296943664551
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.35929012298583984
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.35962772369384766
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.3603858947753906
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.3613433837890625
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.3593864440917969
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.3603668212890625
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.36046791076660156
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.35302734375
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.3605194091796875
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.419921875
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.36087544759114587
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.384765625
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.36295572916666663
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.3600572858537946
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.3599003383091518
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.36040278843470985
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.35996464320591515
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.36002131870814735
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.36014992850167415
Final sparsity level of 0.36: 0.360000615002676
Transfer numbers...
initialize by random pruning
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.3600529433585331
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.35991204604409854
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 0.38802083333333337
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.35988362630208337
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.3594377305772569
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.35835605197482634
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.360321044921875
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.35996034410264754
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.3598700629340278
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.36082119411892366
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.36119418674045134
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.35982259114583337
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.3599022759331597
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.36028501722547746
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.35981241861979163
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.35940890842013884
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.36111111111111116
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.35943942599826384
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.3605211046006944
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.35971662733289933
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.3599357604980469
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.3596632215711806
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.3581865098741319
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.3611670600043403
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.35912746853298616
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.35970772637261283
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.3603278266059028
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.35942586263020837
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.3594801161024306
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.36000569661458337
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.3596479627821181
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.3592991299099393
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.3598175048828125
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.36011081271701384
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.3595801459418403
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.3597785101996528
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.35976664225260413
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.36017820570203996
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.35980818006727433
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.3597852918836806
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.3601158989800347
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.3591783311631944
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.3602667914496528
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.3598954942491319
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.3600090874565972
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.3599633110894097
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.3585662841796875
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.3606719970703125
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.3609330919053819
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.3600243462456597
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.3599052429199219
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.3595598008897569
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.36144680447048616
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.3608161078559028
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.3599073621961806
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.3598933749728732
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.35957929823133683
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.3597327338324653
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.36020406087239587
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.3596886528862847
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.35969882541232634
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.360443115234375
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.3599043952094184
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.36038547092013884
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.3609398735894097
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.3602837456597222
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.35936313205295134
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.3603841993543837
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.36011335584852433
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.35994296603732634
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.3585493299696181
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.36089579264322913
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.35986328125
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.36010403103298616
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.3599357604980469
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.3602583143446181
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.3604996999104818
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.35988426208496094
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.3604513804117838
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.3602123260498047
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.3602156639099121
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.3597068786621094
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.36004066467285156
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.36026477813720703
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.3605461120605469
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.3611106872558594
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.36212921142578125
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.3607521057128906
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.35693359375
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.3605842590332031
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.33203125
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.3600285847981771
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.33984375
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.35546875
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.36062949044363835
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.3596518380301339
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.35987309047154015
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.3603166852678571
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.36042676653180805
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.35955919538225445
Final sparsity level of 0.36: 0.36001574777901235
wandb: WARNING W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.
2022-09-10 07:24:08,844 | INFO | main_after.py: 96 : main() ::	 Experiment Name: cv_svamp_augmented
2022-09-10 07:24:08,844 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:24:08,844 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:24:09,877 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:24:09,878 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 3932
2022-09-10 07:24:09,878 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 206
2022-09-10 07:24:09,878 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: ['0.01', '12.0', '1.0', '100.0', '0.1', '0.5', '3.0', '4.0', '7.0']
2022-09-10 07:24:09,878 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:24:09,878 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:24:10,454 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 4068 / 4068 = 1.0
2022-09-10 07:24:10,633 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 4071 words in input language, 21 words in output
2022-09-10 07:24:10,642 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:24:13,744 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:24:13,744 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:24:17,038 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:24:17,038 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/svamp/dense/models/cv_svamp_augmented_fold0.
2022-09-10 07:24:21,540 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:24:45,702 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.0	
 equation acc epoch: 0.0	
 max val acc: 0.0	
 equation acc: 0.0	
2022-09-10 07:24:45,702 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 24s
2022-09-10 07:24:45,705 | INFO | main_after.py: 96 : main() ::	 Experiment Name: cv_svamp_augmented
2022-09-10 07:24:45,705 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:24:45,705 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:24:46,660 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:24:46,661 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 3920
2022-09-10 07:24:46,661 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 218
2022-09-10 07:24:46,661 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: ['0.01', '12.0', '1.0', '100.0', '0.1', '0.5', '3.0', '4.0', '7.0']
2022-09-10 07:24:46,661 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:24:46,661 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:24:47,231 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 4068 / 4068 = 1.0
2022-09-10 07:24:47,406 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 4071 words in input language, 21 words in output
2022-09-10 07:24:47,418 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:24:50,347 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:24:50,347 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:24:50,426 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:24:50,426 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/svamp/dense/models/cv_svamp_augmented_fold1.
2022-09-10 07:24:52,361 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:24:58,822 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.06422018348623854	
 equation acc epoch: 0.06422018348623854	
 max val acc: 0.06422018348623854	
 equation acc: 0.06422018348623854	
2022-09-10 07:24:58,822 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 6s
2022-09-10 07:24:58,825 | INFO | main_after.py: 96 : main() ::	 Experiment Name: cv_svamp_augmented
2022-09-10 07:24:58,825 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:24:58,825 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:24:59,778 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:24:59,778 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 3911
2022-09-10 07:24:59,778 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 227
2022-09-10 07:24:59,778 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: ['0.01', '12.0', '1.0', '100.0', '0.1', '0.5', '3.0', '4.0', '7.0']
2022-09-10 07:24:59,778 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:24:59,778 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:25:00,354 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 4068 / 4068 = 1.0
2022-09-10 07:25:00,534 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 4071 words in input language, 21 words in output
2022-09-10 07:25:00,546 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:25:03,416 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:25:03,416 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:25:03,501 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:25:03,501 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/svamp/dense/models/cv_svamp_augmented_fold2.
2022-09-10 07:25:05,346 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:25:12,943 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.04405286343612335	
 equation acc epoch: 0.00881057268722467	
 max val acc: 0.04405286343612335	
 equation acc: 0.00881057268722467	
2022-09-10 07:25:12,943 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 7s
2022-09-10 07:25:12,945 | INFO | main_after.py: 96 : main() ::	 Experiment Name: cv_svamp_augmented
2022-09-10 07:25:12,945 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:25:12,945 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:25:13,900 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:25:13,900 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 3973
2022-09-10 07:25:13,900 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 165
2022-09-10 07:25:13,900 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: ['0.01', '12.0', '1.0', '100.0', '0.1', '0.5', '3.0', '4.0', '7.0']
2022-09-10 07:25:13,900 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:25:13,900 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:25:14,479 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 4068 / 4068 = 1.0
2022-09-10 07:25:14,663 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 4071 words in input language, 21 words in output
2022-09-10 07:25:14,673 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:25:17,530 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:25:17,530 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:25:17,608 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:25:17,609 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/svamp/dense/models/cv_svamp_augmented_fold3.
2022-09-10 07:25:19,617 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:25:24,321 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.01818181818181818	
 equation acc epoch: 0.012121212121212121	
 max val acc: 0.01818181818181818	
 equation acc: 0.012121212121212121	
2022-09-10 07:25:24,321 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 4s
2022-09-10 07:25:24,323 | INFO | main_after.py: 96 : main() ::	 Experiment Name: cv_svamp_augmented
2022-09-10 07:25:24,324 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:25:24,324 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:25:25,277 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:25:25,278 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 3954
2022-09-10 07:25:25,278 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 184
2022-09-10 07:25:25,278 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: ['0.01', '12.0', '1.0', '100.0', '0.1', '0.5', '3.0', '4.0', '7.0']
2022-09-10 07:25:25,278 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:25:25,278 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:25:25,858 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 4068 / 4068 = 1.0
2022-09-10 07:25:26,039 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 4071 words in input language, 21 words in output
2022-09-10 07:25:26,050 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:25:28,843 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:25:28,843 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:25:28,921 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:25:28,921 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/svamp/dense/models/cv_svamp_augmented_fold4.
2022-09-10 07:25:30,540 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:25:36,226 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.03804347826086957	
 equation acc epoch: 0.021739130434782608	
 max val acc: 0.03804347826086957	
 equation acc: 0.021739130434782608	
2022-09-10 07:25:36,226 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 5s
Traceback (most recent call last):
  File "/home/sliu/miniconda3/envs/prune_cry/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/sliu/miniconda3/envs/prune_cry/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/gpfs/home6/sliu/Projects/SVAMP/code/gts/src/main_after.py", line 276, in <module>
    main()
  File "/gpfs/home6/sliu/Projects/SVAMP/code/gts/src/main_after.py", line 266, in main
    folds_scores.append(float(best_acc[w][0])/best_acc[w][1])
ZeroDivisionError: float division by zero
Transfer numbers...
initialize by random pruning
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.4879064396200139
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.4899987840466926
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 0.48046875
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.4877404106987847
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.4874793158637153
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.4880286322699653
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.4880133734809028
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.48819266425238717
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.4880934821234809
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.4873741997612847
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.4879014756944444
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.48813883463541663
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.4887932671440972
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.48755857679578996
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.48783577813042533
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.48829989963107634
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.48754374186197913
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.4874403211805556
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.48827107747395837
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.48786629570855033
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.48799260457356775
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.4871809217664931
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.4879591200086806
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.488800048828125
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.4872368706597222
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.488043467203776
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.48769378662109375
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.48826938205295134
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.4872962103949653
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.48821682400173616
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.4877861870659722
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.4875810411241319
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.48796590169270837
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.48839992947048616
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.4890662299262153
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.48838467068142366
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.4882795545789931
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.4878209431966146
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.48789215087890625
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.4885118272569444
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.4894188774956597
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.4875166151258681
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.4880082872178819
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.48797268337673616
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.48770480685763884
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.4879540337456597
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.4868316650390625
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.48777431911892366
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.4880659315321181
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.4882286919487847
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.4880939059787326
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.48787434895833337
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.48798116048177087
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.48751322428385413
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.48795572916666663
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.4882329305013021
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.4880523681640625
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.48782687717013884
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.4876556396484375
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.48821004231770837
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.4880048963758681
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.487829844156901
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.4876488579644097
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.48959859212239587
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.4879065619574653
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.4883795844184028
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.48888651529947913
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.48817571004231775
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.48755009969075525
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.4878709581163194
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.4882490370008681
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.48861355251736116
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.4880218505859375
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.4881477355957031
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.4878777398003472
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.4878302680121528
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.4875418345133463
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.48747730255126953
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.4870821634928385
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.4877471923828125
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.4877352714538574
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.48821258544921875
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.4880509376525879
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.48859119415283203
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.4874229431152344
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.4882640838623047
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.4879341125488281
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.4879169464111328
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.494873046875
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.4877300262451172
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.51953125
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.4879633585611979
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.484375
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.49186197916666663
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.4881744384765625
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.48778969900948665
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.4889155796595982
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.4875553676060268
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.4882572719029018
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.48805781773158485
Final sparsity level of 0.488: 0.4879438153935308
Transfer numbers...
initialize by random pruning
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.4879565386783381
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.48978599221789887
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 0.5013020833333333
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.4881371392144097
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.4887932671440972
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.4880744086371528
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.4887068006727431
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.48810619778103304
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.48738861083984375
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.48938496907552087
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.4881608751085069
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.48786417643229163
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.4878997802734375
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.48819944593641496
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.48765648735894096
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.4871656629774306
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.48945278591579866
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.4876081678602431
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.48741488986545134
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.48854870266384554
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.48784806993272567
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.4862586127387153
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.4878370496961806
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.48839823404947913
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.4881269666883681
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.4879188537597656
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.48776753743489587
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.4880761040581597
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.4876708984375
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.4869774712456597
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.4878150092230903
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.48781543307834196
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.4877925448947482
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.4886389838324653
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.488616943359375
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.4876335991753472
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.4880082872178819
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.48780695597330725
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.48807313707139754
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.48776753743489587
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.4879625108506944
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.48769802517361116
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.48875088161892366
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.48827531602647567
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.4876619974772135
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.4874352349175347
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.489410400390625
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.4884626600477431
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.4876708984375
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.48834567599826384
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.4875988430447049
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.4872877332899306
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.48802693684895837
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.48808797200520837
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.4874810112847222
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.48806677924262154
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.4881197611490885
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.4873521592881944
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.48755221896701384
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.48862202962239587
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.4889967176649306
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.4875861273871528
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.4881612989637587
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.4876335991753472
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.48862033420138884
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.4882422553168403
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.48707071940104163
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.48757595486111116
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.48796378241644967
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.4882744683159722
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.4890509711371528
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.48795572916666663
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.4877556694878472
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.48763953314887154
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.48790529039171004
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.48766072591145837
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.48760859171549475
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.48879146575927734
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.48780504862467444
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.48743152618408203
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.4883766174316406
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.48819541931152344
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.48810911178588867
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.48802852630615234
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.48712158203125
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.4882640838623047
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.48789215087890625
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.4880237579345703
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.478271484375
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.48826026916503906
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.51953125
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.4882596333821615
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.494140625
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.4912109375
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.4879564557756696
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.4874965122767857
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.4869777134486607
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.4889504568917411
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.4865853445870536
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.48754555838448665
Final sparsity level of 0.488: 0.4879537493066336
Transfer numbers...
initialize by random pruning
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.4880026225587718
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.489165349383917
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 0.49739583333333337
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.48791842990451384
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.4878251817491319
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.4876539442274306
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.4871385362413194
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.4882808261447482
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.4878556993272569
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.48716227213541663
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.48790317111545134
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.48808627658420134
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.4864044189453125
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.4880811903211806
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.4879722595214844
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.48844570583767366
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.4874030219184028
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.48802693684895837
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.4887305365668403
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.48770268758138025
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.4879544576009115
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.48736911349826384
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.4886033799913194
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.48630777994791663
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.48829989963107634
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.4879281785753038
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.48784849378797746
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.4871673583984375
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.4863976372612847
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.4890289306640625
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.48812527126736116
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.48862499660915804
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.48770268758138025
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.4882032606336806
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.48757425944010413
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.4879218207465278
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.48903232150607634
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.4878493414984809
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.48799472384982634
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.48780483669704866
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.48798116048177087
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.48811848958333337
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.48858642578125
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.4874704149034288
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.4880180358886719
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.4868486192491319
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.4886915418836806
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.4876624213324653
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.4882965087890625
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.48760901557074654
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.4876929389105903
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.48733181423611116
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.4881201850043403
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.488616943359375
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.48787265353732634
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.4876530965169271
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.4883071051703559
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.4876335991753472
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.4885796440972222
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.48806423611111116
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.48836263020833337
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.48779805501302087
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.48793750339084196
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.48730129665798616
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.4889441596137153
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.4891899956597222
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.4885236952039931
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.48809687296549475
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.48786799112955725
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.48881191677517366
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.4879235161675347
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.48826260036892366
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.48895263671875
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.4873462253146701
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.48745812310112846
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.48675876193576384
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.48815981547037757
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.4889860153198242
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.48791631062825525
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.48741912841796875
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.48819446563720703
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.48795032501220703
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.48756837844848633
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.48810863494873047
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.4886932373046875
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.4864978790283203
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.488128662109375
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.48636817932128906
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.492919921875
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.4880638122558594
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.48046875
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.487066904703776
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.45703125
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.48079427083333337
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.4875553676060268
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.4880501883370536
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.4884229387555804
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.4882921491350446
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.4879608154296875
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.4875335693359375
Final sparsity level of 0.488: 0.48793217852389614
Transfer numbers...
initialize by random pruning
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.4878541386899433
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.4915440580415046
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 0.49869791666666663
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.48712497287326384
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.4890594482421875
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.48811679416232634
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.4881354437934028
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.4882308112250434
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.48793326483832467
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.48776753743489587
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.4883405897352431
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.487823486328125
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.48720296223958337
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.48835627237955725
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.4882244533962674
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.48853556315104163
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.48880343967013884
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.487640380859375
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.48835245768229163
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.4878362019856771
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.4879603915744357
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.48736911349826384
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.4875013563368056
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.48814731174045134
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.4869622124565972
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.4881807963053385
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.48821216159396696
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.48891025119357634
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.4889068603515625
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.48832363552517366
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.4872216118706597
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.4883643256293403
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.48808797200520837
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.48628743489583337
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.4878455268012153
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.48806423611111116
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.4883100721571181
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.4881066216362847
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.48753060234917533
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.48664347330729163
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.4892103407118056
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.48730807834201384
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.4875166151258681
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.4879040188259549
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.48826005723741317
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.4886542426215278
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.4880591498480903
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.48739963107638884
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.4892900254991319
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.4879302978515625
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.48806465996636283
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.48868984646267366
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.48846096462673616
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.4886186387803819
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.48727756076388884
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.48772388034396696
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.4880693223741319
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.4873894585503472
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.4874335394965278
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.4874335394965278
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.48741658528645837
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.4884516398111979
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.4886784023708768
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.48812357584635413
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.48781840006510413
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.4891730414496528
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.4876793755425347
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.4883986579047309
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.4886974758572049
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.48731486002604163
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.48849826388888884
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.48881191677517366
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.4880354139539931
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.48828252156575525
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.4878391689724393
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.4884253607855903
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.48780250549316406
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.4874887466430664
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.4874547322591146
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.4881324768066406
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.4887862205505371
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.48735713958740234
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.4876413345336914
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.48807716369628906
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.4886054992675781
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.4880847930908203
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.4887275695800781
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.48874664306640625
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.484130859375
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.48766517639160156
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.55078125
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.48834228515625
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.5
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.49609375
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.4882289341517857
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.48821040562220985
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.48804691859654015
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.48789433070591515
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.48853084019252235
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.48828451974051335
Final sparsity level of 0.488: 0.48802801482563496
Transfer numbers...
initialize by random pruning
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.48802655819987395
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.4886612354085603
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 0.5182291666666667
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.4877302381727431
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.4874131944444444
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.4859619140625
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.48861355251736116
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.48780483669704866
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.4880714416503906
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.4881608751085069
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.4884185791015625
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.48768107096354163
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.488616943359375
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.48813798692491317
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.48795572916666663
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.4878607855902778
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.48890177408854163
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.48746236165364587
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.48825581868489587
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.4877204895019531
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.4882528516981337
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.4872911241319444
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.4868537055121528
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.48859151204427087
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.48768276638454866
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.48740175035264754
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.4882596333821615
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.4871673583984375
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.48780483669704866
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.4875962999131944
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.4880286322699653
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.48768954806857634
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.4874852498372396
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.4879438612196181
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.48769124348958337
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.48764377170138884
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.48809475368923616
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.48808373345269096
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.48743184407552087
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.48769632975260413
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.4872521294487847
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.4874894883897569
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.48769802517361116
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.4878840976291232
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.4880082872178819
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.4879998101128472
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.4871504041883681
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.4890051947699653
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.4889611138237847
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.4879408942328559
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.4881095886230469
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.48762173122829866
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.4886254204644097
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.4889899359809028
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.4868147108289931
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.48805575900607634
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.4877671135796441
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.48780314127604163
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.488372802734375
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.48753865559895837
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.4874657524956597
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.48818376329210067
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.48736275566948783
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.4889543321397569
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.48902384440104163
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.4881134033203125
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.4872656928168403
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.4882223341200087
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.48784891764322913
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.48900180392795134
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.4867265489366319
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.4882575141059028
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.48748270670572913
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.48773786756727433
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.4879612392849393
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.48702663845486116
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.48826917012532556
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.4875955581665039
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.48838043212890625
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.4879732131958008
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.4880251884460449
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.48743724822998047
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.4884223937988281
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.4884357452392578
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.4883918762207031
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.4887962341308594
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.4906005859375
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.489105224609375
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.483642578125
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.4891948699951172
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.453125
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.4880040486653646
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.494140625
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.47819010416666663
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.4888065883091518
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.48778969900948665
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.48772212437220985
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.4878387451171875
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.4885450090680804
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.48797607421875
Final sparsity level of 0.488: 0.4879710211625162
wandb: WARNING W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.
2022-09-10 07:25:39,261 | INFO | main_after.py: 96 : main() ::	 Experiment Name: cv_svamp_augmented
2022-09-10 07:25:39,261 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:25:39,261 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:25:40,297 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:25:40,298 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 3932
2022-09-10 07:25:40,298 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 206
2022-09-10 07:25:40,298 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: ['0.01', '12.0', '1.0', '100.0', '0.1', '0.5', '3.0', '4.0', '7.0']
2022-09-10 07:25:40,298 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:25:40,298 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:25:40,870 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 4068 / 4068 = 1.0
2022-09-10 07:25:41,039 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 4071 words in input language, 21 words in output
2022-09-10 07:25:41,048 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:25:44,105 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:25:44,105 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:25:47,382 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:25:47,382 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/svamp/dense/models/cv_svamp_augmented_fold0.
2022-09-10 07:25:52,207 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:26:36,572 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.009708737864077669	
 equation acc epoch: 0.0	
 max val acc: 0.009708737864077669	
 equation acc: 0.0	
2022-09-10 07:26:36,573 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 44s
2022-09-10 07:26:36,575 | INFO | main_after.py: 96 : main() ::	 Experiment Name: cv_svamp_augmented
2022-09-10 07:26:36,575 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:26:36,575 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:26:37,535 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:26:37,535 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 3920
2022-09-10 07:26:37,535 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 218
2022-09-10 07:26:37,535 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: ['0.01', '12.0', '1.0', '100.0', '0.1', '0.5', '3.0', '4.0', '7.0']
2022-09-10 07:26:37,535 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:26:37,535 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:26:38,103 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 4068 / 4068 = 1.0
2022-09-10 07:26:38,363 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 4071 words in input language, 21 words in output
2022-09-10 07:26:38,374 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:26:41,282 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:26:41,282 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:26:41,362 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:26:41,362 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/svamp/dense/models/cv_svamp_augmented_fold1.
2022-09-10 07:26:43,233 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:26:51,571 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.01834862385321101	
 equation acc epoch: 0.01834862385321101	
 max val acc: 0.01834862385321101	
 equation acc: 0.01834862385321101	
2022-09-10 07:26:51,571 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 8s
2022-09-10 07:26:51,573 | INFO | main_after.py: 96 : main() ::	 Experiment Name: cv_svamp_augmented
2022-09-10 07:26:51,574 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:26:51,574 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:26:52,530 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:26:52,530 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 3911
2022-09-10 07:26:52,530 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 227
2022-09-10 07:26:52,531 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: ['0.01', '12.0', '1.0', '100.0', '0.1', '0.5', '3.0', '4.0', '7.0']
2022-09-10 07:26:52,531 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:26:52,531 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:26:53,111 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 4068 / 4068 = 1.0
2022-09-10 07:26:53,358 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 4071 words in input language, 21 words in output
2022-09-10 07:26:53,369 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:26:56,223 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:26:56,223 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:26:56,300 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:26:56,300 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/svamp/dense/models/cv_svamp_augmented_fold2.
2022-09-10 07:26:58,195 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:27:05,424 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.05726872246696035	
 equation acc epoch: 0.00881057268722467	
 max val acc: 0.05726872246696035	
 equation acc: 0.00881057268722467	
2022-09-10 07:27:05,424 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 7s
2022-09-10 07:27:05,426 | INFO | main_after.py: 96 : main() ::	 Experiment Name: cv_svamp_augmented
2022-09-10 07:27:05,426 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:27:05,426 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:27:06,385 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:27:06,385 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 3973
2022-09-10 07:27:06,385 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 165
2022-09-10 07:27:06,385 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: ['0.01', '12.0', '1.0', '100.0', '0.1', '0.5', '3.0', '4.0', '7.0']
2022-09-10 07:27:06,385 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:27:06,385 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:27:06,967 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 4068 / 4068 = 1.0
2022-09-10 07:27:07,141 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 4071 words in input language, 21 words in output
2022-09-10 07:27:07,151 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:27:10,039 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:27:10,040 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:27:10,118 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:27:10,118 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/svamp/dense/models/cv_svamp_augmented_fold3.
2022-09-10 07:27:12,785 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:27:17,389 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.030303030303030304	
 equation acc epoch: 0.006060606060606061	
 max val acc: 0.030303030303030304	
 equation acc: 0.006060606060606061	
2022-09-10 07:27:17,389 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 4s
2022-09-10 07:27:17,392 | INFO | main_after.py: 96 : main() ::	 Experiment Name: cv_svamp_augmented
2022-09-10 07:27:17,392 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:27:17,392 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:27:18,355 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:27:18,355 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 3954
2022-09-10 07:27:18,355 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 184
2022-09-10 07:27:18,355 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: ['0.01', '12.0', '1.0', '100.0', '0.1', '0.5', '3.0', '4.0', '7.0']
2022-09-10 07:27:18,355 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:27:18,355 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:27:18,941 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 4068 / 4068 = 1.0
2022-09-10 07:27:19,114 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 4071 words in input language, 21 words in output
2022-09-10 07:27:19,125 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:27:22,072 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:27:22,072 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:27:22,152 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:27:22,153 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/svamp/dense/models/cv_svamp_augmented_fold4.
2022-09-10 07:27:24,454 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:27:31,252 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.03260869565217391	
 equation acc epoch: 0.021739130434782608	
 max val acc: 0.03260869565217391	
 equation acc: 0.021739130434782608	
2022-09-10 07:27:31,252 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 6s
2022-09-10 07:27:31,252 | INFO | main_after.py: 272 : main() ::	 Final Val score: 0.03
Transfer numbers...
initialize by random pruning
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.5898994962117444
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.5917056785019454
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 0.5768229166666667
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5904117160373263
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5897911919487847
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.59002685546875
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5900404188368056
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5900675455729167
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5901930067274306
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5898047553168403
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5897352430555556
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5905914306640625
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5909322102864583
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.589798821343316
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5898666381835938
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5906558566623263
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5905609130859375
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5900082058376737
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5910966661241319
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.589774661593967
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5900302463107638
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5902218288845487
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5895284016927083
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5910814073350694
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5897030300564237
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5904053582085503
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5898335774739583
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5893181694878472
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5894114176432292
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5899556477864583
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5895555284288194
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5895148383246528
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5896538628472222
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5908169216579862
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5907474093967013
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5901811387803819
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5909881591796875
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5900179545084636
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5899314880371094
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5901607937282987
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5905914306640625
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.589630126953125
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5897979736328125
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.589922587076823
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5896555582682292
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5897708468967013
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.589324951171875
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5901217990451388
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5896555582682292
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5901010301378038
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5901472303602431
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5898047553168403
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5899115668402778
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5901014539930556
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5897098117404513
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5902726915147569
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5903506808810763
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5899641248914931
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.589569091796875
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5902879503038194
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5894521077473958
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5898746914333768
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5900828043619792
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5905117458767362
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5898929172092013
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5905592176649306
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5906389024522569
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5903104146321614
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5898607042100694
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5902811686197917
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5900149875217013
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5910576714409722
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5904083251953125
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5899535285101997
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5896843804253472
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.589813232421875
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.5898323059082031
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.5897150039672852
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.5891183217366536
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.5894956588745117
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.5899381637573242
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.5900897979736328
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.5900378227233887
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.5901165008544922
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.5894699096679688
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.5902919769287109
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.5895538330078125
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.5898208618164062
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.597412109375
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.5902271270751953
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.591796875
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.5898844401041667
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.599609375
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.5895182291666667
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.5897739955357143
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.5906099591936385
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.590439932686942
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.5894012451171875
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.5900606427873885
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.5897489275251115
Final sparsity level of 0.59: 0.5899749607240075
Transfer numbers...
initialize by random pruning
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.5900401051510329
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.5917867420557716
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 0.6119791666666667
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5895843505859375
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5910152859157987
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5893741183810763
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5907084147135417
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5898984273274739
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5896305508083768
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5910746256510417
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5898149278428819
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5901743570963542
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5902370876736112
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5900382995605469
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5899026658799913
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5897894965277778
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5909745958116319
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5893910725911458
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5894724527994792
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5904134114583333
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5896046956380208
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5889638264973958
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5902879503038194
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5903235541449653
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5898488362630208
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5898056030273438
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5897975497775607
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5903947618272569
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5895741780598958
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5896792941623263
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5894148084852431
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.590288798014323
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5898874070909288
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5904320610894097
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.590240478515625
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5892469618055556
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5898607042100694
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5895614624023438
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5900251600477431
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.58990478515625
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5897148980034722
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.589935302734375
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5904490152994792
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5900179545084636
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5902951558430989
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.589874267578125
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5918629964192708
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.590240478515625
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5898963080512153
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5903828938802083
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5896364847819011
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5896352132161458
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5900522867838542
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5903303358289931
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5898183186848958
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5901209513346355
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5900514390733507
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5893944634331597
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5889333089192708
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5900590684678819
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5906405978732638
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5898263719346788
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5900611877441406
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5895368787977431
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5907643636067708
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5905507405598958
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5882280137803819
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5895474751790364
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5899200439453125
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5901862250434028
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5903303358289931
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5897199842664931
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5893452962239583
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.589699215359158
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.589892069498698
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.589263916015625
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.5897363026936848
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.5905771255493164
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.5900115966796875
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.5896501541137695
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.5903267860412598
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.590184211730957
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.5901474952697754
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.5895357131958008
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.5889930725097656
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.5898532867431641
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.5910148620605469
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.590087890625
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.5791015625
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.5905418395996094
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.603515625
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.5903065999348958
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.548828125
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.5953776041666667
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.5899919782366072
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.5891789027622768
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.5891505650111607
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.5906622750418526
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.5895233154296875
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.5903887067522322
Final sparsity level of 0.59: 0.5899819871503484
Transfer numbers...
initialize by random pruning
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.5899910163632747
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.5911357003891051
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 0.6028645833333333
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5896623399522569
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5895792643229167
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5897861056857638
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5888180202907987
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5902108086480035
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.589973873562283
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5894351535373263
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5900980631510417
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5898691813151042
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5889451768663194
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5897577073838975
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.590226067437066
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5905982123480903
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5889383951822917
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.59027099609375
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5911271837022569
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5898891025119357
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.589859856499566
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5890096028645833
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5905303955078125
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5882958306206597
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.591094970703125
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5899670918782551
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5898145039876301
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5885281032986112
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5883568657769097
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5909423828125
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5897725423177083
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.590553707546658
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5895284016927083
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5900336371527778
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5899217393663194
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5900387234157987
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5911356608072917
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5898861355251737
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5899314880371094
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5899590386284722
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5899522569444444
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5899336073133681
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5907084147135417
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5895873175726997
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5899967617458768
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5888705783420138
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5901455349392362
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5894283718532987
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5908559163411458
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5900624593098958
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5896233452690972
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5894639756944444
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5900861952039931
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5908915201822917
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5898284912109375
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5896674262152778
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5902010599772136
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5894554985894097
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5906219482421875
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5898284912109375
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5904134114583333
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5899543762207031
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.589951409233941
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5891469319661458
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5913289388020833
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5912034776475694
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.590484619140625
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.590156979031033
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5897543165418837
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5909237331814237
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5901624891493056
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5909525553385417
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5904032389322917
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5899993048773872
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5895419650607638
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.5893622504340278
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.5900904337565105
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.5902185440063477
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.5901622772216797
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.5890111923217773
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.5898909568786621
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.5901966094970703
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.5896782875061035
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.5903692245483398
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.5896835327148438
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.5887031555175781
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.5897445678710938
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.5885257720947266
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.593017578125
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.5900249481201172
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.583984375
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.5896313985188801
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.548828125
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.5787760416666667
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.5899222237723214
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.589705330984933
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.5901870727539062
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.5903538295200893
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.5900290352957589
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.58966064453125
Final sparsity level of 0.59: 0.5899471803802572
Transfer numbers...
initialize by random pruning
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.5898896525498193
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.5929824294747081
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 0.5924479166666667
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5893368191189237
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5904151068793403
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5897403293185763
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5899098714192708
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.590042961968316
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5901578267415364
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5899251302083333
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5904134114583333
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5899030897352431
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5893520779079862
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5903600056966145
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5901705423990886
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5903591579861112
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5907219780815972
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5894605848524306
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5906236436631944
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5897810194227431
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.58954832288954
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5900692409939237
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.589813232421875
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5903422037760417
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5892418755425347
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5900137159559462
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5900980631510417
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5910627577039931
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5905117458767362
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5901557074652778
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.58966064453125
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5901862250434028
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5902146233452691
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5887807210286458
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5900099012586806
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5901336669921875
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5905337863498263
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.59015867445204
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5897958543565538
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.58917236328125
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5911729600694444
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5896860758463542
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5901624891493056
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5900552537706163
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5900514390733507
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5898878309461806
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.59039306640625
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5896369086371528
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5911305745442708
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5899361504448785
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5900035434299045
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5911797417534722
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5900777180989583
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5906812879774306
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5898420545789931
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5894830491807725
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5899815029568143
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5902862548828125
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5897216796875
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5893046061197917
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5891045464409722
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5903676350911458
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5906867980957031
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5899675157335069
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5900217692057292
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5904930962456597
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5895962185329862
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5905490451388888
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5905744764539931
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5892927381727431
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5906253390842013
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5907474093967013
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5909169514973958
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5902345445421007
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5900730556911893
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.5901217990451388
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.590084711710612
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.5896720886230469
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.5895951588948567
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.5899639129638672
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.590144157409668
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.5898294448852539
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.589930534362793
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.590245246887207
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.5904464721679688
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.5897197723388672
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.5909919738769531
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.5909156799316406
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.587646484375
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.5897083282470703
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.6328125
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.5904172261555989
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.587890625
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.5940755208333333
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.5899440220424107
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.5902295793805803
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.5897674560546875
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.5894502912248885
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.5902808053152901
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.5901380266462053
Final sparsity level of 0.59: 0.590035595668166
Transfer numbers...
initialize by random pruning
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.5899356069083855
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.5905302569714657
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 0.625
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5899742974175347
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5894148084852431
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5875769721137153
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5905778672960069
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5898874070909288
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5901764763726128
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5906236436631944
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5903337266710069
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5900861952039931
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5904557969835069
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5900162590874566
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5900832282172309
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5899624294704862
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5909881591796875
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5894758436414931
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5900472005208333
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5896585252549913
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5900149875217013
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5894792344835069
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5890926784939237
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5905083550347222
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5896131727430556
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.589424557156033
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.590118408203125
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5894148084852431
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5902574327256944
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5892791748046875
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5904897054036458
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5898751152886285
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5895962185329862
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5896216498480903
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5894215901692708
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5892859564887153
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5901302761501737
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5898072984483507
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5892401801215278
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5892435709635417
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5891994900173612
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5897555881076388
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.58984375
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5897259182400174
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5896241929796007
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.589874267578125
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5894521077473958
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5911712646484375
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5907864040798612
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5899264017740886
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5900238884819878
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5898115370008681
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5901387532552083
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5908898247612847
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.58917236328125
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.590044657389323
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5900213453504775
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5900912814670138
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5908270941840278
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5902133517795138
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5899895562065972
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5903964572482638
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5895300971137153
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5905524359809028
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5904371473524306
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5907762315538194
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5889265272352431
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5901629130045574
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5896339416503906
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.59051513671875
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5887552897135417
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5908525254991319
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5892401801215278
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5900866190592449
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5895517137315538
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.5894639756944444
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.5905965169270833
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.5897102355957031
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.5901896158854167
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.5902681350708008
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.5899538993835449
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.5895452499389648
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.590233325958252
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.5901212692260742
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.5902519226074219
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.590576171875
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.5922317504882812
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.5916309356689453
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.583740234375
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.5908966064453125
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.55859375
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.5898946126302083
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.609375
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.5817057291666667
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.5901532854352678
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.5902622767857143
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.5898949759347099
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.5895156860351562
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.5903385707310268
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.5901467459542411
Final sparsity level of 0.59: 0.5899388525074609
wandb: WARNING W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.
2022-09-10 07:27:34,268 | INFO | main_after.py: 96 : main() ::	 Experiment Name: cv_svamp_augmented
2022-09-10 07:27:34,269 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:27:34,269 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:27:35,303 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:27:35,303 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 3932
2022-09-10 07:27:35,304 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 206
2022-09-10 07:27:35,304 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: ['0.01', '12.0', '1.0', '100.0', '0.1', '0.5', '3.0', '4.0', '7.0']
2022-09-10 07:27:35,304 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:27:35,304 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:27:35,887 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 4068 / 4068 = 1.0
2022-09-10 07:27:36,053 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 4071 words in input language, 21 words in output
2022-09-10 07:27:36,061 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:27:39,059 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:27:39,059 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:27:42,373 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:27:42,373 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/svamp/dense/models/cv_svamp_augmented_fold0.
2022-09-10 07:27:44,426 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:28:52,629 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.0048543689320388345	
 equation acc epoch: 0.0	
 max val acc: 0.0048543689320388345	
 equation acc: 0.0	
2022-09-10 07:28:52,629 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 1m 8s
2022-09-10 07:28:52,632 | INFO | main_after.py: 96 : main() ::	 Experiment Name: cv_svamp_augmented
2022-09-10 07:28:52,632 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:28:52,632 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:28:53,685 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:28:53,685 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 3920
2022-09-10 07:28:53,685 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 218
2022-09-10 07:28:53,685 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: ['0.01', '12.0', '1.0', '100.0', '0.1', '0.5', '3.0', '4.0', '7.0']
2022-09-10 07:28:53,685 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:28:53,685 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:28:54,261 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 4068 / 4068 = 1.0
2022-09-10 07:28:54,421 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 4071 words in input language, 21 words in output
2022-09-10 07:28:54,431 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:28:57,366 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:28:57,367 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:28:57,445 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:28:57,445 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/svamp/dense/models/cv_svamp_augmented_fold1.
2022-09-10 07:28:59,282 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:29:06,126 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.0045871559633027525	
 equation acc epoch: 0.0	
 max val acc: 0.0045871559633027525	
 equation acc: 0.0	
2022-09-10 07:29:06,126 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 6s
2022-09-10 07:29:06,129 | INFO | main_after.py: 96 : main() ::	 Experiment Name: cv_svamp_augmented
2022-09-10 07:29:06,129 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:29:06,129 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:29:07,182 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:29:07,182 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 3911
2022-09-10 07:29:07,182 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 227
2022-09-10 07:29:07,182 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: ['0.01', '12.0', '1.0', '100.0', '0.1', '0.5', '3.0', '4.0', '7.0']
2022-09-10 07:29:07,182 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:29:07,182 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:29:07,766 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 4068 / 4068 = 1.0
2022-09-10 07:29:07,930 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 4071 words in input language, 21 words in output
2022-09-10 07:29:07,941 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:29:10,778 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:29:10,779 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:29:10,856 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:29:10,856 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/svamp/dense/models/cv_svamp_augmented_fold2.
2022-09-10 07:29:12,721 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:29:21,396 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.11894273127753303	
 equation acc epoch: 0.0	
 max val acc: 0.11894273127753303	
 equation acc: 0.0	
2022-09-10 07:29:21,397 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 8s
2022-09-10 07:29:21,399 | INFO | main_after.py: 96 : main() ::	 Experiment Name: cv_svamp_augmented
2022-09-10 07:29:21,399 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:29:21,399 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:29:22,443 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:29:22,443 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 3973
2022-09-10 07:29:22,443 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 165
2022-09-10 07:29:22,443 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: ['0.01', '12.0', '1.0', '100.0', '0.1', '0.5', '3.0', '4.0', '7.0']
2022-09-10 07:29:22,443 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:29:22,443 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:29:23,026 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 4068 / 4068 = 1.0
2022-09-10 07:29:23,195 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 4071 words in input language, 21 words in output
2022-09-10 07:29:23,204 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:29:26,013 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:29:26,013 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:29:26,090 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:29:26,090 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/svamp/dense/models/cv_svamp_augmented_fold3.
2022-09-10 07:29:29,781 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:29:34,462 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.01818181818181818	
 equation acc epoch: 0.0	
 max val acc: 0.01818181818181818	
 equation acc: 0.0	
2022-09-10 07:29:34,462 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 4s
2022-09-10 07:29:34,464 | INFO | main_after.py: 96 : main() ::	 Experiment Name: cv_svamp_augmented
2022-09-10 07:29:34,464 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:29:34,464 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:29:35,424 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:29:35,425 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 3954
2022-09-10 07:29:35,425 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 184
2022-09-10 07:29:35,425 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: ['0.01', '12.0', '1.0', '100.0', '0.1', '0.5', '3.0', '4.0', '7.0']
2022-09-10 07:29:35,425 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:29:35,425 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:29:36,014 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 4068 / 4068 = 1.0
2022-09-10 07:29:36,264 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 4071 words in input language, 21 words in output
2022-09-10 07:29:36,273 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:29:39,124 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:29:39,124 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:29:39,201 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:29:39,201 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/svamp/dense/models/cv_svamp_augmented_fold4.
2022-09-10 07:29:43,356 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:29:49,599 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.059782608695652176	
 equation acc epoch: 0.03804347826086957	
 max val acc: 0.059782608695652176	
 equation acc: 0.03804347826086957	
2022-09-10 07:29:49,599 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 6s
2022-09-10 07:29:49,599 | INFO | main_after.py: 272 : main() ::	 Final Val score: 0.043
Transfer numbers...
initialize by random pruning
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.6719265756076129
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.673351876621271
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 0.65625
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6723649766710069
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6721106635199653
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6722734239366319
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6720394558376737
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6721738179524739
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6722471449110243
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6719529893663194
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6716444227430556
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6724277072482638
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.672943115234375
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.671730465359158
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6717274983723958
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6723005506727431
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6725870768229167
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6719546847873263
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6727498372395833
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6717478434244792
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6723276774088542
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6725379096137153
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6716274685329862
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.672393798828125
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.671722412109375
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6722191704644097
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6717910766601562
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6712629530164931
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6714409722222222
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6718377007378472
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6713019476996528
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6718279520670574
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6717775132921007
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6728600396050347
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6728074815538194
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6717919243706597
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6733449300130208
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6719419691297743
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6717207166883681
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6720971001519097
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6723225911458333
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6714664035373263
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6715596516927083
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6721746656629775
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6717567443847656
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6717258029513888
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6720496283637153
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6727176242404513
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6721750895182292
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6720182630750868
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6719008551703559
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6721937391493056
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6715511745876737
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6720937093098958
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6714426676432292
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6721589830186632
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6721984015570747
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6722700330946181
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6718868679470487
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6719156901041667
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6714155409071181
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6718054877387153
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6721178690592449
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6727328830295138
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6723107231987847
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6724904378255208
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6725955539279513
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6724349127875434
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6722598605685763
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6718224419487847
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6716054280598958
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6731126573350694
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6713409423828125
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6722191704644097
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6719741821289062
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.6721886528862847
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.6718743642171223
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.6716480255126953
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.6714293162027996
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.6713008880615234
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.6720194816589355
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.6720752716064453
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.6718626022338867
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.6724376678466797
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.6712760925292969
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.6718368530273438
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.6718788146972656
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.6721687316894531
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.677734375
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.6727199554443359
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.67578125
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.6714210510253906
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.701171875
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.6744791666666667
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.6715066092354911
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.6727534702845982
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.6723763602120536
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.6709660121372768
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.6727948869977678
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.6715523856026786
Final sparsity level of 0.672: 0.6719947607642359
Transfer numbers...
initialize by random pruning
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.6720524967671342
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.673607733463035
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 0.6731770833333333
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6718800862630208
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6729142930772569
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6715087890625
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6726820203993056
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6719661288791232
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6715439690483941
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6729736328125
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6712731255425347
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6727956136067708
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6719207763671875
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6720648871527778
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6719928317599826
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6716596815321181
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6728786892361112
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6715799967447917
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6721038818359375
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6722929212782118
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6718711853027344
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6709204779730903
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6722835964626737
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.672576904296875
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6715647379557292
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.671984354654948
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6722005208333333
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6723242865668403
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6721971299913194
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6721445719401042
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6714375813802083
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.672318352593316
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6721873813205295
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6718020968967013
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6725413004557292
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6718275282118056
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6721683078342013
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6718287997775607
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6720920138888888
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6720987955729167
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6728040907118056
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6717715793185763
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6726565890842013
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6720508999294705
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6720750596788194
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6714409722222222
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6733805338541667
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6724717881944444
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6712866889105903
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6722102695041232
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6718639797634549
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6710408528645833
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6725853814019097
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6723649766710069
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6720767550998263
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6718733045789931
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6720428466796875
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6717274983723958
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6711069742838542
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6723073323567708
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6727650960286458
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6718546549479167
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.671958499484592
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6719394259982638
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6723853217230903
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6722835964626737
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6707780626085069
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6720297071668837
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6721119350857205
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6729210747612847
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6721988254123263
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6716885036892362
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6720445421006944
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6716300116644965
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6721081203884549
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.6712205674913194
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.6720517476399739
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.6723842620849609
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.6717154184977214
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.67144775390625
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.672339916229248
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.6722288131713867
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.6720032691955566
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.6715326309204102
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.6720771789550781
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.6718502044677734
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.6728057861328125
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.6717433929443359
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.667724609375
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.6721668243408203
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.6953125
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.672655741373698
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.638671875
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.6796875
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.6721572875976562
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.6713463919503349
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.6715185982840401
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.672821044921875
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.6714848109654018
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.672839573451451
Final sparsity level of 0.672: 0.6720392868472166
Transfer numbers...
initialize by random pruning
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.6720607861666501
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.6737343952658885
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 0.6875
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6711968315972222
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6724260118272569
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.672088623046875
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6709984673394097
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6723395453559028
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6717334323459201
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6715749104817708
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6721394856770833
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6722666422526042
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6709509955512153
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6718936496310763
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6718983120388455
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6723208957248263
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6703219943576388
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6725565592447917
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6729092068142362
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6718296474880643
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6722488403320312
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6718190511067708
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6723598904079862
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6709153917100694
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6730516221788194
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6719669765896268
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6718393961588542
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6708204481336806
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6709442138671875
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6727820502387153
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6722903781467013
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.672528584798177
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6716393364800347
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6709306504991319
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6718919542100694
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6716681586371528
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6733042399088542
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6717486911349826
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.672019534640842
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6720208062065972
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6719835069444444
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6717122395833333
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6726769341362847
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.67169189453125
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6719835069444444
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6714274088541667
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6725073920355903
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6716647677951388
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6724700927734375
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6719224717881944
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6717258029513888
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6717732747395833
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6722191704644097
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6728600396050347
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6716647677951388
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.671646965874566
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6722488403320312
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6716512044270833
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6722683376736112
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6721377902560763
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6714104546440972
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6722611321343316
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.671927981906467
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6709103054470487
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6734653049045138
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6731804741753472
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6725175645616319
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6719203525119357
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6716482374403212
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6726701524522569
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6725599500868056
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6726430257161458
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6717308892144097
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6719962226019965
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6717491149902344
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.6714375813802083
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.6719735463460286
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.6722574234008789
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.6723346710205078
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.6715097427368164
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.6718435287475586
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.6726102828979492
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.6714687347412109
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.6720714569091797
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.6721343994140625
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.6709365844726562
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.6716156005859375
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.6706333160400391
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.668701171875
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.6720008850097656
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.669921875
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.6716245015462239
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.630859375
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.6617838541666667
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.671630859375
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.671891348702567
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.6719948904854911
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.6721812656947544
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.6716297694614956
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.671551295689174
Final sparsity level of 0.672: 0.6719731830589109
Transfer numbers...
initialize by random pruning
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.6718931071570675
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.6749300826848249
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 0.6822916666666667
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6711510552300347
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6721004909939237
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6723344590928819
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6715799967447917
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6720763312445747
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6719996134440105
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6715562608506944
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6721208360460069
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6715918646918403
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6713731553819444
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.67243406507704
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6720945570203993
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6722547743055556
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6726769341362847
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6717173258463542
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6723395453559028
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6718839009602864
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6713316175672743
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6718563503689237
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6718936496310763
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6726515028211806
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6718360053168403
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6720381842719184
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.671944088406033
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6734025743272569
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6730973985460069
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6716630723741319
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6709764268663194
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6721810234917535
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6725027296278212
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6705169677734375
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.67236328125
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6715613471137153
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6723429361979167
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6719737582736545
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6717597113715278
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6712324354383681
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6731753879123263
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6717614067925347
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6721140543619792
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6723954942491319
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6720144483778212
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6719343397352431
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.672821044921875
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6718733045789931
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6728244357638888
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6716829935709636
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6720653110080295
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.67333984375
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6717902289496528
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6726464165581597
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6720716688368056
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6712426079644097
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.672149658203125
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.673187255859375
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6715969509548612
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6713341606987847
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6711324055989583
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6724010043674045
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6725167168511285
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6723429361979167
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6715342203776042
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6724395751953125
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6714053683810763
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6724141438802083
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6724794175889757
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6719597710503472
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6724344889322917
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6726769341362847
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6721598307291667
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6723403930664062
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6721920437282987
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.6721683078342013
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.6717274983723958
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.6720609664916992
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.6718940734863281
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.6716690063476562
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.6722502708435059
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.6717109680175781
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.6721868515014648
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.6727819442749023
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.6726188659667969
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.6724224090576172
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.6722335815429688
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.6725425720214844
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.669677734375
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.6719226837158203
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.70703125
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.6724967956542969
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.673828125
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.6748046875
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.671860831124442
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.672027587890625
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.6718858991350447
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.671647208077567
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.672619410923549
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.6721605573381697
Final sparsity level of 0.672: 0.6720386430465766
Transfer numbers...
initialize by random pruning
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.6719326631353825
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.6721612556744487
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 0.7018229166666667
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6716172960069444
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6712070041232638
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6698320176866319
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6727786593967013
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6718402438693576
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.672310299343533
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6730397542317708
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6723904079861112
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6726464165581597
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6722802056206597
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6722522311740451
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6719148423936632
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6723802354600694
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6728244357638888
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.671966552734375
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6717902289496528
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6718029446072049
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6718165079752605
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6713036431206597
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6713578965928819
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6724175347222222
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6717987060546875
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6719682481553819
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6719669765896268
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6718919542100694
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6721259223090278
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6719004313151042
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6729075113932292
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.671630859375
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6718423631456163
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6719004313151042
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6711239284939237
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6715766059027778
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6724582248263888
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6719936794704862
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6712650722927518
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6714019775390625
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6716037326388888
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6715867784288194
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6718343098958333
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6720191107855903
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6716431511773003
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6718851725260417
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6715087890625
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6732974582248263
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6724870469835069
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6718063354492188
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6720089382595487
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6717885335286458
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6717970106336806
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6731211344401042
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.67144775390625
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6718330383300781
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6723319159613715
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.672760009765625
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6725379096137153
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6725565592447917
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6724667019314237
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6725569831000434
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6716812981499566
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6728040907118056
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6721700032552083
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6727786593967013
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6713629828559028
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6723997328016493
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6714036729600694
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.672943115234375
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6714731852213542
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6726294623480903
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6712578667534722
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6722200181749132
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6717105441623263
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.6715783013237847
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.6723836263020833
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.6719369888305664
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.6719512939453125
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.6719369888305664
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.6716327667236328
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.6713085174560547
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.6722583770751953
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.6720781326293945
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.6711235046386719
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.6724624633789062
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.6736068725585938
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.6730728149414062
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.66162109375
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.6728839874267578
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.650390625
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.6717732747395833
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.69140625
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.6608072916666667
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.6720352172851562
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.6724308558872768
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.671795436314174
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.6715611049107143
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.6724493844168526
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.6722063337053572
Final sparsity level of 0.672: 0.6719721792837193
wandb: WARNING W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.
2022-09-10 07:29:52,684 | INFO | main_after.py: 96 : main() ::	 Experiment Name: cv_svamp_augmented
2022-09-10 07:29:52,684 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:29:52,684 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:29:53,724 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:29:53,724 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 3932
2022-09-10 07:29:53,724 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 206
2022-09-10 07:29:53,724 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: ['0.01', '12.0', '1.0', '100.0', '0.1', '0.5', '3.0', '4.0', '7.0']
2022-09-10 07:29:53,724 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:29:53,724 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:29:54,300 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 4068 / 4068 = 1.0
2022-09-10 07:29:54,465 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 4071 words in input language, 21 words in output
2022-09-10 07:29:54,474 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:29:57,455 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:29:57,456 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:30:00,721 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:30:00,721 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/svamp/dense/models/cv_svamp_augmented_fold0.
2022-09-10 07:30:05,655 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:31:32,669 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.043689320388349516	
 equation acc epoch: 0.0	
 max val acc: 0.043689320388349516	
 equation acc: 0.0	
2022-09-10 07:31:32,669 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 1m 27s
2022-09-10 07:31:32,671 | INFO | main_after.py: 96 : main() ::	 Experiment Name: cv_svamp_augmented
2022-09-10 07:31:32,671 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:31:32,671 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:31:33,626 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:31:33,627 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 3920
2022-09-10 07:31:33,627 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 218
2022-09-10 07:31:33,627 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: ['0.01', '12.0', '1.0', '100.0', '0.1', '0.5', '3.0', '4.0', '7.0']
2022-09-10 07:31:33,627 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:31:33,627 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:31:34,195 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 4068 / 4068 = 1.0
2022-09-10 07:31:34,358 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 4071 words in input language, 21 words in output
2022-09-10 07:31:34,368 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:31:37,322 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:31:37,322 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:31:37,401 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:31:37,402 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/svamp/dense/models/cv_svamp_augmented_fold1.
2022-09-10 07:31:40,327 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:31:45,891 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.0	
 equation acc epoch: 0.0	
 max val acc: 0.0	
 equation acc: 0.0	
2022-09-10 07:31:45,892 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 5s
2022-09-10 07:31:45,894 | INFO | main_after.py: 96 : main() ::	 Experiment Name: cv_svamp_augmented
2022-09-10 07:31:45,894 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:31:45,894 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:31:46,928 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:31:46,928 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 3911
2022-09-10 07:31:46,928 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 227
2022-09-10 07:31:46,928 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: ['0.01', '12.0', '1.0', '100.0', '0.1', '0.5', '3.0', '4.0', '7.0']
2022-09-10 07:31:46,928 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:31:46,928 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:31:47,506 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 4068 / 4068 = 1.0
2022-09-10 07:31:47,673 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 4071 words in input language, 21 words in output
2022-09-10 07:31:47,684 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:31:50,491 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:31:50,491 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:31:50,567 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:31:50,567 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/svamp/dense/models/cv_svamp_augmented_fold2.
2022-09-10 07:31:53,750 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:33:09,911 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.11894273127753303	
 equation acc epoch: 0.0	
 max val acc: 0.11894273127753303	
 equation acc: 0.0	
2022-09-10 07:33:09,912 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 1m 16s
2022-09-10 07:33:09,914 | INFO | main_after.py: 96 : main() ::	 Experiment Name: cv_svamp_augmented
2022-09-10 07:33:09,914 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:33:09,914 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:33:10,974 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:33:10,974 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 3973
2022-09-10 07:33:10,975 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 165
2022-09-10 07:33:10,975 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: ['0.01', '12.0', '1.0', '100.0', '0.1', '0.5', '3.0', '4.0', '7.0']
2022-09-10 07:33:10,975 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:33:10,975 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:33:11,551 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 4068 / 4068 = 1.0
2022-09-10 07:33:11,722 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 4071 words in input language, 21 words in output
2022-09-10 07:33:11,731 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:33:14,606 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:33:14,606 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:33:14,684 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:33:14,684 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/svamp/dense/models/cv_svamp_augmented_fold3.
2022-09-10 07:33:16,699 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:33:20,703 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.012121212121212121	
 equation acc epoch: 0.0	
 max val acc: 0.012121212121212121	
 equation acc: 0.0	
2022-09-10 07:33:20,703 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 4s
2022-09-10 07:33:20,705 | INFO | main_after.py: 96 : main() ::	 Experiment Name: cv_svamp_augmented
2022-09-10 07:33:20,705 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:33:20,705 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:33:21,750 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:33:21,750 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 3954
2022-09-10 07:33:21,750 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 184
2022-09-10 07:33:21,750 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: ['0.01', '12.0', '1.0', '100.0', '0.1', '0.5', '3.0', '4.0', '7.0']
2022-09-10 07:33:21,750 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:33:21,750 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:33:22,327 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 4068 / 4068 = 1.0
2022-09-10 07:33:22,495 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 4071 words in input language, 21 words in output
2022-09-10 07:33:22,504 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:33:25,384 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:33:25,384 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:33:25,462 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:33:25,462 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/svamp/dense/models/cv_svamp_augmented_fold4.
2022-09-10 07:33:27,550 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:33:32,432 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.005434782608695652	
 equation acc epoch: 0.0	
 max val acc: 0.005434782608695652	
 equation acc: 0.0	
2022-09-10 07:33:32,432 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 4s
Traceback (most recent call last):
  File "/home/sliu/miniconda3/envs/prune_cry/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/sliu/miniconda3/envs/prune_cry/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/gpfs/home6/sliu/Projects/SVAMP/code/gts/src/main_after.py", line 276, in <module>
    main()
  File "/gpfs/home6/sliu/Projects/SVAMP/code/gts/src/main_after.py", line 266, in main
    folds_scores.append(float(best_acc[w][0])/best_acc[w][1])
ZeroDivisionError: float division by zero
Transfer numbers...
initialize by random pruning
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.7379357374664279
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.7387119001297017
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 0.70703125
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7383999294704862
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7378438313802083
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7385203043619792
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7378421359592013
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7381693522135417
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7381337483723958
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7381557888454862
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.73779296875
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7378150092230903
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7384745279947917
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7377069261338975
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7375653584798176
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7385728624131944
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7387237548828125
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7379235161675347
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7382032606336806
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7378078036838107
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7384016248914931
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7383049858940972
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7371266682942708
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7382100423177083
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7379557291666667
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7381557888454862
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.737900627983941
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7373979356553819
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7376302083333333
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7383066813151042
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7374318440755208
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7381104363335503
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7378285725911458
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7387847900390625
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7384355333116319
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7379506429036458
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7390407986111112
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.738037109375
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7379845513237847
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7385999891493056
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7382473415798612
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7374454074435763
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7376488579644097
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7380688985188801
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7378726535373263
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7375539143880208
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7375098334418403
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7382371690538194
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7382541232638888
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7379307217068143
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7378709581163194
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7383694118923612
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7377861870659722
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7383473714192708
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.73779296875
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7382091946072049
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7379451327853732
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.73785400390625
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7377183702256944
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7377878824869792
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7373843722873263
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7380362616644965
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7383160061306424
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7385406494140625
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7385830349392362
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7388186984592013
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7385389539930556
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7380867004394531
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7382019890679253
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7378658718532987
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7375352647569444
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7391628689236112
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7374827067057292
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7381680806477864
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.737841288248698
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.7380150689019097
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.738037109375
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.7380399703979492
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.7374528249104817
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.7370624542236328
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.7380757331848145
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.7379055023193359
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.7378172874450684
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.7386264801025391
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.7376022338867188
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.7378959655761719
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.7368927001953125
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.7381134033203125
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.742919921875
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.7385692596435547
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.740234375
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.7377688090006511
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.765625
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.7415364583333333
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.737640380859375
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.7383597237723214
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.7383041381835938
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.737103053501674
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.7387074061802456
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.7373199462890625
Final sparsity level of 0.738: 0.7379914220279615
Transfer numbers...
initialize by random pruning
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.7380319722139328
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.7394364056420233
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 0.7317708333333333
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7378726535373263
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7389984130859375
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7374369303385417
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7382100423177083
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7377594841851128
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7376191880967882
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7392306857638888
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7371843126085069
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.73876953125
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7378760443793403
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7377980550130208
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.738197750515408
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7375403510199653
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.73822021484375
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7366078694661458
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7379218207465278
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7381070454915364
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7380455864800347
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7371266682942708
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7381778293185763
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7385982937282987
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7372928195529513
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7378768920898438
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7382782830132378
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7381778293185763
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7386898464626737
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7383338080512153
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7376047770182292
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7383923000759549
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7382295396592882
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7380710177951388
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7388288709852431
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7381710476345487
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7384117974175347
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.738085428873698
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7381040785047743
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7377285427517362
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7389102511935763
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7375556098090278
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7389882405598958
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7381015353732638
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7382019890679253
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7375861273871528
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7388339572482638
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7384423149956597
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7371605767144097
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7383100721571181
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7382867601182725
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7372707790798612
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7384609646267362
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7386203342013888
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7382354736328125
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7377514309353299
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7383469475640191
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7378828260633681
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7373979356553819
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7384982638888888
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7385338677300347
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7379480997721355
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7378815544976128
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7378353542751737
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7383405897352431
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7386881510416667
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7369639078776042
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7379527621799045
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7378277248806424
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7387000189887153
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7382693820529513
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7376539442274306
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7381049262152778
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7375670539008247
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7381579081217449
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.7377522786458333
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.7380739847819011
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.7381162643432617
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.7377548217773438
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.7374458312988281
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.7384781837463379
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.7382431030273438
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.7381114959716797
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.7374486923217773
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.7379150390625
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.7377586364746094
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.7385978698730469
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.7383537292480469
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.73095703125
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.7382659912109375
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.75
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.7388394673665364
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.724609375
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.7389322916666667
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.738360813685826
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.737152099609375
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.7378747122628349
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.7388665335518974
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.7375139508928572
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.7389428274972099
Final sparsity level of 0.738: 0.7380538222319397
Transfer numbers...
initialize by random pruning
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.7380634460277198
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.7397353274967575
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 0.7604166666666666
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7371198866102431
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7381540934244792
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7373572455512153
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7374776204427083
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7379862467447917
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.737457275390625
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7376878526475694
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7384016248914931
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7383168538411458
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7375437418619792
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7379464043511285
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7378811306423612
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7384999593098958
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7363637288411458
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7384728325737847
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7386711968315972
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7379370795355903
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7383783128526475
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7374708387586806
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7380727132161458
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7368791368272569
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7386135525173612
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.738236321343316
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7380095587836372
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7379235161675347
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7368994818793403
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7385457356770833
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7385898166232638
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7380816141764324
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7376912434895833
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7367892795138888
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7375861273871528
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7379879421657987
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7390509711371528
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7377713521321614
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7381451924641926
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7374996609157987
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7380286322699653
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7376471625434028
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7379370795355903
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7377230326334636
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.737968020968967
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7378828260633681
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7383626302083333
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7380082872178819
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7378862169053819
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7380638122558594
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7376369900173612
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7375606960720487
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7377031114366319
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.738555908203125
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7377861870659722
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7377853393554688
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7382481892903645
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7375912136501737
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7384016248914931
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7378217909071181
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7372470431857638
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7381913926866319
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7380188835991753
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7371792263454862
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.738739013671875
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.73870849609375
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7387186686197917
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.738293965657552
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7378323872884114
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7390374077690972
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7385101318359375
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7384609646267362
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7373708089192708
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7378569708930122
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7376310560438368
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.737762451171875
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.7378946940104167
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.7381677627563477
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.7380987803141277
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.7379817962646484
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.7379097938537598
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.7385978698730469
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.7377762794494629
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.7381839752197266
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.7378005981445312
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.7369213104248047
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.7371826171875
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.736846923828125
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.73193359375
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.7385578155517578
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.7265625
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.7376658121744792
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.705078125
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.7265625
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.7376458304268974
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.7381700788225447
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.7379913330078125
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.7382823399135044
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.737717764718192
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.737225123814174
Final sparsity level of 0.738: 0.7379747039790812
Transfer numbers...
initialize by random pruning
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.7379730397642494
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.740234375
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 0.7513020833333334
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7377777099609375
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7379709879557292
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7385389539930556
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7379065619574653
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7380277845594618
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.737836201985677
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7374064127604167
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7378641764322917
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7377166748046875
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7378590901692708
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7384088304307725
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7378429836697049
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7376539442274306
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7384592692057292
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7376285129123263
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7380015055338542
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7380655076768663
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7372156778971355
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7381371392144097
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7382032606336806
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7381947835286458
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7384863959418403
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.738106197781033
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7380960252549913
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7393934461805556
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7385677761501737
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7378963894314237
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7371741400824653
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7382553948296441
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7382931179470487
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7372928195529513
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7381422254774306
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7373114691840278
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7378438313802083
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7380142211914062
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7376738654242622
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7376708984375
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7390407986111112
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7377370198567708
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7383388943142362
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7381668090820312
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7385016547309028
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7383982340494792
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7388966878255208
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7373911539713542
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7380794949001737
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7375348409016926
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7378404405381944
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7384660508897569
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7377556694878472
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7387305365668403
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7379031711154513
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7374098036024306
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7381028069390191
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7393459743923612
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7378353542751737
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7376200358072917
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7375725640190972
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7384071350097656
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.738543192545573
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.738372802734375
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7373996310763888
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7382456461588542
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.737762451171875
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7381337483723958
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.738302018907335
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.73834228515625
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7381676567925347
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7390543619791667
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7374420166015625
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7381731669108074
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7380680508083768
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.7386050754123263
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.7377395629882812
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.7383975982666016
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.7375876108805339
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.7377490997314453
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.7380771636962891
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.7378005981445312
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.738070011138916
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.7387857437133789
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.7387008666992188
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.7384243011474609
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.738311767578125
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.7386589050292969
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.738037109375
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.7384700775146484
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.748046875
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.737958272298177
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.734375
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.7467447916666667
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.7379117693219865
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.737685067313058
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.7375204903738839
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.7375946044921875
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.7388174874441964
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.7385613577706474
Final sparsity level of 0.738: 0.7380384956231525
Transfer numbers...
initialize by random pruning
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.737946798633907
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.7381799205577173
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 0.7578125
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7372724745008681
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7377268473307292
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7362111409505208
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7385389539930556
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7380867004394531
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7381248474121094
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7386067708333333
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7380082872178819
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7383304172092013
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7380862765842013
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7382189432779949
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.73786375257704
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7380794949001737
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7389102511935763
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7375132242838542
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7376658121744792
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7377459208170574
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7377433776855469
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7367774115668403
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7378879123263888
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7382880316840278
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7377827962239583
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7379171583387587
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7378523084852431
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7377251519097222
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7381981743706597
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7375996907552083
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7389475504557292
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7377764383951824
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7378807067871094
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7375098334418403
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7376386854383681
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7384219699435763
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7388983832465278
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7381231519911025
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7376153733995225
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7372283935546875
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7374776204427083
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7374708387586806
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7383744981553819
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7382244533962674
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7379235161675347
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7374098036024306
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7375369601779513
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7390933566623263
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7383812798394097
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.737809075249566
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7378828260633681
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.738067626953125
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7380235460069444
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7381456163194444
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7373029920789931
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7379942999945747
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7382265726725261
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7384745279947917
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7382998996310763
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7384660508897569
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7388170030381944
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7384863959418403
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7375297546386719
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7386050754123263
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7379625108506944
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7389984130859375
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7373385959201388
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7383054097493489
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7374907599555122
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7394290500217013
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7378251817491319
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7388458251953125
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7373470730251737
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7383689880371094
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.737749735514323
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.7378946940104167
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.7385724385579426
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.7380647659301758
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.7380975087483723
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.7383508682250977
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.7375731468200684
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.7373390197753906
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.7381868362426758
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.7382183074951172
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.7374000549316406
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.7385902404785156
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.73907470703125
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.7391109466552734
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.730224609375
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.7386951446533203
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.73046875
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.7383931477864583
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.759765625
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.7239583333333333
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.738201686314174
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.7380262102399553
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.738020760672433
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.7375967843191964
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.7383226667131697
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.7379738943917411
Final sparsity level of 0.738: 0.7379907090014461
wandb: WARNING W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.
2022-09-10 07:33:35,613 | INFO | main_after.py: 96 : main() ::	 Experiment Name: cv_svamp_augmented
2022-09-10 07:33:35,613 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:33:35,613 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:33:36,657 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:33:36,657 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 3932
2022-09-10 07:33:36,657 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 206
2022-09-10 07:33:36,658 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: ['0.01', '12.0', '1.0', '100.0', '0.1', '0.5', '3.0', '4.0', '7.0']
2022-09-10 07:33:36,658 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:33:36,658 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:33:37,237 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 4068 / 4068 = 1.0
2022-09-10 07:33:37,401 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 4071 words in input language, 21 words in output
2022-09-10 07:33:37,409 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:33:40,387 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:33:40,387 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:33:43,688 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:33:43,688 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/svamp/dense/models/cv_svamp_augmented_fold0.
2022-09-10 07:33:45,767 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:34:22,944 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.03398058252427184	
 equation acc epoch: 0.0	
 max val acc: 0.03398058252427184	
 equation acc: 0.0	
2022-09-10 07:34:22,944 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 37s
2022-09-10 07:34:22,947 | INFO | main_after.py: 96 : main() ::	 Experiment Name: cv_svamp_augmented
2022-09-10 07:34:22,948 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:34:22,948 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:34:23,916 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:34:23,916 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 3920
2022-09-10 07:34:23,916 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 218
2022-09-10 07:34:23,916 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: ['0.01', '12.0', '1.0', '100.0', '0.1', '0.5', '3.0', '4.0', '7.0']
2022-09-10 07:34:23,916 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:34:23,916 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:34:24,490 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 4068 / 4068 = 1.0
2022-09-10 07:34:24,747 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 4071 words in input language, 21 words in output
2022-09-10 07:34:24,758 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:34:27,731 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:34:27,731 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:34:27,809 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:34:27,809 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/svamp/dense/models/cv_svamp_augmented_fold1.
2022-09-10 07:34:29,726 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:34:34,272 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.0	
 equation acc epoch: 0.0	
 max val acc: 0.0	
 equation acc: 0.0	
2022-09-10 07:34:34,272 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 4s
2022-09-10 07:34:34,273 | INFO | main_after.py: 96 : main() ::	 Experiment Name: cv_svamp_augmented
2022-09-10 07:34:34,273 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:34:34,273 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:34:35,240 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:34:35,240 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 3911
2022-09-10 07:34:35,240 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 227
2022-09-10 07:34:35,240 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: ['0.01', '12.0', '1.0', '100.0', '0.1', '0.5', '3.0', '4.0', '7.0']
2022-09-10 07:34:35,240 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:34:35,240 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:34:35,823 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 4068 / 4068 = 1.0
2022-09-10 07:34:36,067 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 4071 words in input language, 21 words in output
2022-09-10 07:34:36,078 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:34:38,956 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:34:38,956 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:34:39,033 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:34:39,033 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/svamp/dense/models/cv_svamp_augmented_fold2.
2022-09-10 07:34:40,869 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:36:21,016 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.11894273127753303	
 equation acc epoch: 0.0	
 max val acc: 0.11894273127753303	
 equation acc: 0.0	
2022-09-10 07:36:21,016 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 1m 40s
2022-09-10 07:36:21,018 | INFO | main_after.py: 96 : main() ::	 Experiment Name: cv_svamp_augmented
2022-09-10 07:36:21,018 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:36:21,018 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:36:21,985 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:36:21,986 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 3973
2022-09-10 07:36:21,986 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 165
2022-09-10 07:36:21,986 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: ['0.01', '12.0', '1.0', '100.0', '0.1', '0.5', '3.0', '4.0', '7.0']
2022-09-10 07:36:21,986 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:36:21,986 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:36:22,560 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 4068 / 4068 = 1.0
2022-09-10 07:36:22,726 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 4071 words in input language, 21 words in output
2022-09-10 07:36:22,735 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:36:25,688 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:36:25,688 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:36:25,765 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:36:25,765 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/svamp/dense/models/cv_svamp_augmented_fold3.
2022-09-10 07:36:31,096 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:36:35,054 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.006060606060606061	
 equation acc epoch: 0.0	
 max val acc: 0.006060606060606061	
 equation acc: 0.0	
2022-09-10 07:36:35,054 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 3s
2022-09-10 07:36:35,056 | INFO | main_after.py: 96 : main() ::	 Experiment Name: cv_svamp_augmented
2022-09-10 07:36:35,056 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:36:35,057 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:36:36,098 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:36:36,098 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 3954
2022-09-10 07:36:36,098 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 184
2022-09-10 07:36:36,098 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: ['0.01', '12.0', '1.0', '100.0', '0.1', '0.5', '3.0', '4.0', '7.0']
2022-09-10 07:36:36,098 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:36:36,098 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:36:36,672 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 4068 / 4068 = 1.0
2022-09-10 07:36:36,834 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 4071 words in input language, 21 words in output
2022-09-10 07:36:36,843 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:36:39,668 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:36:39,669 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:36:39,745 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:36:39,745 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/svamp/dense/models/cv_svamp_augmented_fold4.
2022-09-10 07:36:42,565 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:36:58,737 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.021739130434782608	
 equation acc epoch: 0.0	
 max val acc: 0.021739130434782608	
 equation acc: 0.0	
2022-09-10 07:36:58,738 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 16s
Traceback (most recent call last):
  File "/home/sliu/miniconda3/envs/prune_cry/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/sliu/miniconda3/envs/prune_cry/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/gpfs/home6/sliu/Projects/SVAMP/code/gts/src/main_after.py", line 276, in <module>
    main()
  File "/gpfs/home6/sliu/Projects/SVAMP/code/gts/src/main_after.py", line 266, in main
    folds_scores.append(float(best_acc[w][0])/best_acc[w][1])
ZeroDivisionError: float division by zero
Transfer numbers...
initialize by random pruning
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.7909576639643224
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.7911498865110247
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 0.7708333333333334
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7916429307725694
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7907443576388888
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7915717230902778
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7909122043185763
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7911160786946615
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7909978230794271
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7913750542534722
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7903069390190972
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7907918294270834
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7910919189453125
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7908172607421875
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7905917697482638
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7916598849826388
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7911834716796875
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7907918294270834
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7910478379991319
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7906964619954427
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7914704216851128
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7914852566189237
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7902357313368056
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7909461127387153
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7906409369574653
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7911449008517795
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7909117804633247
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7905917697482638
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7906816270616319
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7912190755208334
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7900136311848959
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.790994856092665
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7907464769151475
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7915089925130209
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7912512885199653
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7912411159939237
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7921990288628472
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7909588283962674
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7911343044704862
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7910122341579862
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.791168212890625
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7905714246961806
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7903713650173612
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7909478081597222
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7909130520290799
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7907884385850694
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7910274929470487
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7914903428819444
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7910919189453125
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7908448113335503
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7910304599338107
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7913631863064237
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7912360297309028
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7911088731553819
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7904103597005209
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7912567986382378
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7910152011447482
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7908952501085069
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7908528645833334
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7909274631076388
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7900577121310763
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.790930430094401
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7912360297309028
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7917955186631944
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7916073269314237
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7922227647569444
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7916700575086806
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7913076612684462
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7913199530707465
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7909155951605903
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7905137803819444
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7920566134982638
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7904035780164931
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7909550136990018
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.791205088297526
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.7910885281032987
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.7912127176920573
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.7907800674438477
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.7903060913085938
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.7902326583862305
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.790916919708252
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.7909259796142578
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.7908449172973633
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.7914829254150391
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.7901039123535156
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.7909431457519531
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.7896461486816406
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.7910518646240234
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.795166015625
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.7915897369384766
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.78125
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.790948232014974
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.806640625
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.7916666666666666
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.7903714861188615
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.7910940987723214
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.7910962785993303
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.7901044573102678
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.7917502267020089
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.7907148088727678
Final sparsity level of 0.791: 0.7909887169022783
Transfer numbers...
initialize by random pruning
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.7910199121738122
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.7924114380674449
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 0.7825520833333334
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7905595567491319
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7917599148220487
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7899831136067709
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7911037868923612
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.790802001953125
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7905201382107205
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7921651204427084
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7905137803819444
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7915140787760416
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7903951009114584
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7907456292046441
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7909762064615885
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7912818060980903
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7905646430121528
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7897067599826388
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7908664279513888
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7909495035807291
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7911194695366753
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7902187771267362
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7915361192491319
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7911953396267362
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7901594373914931
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.79090330335829
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7912250094943576
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7913140190972222
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7916090223524306
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7913903130425347
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7904832628038194
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7913996378580729
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7911114162868924
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7910071478949653
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7919125027126737
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7915869818793403
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7911800808376737
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7910571628146701
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7910508049858941
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7908664279513888
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7917565239800347
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7913343641493056
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7917463514539931
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.791235605875651
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7910482618543837
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7905595567491319
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7916276719835069
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7916480170355903
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7901357014973959
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7911970350477431
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7910787794325087
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7902662489149306
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7912953694661459
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.79193115234375
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7914360894097222
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7910601298014323
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7912712097167969
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7906409369574653
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7904527452256944
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7914445665147569
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7914021809895834
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7912182278103299
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.790771484375
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7907952202690972
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7911970350477431
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7912801106770834
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7899288601345487
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.791282229953342
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7908583747016059
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7915429009331597
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7906392415364584
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7905815972222222
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7910817464192709
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.790726555718316
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7912835015190972
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.790771484375
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.7909170786539713
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.7912750244140625
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.7907619476318359
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.7905492782592773
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.7913379669189453
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.7912778854370117
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.7910361289978027
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.7905664443969727
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.7910652160644531
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.7906646728515625
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.7914886474609375
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.7916488647460938
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.782958984375
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.7914524078369141
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.8046875
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.7915166219075521
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.79296875
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.7955729166666666
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.7910864693777901
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.790480477469308
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.7906821114676339
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.791980198451451
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.7902799333844865
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.7917894635881697
Final sparsity level of 0.791: 0.7910372373182627
Transfer numbers...
initialize by random pruning
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.7910668768029444
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.7921631809338521
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 0.81640625
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7902815077039931
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7912478976779513
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7903391520182291
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7906426323784722
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7910792032877604
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7905930413140191
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7906392415364584
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7912241617838541
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7912919786241319
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7901967366536459
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7909927368164062
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7909113566080729
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7909868028428819
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7900034586588541
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7914818657769097
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7917395697699653
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7909842597113715
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7913975185818143
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7908257378472222
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7916225857204862
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7897542317708334
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7911461724175347
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7914500766330295
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7909715440538194
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7908647325303819
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7905731201171875
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7910410563151041
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7918141682942709
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7911805046929253
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7907011244032118
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7901068793402778
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7907647026909722
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7907223171657987
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7919786241319444
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7906909518771701
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7911482916937934
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7908003065321181
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7910325792100694
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7908681233723959
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7913004557291666
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.791015625
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7907803853352865
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7911597357855903
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7914089626736112
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7910987006293403
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7912851969401041
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7908138699001737
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.790679931640625
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7908969455295138
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7905053032769097
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7914276123046875
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7907596164279513
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7907011244032118
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7911720275878906
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7906256781684028
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7915920681423612
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7910393608940972
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7899814181857638
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7911542256673177
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7907782660590278
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7898830837673612
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7915818956163194
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7917158338758681
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7916531032986112
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7916272481282552
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7908749050564237
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7911800808376737
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7913614908854166
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7912072075737847
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7905866834852431
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7909575568305122
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7906744215223525
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.7910614013671875
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.7907765706380209
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.7910356521606445
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.7910798390706381
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.7909727096557617
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.7907938957214355
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.7916965484619141
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.7907147407531738
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.7912311553955078
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.7919845581054688
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.7895603179931641
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.7900810241699219
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.7900009155273438
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.78271484375
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.7912063598632812
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.7890625
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.7905731201171875
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.759765625
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.783203125
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.7903540475027901
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.7910723005022322
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.7910810198102678
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.791001456124442
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.7910635811941964
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.7903475080217635
Final sparsity level of 0.791: 0.7909837257166705
Transfer numbers...
initialize by random pruning
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.7910298076444843
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.7922594439040207
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 0.8046875
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7908104790581597
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7910647922092013
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7917853461371528
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.791046142578125
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.791184319390191
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7906816270616319
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7904120551215278
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7909189860026041
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7900407579210069
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7910139295789931
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7914704216851128
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7910783555772569
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7907375759548612
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7916836208767362
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7905782063802084
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7905375162760416
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7910563151041666
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7904680040147569
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7914055718315972
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7913614908854166
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7904442681206597
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7911716037326388
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.790948232014974
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7910249498155382
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7922837999131944
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7914547390407987
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7907307942708334
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7903578016493056
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7909677293565538
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7913424173990885
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.79034423828125
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7910800509982638
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7904527452256944
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7905849880642362
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7910609775119357
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7908821105957031
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7907053629557291
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7919633653428819
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7910614013671875
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7911207411024306
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.791001213921441
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7915899488661025
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7912105984157987
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7917717827690972
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7908698187934028
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7908223470052084
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7907600402832031
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7907112969292535
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7913903130425347
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7908952501085069
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7917836507161459
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.79095458984375
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7904171413845487
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7909452650282118
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7920600043402778
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7905375162760416
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7906324598524306
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7910800509982638
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7914632161458334
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7911864386664497
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7910308837890625
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7905595567491319
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7911716037326388
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7906155056423612
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7908867730034722
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7911771138509115
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7911614312065972
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7911020914713541
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7921905517578125
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7907053629557291
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7911796569824219
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7910503811306424
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.7914072672526041
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.7908967336018881
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.791325569152832
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.7904605865478516
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.7907981872558594
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.7911715507507324
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.7913665771484375
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.7912383079528809
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.7919368743896484
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.7911643981933594
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.7914810180664062
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.790618896484375
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.7916507720947266
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.799072265625
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.7916374206542969
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.810546875
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.7911554972330729
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.7734375
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.7981770833333334
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.7908096313476562
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.7909894670758928
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.7903071812220982
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.790740966796875
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.791579110281808
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.791308811732701
Final sparsity level of 0.791: 0.7910436891698389
Transfer numbers...
initialize by random pruning
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.7909644250058026
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.7912892144941635
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 0.8177083333333334
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7908155653211806
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7911716037326388
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7897084554036459
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7912411159939237
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7912135654025607
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7910855611165365
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7917107476128472
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7912326388888888
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7911495632595487
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7914208306206597
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7912326388888888
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7907702128092448
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7911343044704862
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7917666965060763
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7907087537977431
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7908494737413194
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.790838877360026
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7906053331163194
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7903967963324653
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7908121744791666
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7915378146701388
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7907986111111112
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7910965813530816
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7907625834147135
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7906782362196181
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7905205620659722
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7905544704861112
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7915920681423612
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7908545600043403
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7908939785427518
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7906850179036459
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7906951904296875
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7911716037326388
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7918412950303819
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7911885579427084
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7906922234429253
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.790191650390625
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7906968858506944
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7909511990017362
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7915191650390625
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7912309434678819
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7909431457519531
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7903323703342013
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.791046142578125
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7918209499782987
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7905426025390625
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7908147176106771
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7908274332682291
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7909681532118056
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7907748752170138
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7904018825954862
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7905069986979166
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.791030036078559
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7911877102322049
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7914191351996528
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7914615207248263
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7910953097873263
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7917005750868056
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7913466559516059
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.790679931640625
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7914513481987847
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7907952202690972
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7916751437717013
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7902764214409722
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.791182623969184
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7904997931586372
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7922549777560763
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7907443576388888
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7917887369791666
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7905951605902778
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7913085089789497
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7909244961208768
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.7910122341579862
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.7916463216145834
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.7914276123046875
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.7911485036214193
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.7913599014282227
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.7905135154724121
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.7903032302856445
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.7911872863769531
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.7914028167724609
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.7910118103027344
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.7917404174804688
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.791717529296875
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.791595458984375
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.7841796875
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.7921314239501953
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.78125
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.7914772033691406
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.7890625
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.77734375
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.7911202566964286
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.7908041817801339
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.7910221644810268
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.7905011858258928
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.7912368774414062
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.7912532261439732
Final sparsity level of 0.791: 0.7910033512523127
wandb: WARNING W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.
2022-09-10 07:37:02,104 | INFO | main_after.py: 96 : main() ::	 Experiment Name: cv_svamp_augmented
2022-09-10 07:37:02,104 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:37:02,104 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:37:03,135 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:37:03,135 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 3932
2022-09-10 07:37:03,135 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 206
2022-09-10 07:37:03,136 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: ['0.01', '12.0', '1.0', '100.0', '0.1', '0.5', '3.0', '4.0', '7.0']
2022-09-10 07:37:03,136 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:37:03,136 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:37:03,713 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 4068 / 4068 = 1.0
2022-09-10 07:37:03,880 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 4071 words in input language, 21 words in output
2022-09-10 07:37:03,889 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:37:07,100 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:37:07,100 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:37:10,494 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:37:10,494 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/svamp/dense/models/cv_svamp_augmented_fold0.
2022-09-10 07:37:12,969 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:37:17,591 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.043689320388349516	
 equation acc epoch: 0.0	
 max val acc: 0.043689320388349516	
 equation acc: 0.0	
2022-09-10 07:37:17,591 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 4s
2022-09-10 07:37:17,594 | INFO | main_after.py: 96 : main() ::	 Experiment Name: cv_svamp_augmented
2022-09-10 07:37:17,594 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:37:17,594 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:37:18,547 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:37:18,547 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 3920
2022-09-10 07:37:18,547 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 218
2022-09-10 07:37:18,547 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: ['0.01', '12.0', '1.0', '100.0', '0.1', '0.5', '3.0', '4.0', '7.0']
2022-09-10 07:37:18,547 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:37:18,547 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:37:19,116 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 4068 / 4068 = 1.0
2022-09-10 07:37:19,277 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 4071 words in input language, 21 words in output
2022-09-10 07:37:19,288 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:37:22,243 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:37:22,243 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:37:22,325 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:37:22,325 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/svamp/dense/models/cv_svamp_augmented_fold1.
2022-09-10 07:37:24,497 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:37:28,869 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.0	
 equation acc epoch: 0.0	
 max val acc: 0.0	
 equation acc: 0.0	
2022-09-10 07:37:28,869 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 4s
2022-09-10 07:37:28,871 | INFO | main_after.py: 96 : main() ::	 Experiment Name: cv_svamp_augmented
2022-09-10 07:37:28,871 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:37:28,871 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:37:29,822 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:37:29,822 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 3911
2022-09-10 07:37:29,822 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 227
2022-09-10 07:37:29,822 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: ['0.01', '12.0', '1.0', '100.0', '0.1', '0.5', '3.0', '4.0', '7.0']
2022-09-10 07:37:29,822 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:37:29,822 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:37:30,401 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 4068 / 4068 = 1.0
2022-09-10 07:37:30,568 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 4071 words in input language, 21 words in output
2022-09-10 07:37:30,582 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:37:33,431 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:37:33,431 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:37:33,509 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:37:33,509 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/svamp/dense/models/cv_svamp_augmented_fold2.
2022-09-10 07:37:35,474 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:39:14,511 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.0	
 equation acc epoch: 0.0	
 max val acc: 0.0	
 equation acc: 0.0	
2022-09-10 07:39:14,511 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 1m 39s
2022-09-10 07:39:14,513 | INFO | main_after.py: 96 : main() ::	 Experiment Name: cv_svamp_augmented
2022-09-10 07:39:14,513 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:39:14,513 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:39:15,471 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:39:15,471 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 3973
2022-09-10 07:39:15,471 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 165
2022-09-10 07:39:15,471 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: ['0.01', '12.0', '1.0', '100.0', '0.1', '0.5', '3.0', '4.0', '7.0']
2022-09-10 07:39:15,471 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:39:15,471 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:39:16,046 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 4068 / 4068 = 1.0
2022-09-10 07:39:16,219 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 4071 words in input language, 21 words in output
2022-09-10 07:39:16,229 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:39:19,106 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:39:19,106 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:39:19,187 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:39:19,187 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/svamp/dense/models/cv_svamp_augmented_fold3.
2022-09-10 07:39:24,720 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:39:28,467 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.01818181818181818	
 equation acc epoch: 0.0	
 max val acc: 0.01818181818181818	
 equation acc: 0.0	
2022-09-10 07:39:28,467 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 3s
2022-09-10 07:39:28,469 | INFO | main_after.py: 96 : main() ::	 Experiment Name: cv_svamp_augmented
2022-09-10 07:39:28,469 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:39:28,469 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:39:29,423 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:39:29,423 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 3954
2022-09-10 07:39:29,423 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 184
2022-09-10 07:39:29,423 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: ['0.01', '12.0', '1.0', '100.0', '0.1', '0.5', '3.0', '4.0', '7.0']
2022-09-10 07:39:29,423 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:39:29,423 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:39:29,998 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 4068 / 4068 = 1.0
2022-09-10 07:39:30,170 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 4071 words in input language, 21 words in output
2022-09-10 07:39:30,180 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:39:33,017 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:39:33,017 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:39:33,094 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:39:33,094 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/svamp/dense/models/cv_svamp_augmented_fold4.
2022-09-10 07:39:38,030 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:39:42,970 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.021739130434782608	
 equation acc epoch: 0.0	
 max val acc: 0.021739130434782608	
 equation acc: 0.0	
2022-09-10 07:39:42,970 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 4s
Traceback (most recent call last):
  File "/home/sliu/miniconda3/envs/prune_cry/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/sliu/miniconda3/envs/prune_cry/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/gpfs/home6/sliu/Projects/SVAMP/code/gts/src/main_after.py", line 276, in <module>
    main()
  File "/gpfs/home6/sliu/Projects/SVAMP/code/gts/src/main_after.py", line 266, in main
    folds_scores.append(float(best_acc[w][0])/best_acc[w][1])
ZeroDivisionError: float division by zero
Transfer numbers...
initialize by random pruning
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.8324397619698929
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.8330648103112841
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 0.8229166666666666
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8329484727647569
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8322550455729166
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8324449327256944
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8322736952039931
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8325614929199219
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8324627346462674
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.832763671875
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8324839274088541
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8322279188368056
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8327450222439237
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8323614332411025
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8320566813151041
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8332536485460069
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8325992160373263
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8324618869357638
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8328365749782987
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8322881062825521
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8329391479492188
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8327178955078125
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8319922553168403
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8321702745225694
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8317447238498263
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.832617441813151
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8321639166937934
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8318837483723959
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8323092990451388
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8326755099826388
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8317192925347222
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8324250115288628
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8322419060601128
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8327568901909722
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8321109347873263
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8325415717230903
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8335757785373263
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8323800828721788
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8326267666286893
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8322804768880209
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8329789903428819
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8320024278428819
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8322923448350694
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8323343065049913
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.832405514187283
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8323296440972222
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.83258056640625
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8328755696614584
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8327891031901041
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8326165941026475
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8325712415907118
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.832611083984375
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8327280680338541
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8324432373046875
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8323449028862847
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8328200446234809
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.832574208577474
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8324347601996528
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8323347303602431
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8327043321397569
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8319447835286459
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8323656717936198
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.832665761311849
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8332061767578125
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8331858317057291
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8336825900607638
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8330468071831597
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.832940419514974
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8328971862792969
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8326195610894097
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8317006429036459
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8333231608072916
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8321448432074653
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8325085110134549
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8325360616048177
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.8329366048177084
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.8325602213541666
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.8324785232543945
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.8317991892496744
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.8320045471191406
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.8324289321899414
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.8329935073852539
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.8323512077331543
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.8330745697021484
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.8318672180175781
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.8326492309570312
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.8313636779785156
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.8323307037353516
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.839111328125
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.8336277008056641
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.8046875
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.8323186238606771
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.849609375
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.8330078125
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.8321609497070312
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.8324454171316964
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.8325849260602678
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.8319157191685268
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.832871573311942
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.8324301583426339
Final sparsity level of 0.8325: 0.8324916020706179
Transfer numbers...
initialize by random pruning
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.8325530936038994
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.8336297219520103
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 0.8294270833333334
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8320176866319444
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8333841959635416
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8318244086371528
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8327704535590278
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8323601616753472
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8320295545789931
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8335537380642362
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8320126003689237
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.83306884765625
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8317413330078125
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8324398464626737
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8324856228298612
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8325093587239584
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8317820231119791
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8313717312282987
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8322245279947916
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8323347303602431
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.832708994547526
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8314480251736112
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8332197401258681
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8328196207682291
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8316192626953125
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8325801425509982
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8325373331705729
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8327094184027778
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8327484130859375
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8329213460286459
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8317955864800347
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8327823215060763
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8326886494954427
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8320685492621528
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8333350287543403
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8328264024522569
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8327738444010416
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.832681867811415
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8326805962456597
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8326331244574653
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8331264919704862
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8324483235677084
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8328806559244791
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.832680172390408
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8325551350911459
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8324907090928819
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8330976698133681
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8332621256510416
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8319346110026041
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8326869540744357
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8326738145616319
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8319651285807291
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8325975206163194
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8331756591796875
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8322363959418403
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8323741488986545
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.832511478000217
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8324466281467013
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8320075141059028
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8328416612413194
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8324635823567709
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.832541995578342
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8324317932128906
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8322669135199653
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8326314290364584
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.832763671875
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8321058485243056
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8326810201009115
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8324911329481337
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8327772352430556
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8320838080512153
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8321516248914931
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8326348198784722
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8323008219401041
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8326627943250868
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.8320465087890625
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.8323631286621094
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.8324861526489258
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.8323135375976562
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.8320713043212891
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.8330702781677246
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.8324623107910156
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.8324108123779297
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.8323831558227539
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.8326072692871094
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.8322639465332031
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.8327560424804688
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.8331432342529297
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.827392578125
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.8327827453613281
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.837890625
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.8328094482421875
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.837890625
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.8349609375
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.8325086321149553
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.8320890154157365
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.8327364240373885
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.8332421439034599
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.8318405151367188
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.832897731236049
Final sparsity level of 0.8325: 0.8325342867453162
Transfer numbers...
initialize by random pruning
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.8325507622102855
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.8335182595654993
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 0.8502604166666666
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8320634629991319
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8326009114583334
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8316362169053819
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8321007622612847
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8325797186957465
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8322550455729166
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8321380615234375
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8326958550347222
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8327467176649306
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8316023084852431
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8322253757052951
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8324881659613715
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8326161702473959
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8315921359592013
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8329230414496528
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8328484429253472
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8323330349392362
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8329832288953993
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8324788411458334
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8328772650824653
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8315260145399306
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8324873182508681
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8328365749782987
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8323554992675781
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8325449625651041
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8320736355251737
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8329298231336806
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8328586154513888
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8326331244574653
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8323847452799479
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8314548068576388
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8323279486762153
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8323838975694444
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8332061767578125
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8324089050292969
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8325178358289931
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8325398763020834
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8327433268229166
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8325602213541666
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8327297634548612
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8325729370117188
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8324788411458334
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8321584065755209
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8327857123480903
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8324330647786459
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8324500189887153
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8321024576822916
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8323063320583768
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8325958251953125
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8318803575303819
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8330027262369791
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.832427978515625
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8323228624131944
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8324436611599393
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8321974012586806
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.83251953125
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8325466579861112
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8318210177951388
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8326958550347222
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8322347005208334
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8313835991753472
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8332553439670138
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.83282470703125
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8332044813368056
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8328950670030382
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8323728773328993
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8328297932942709
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8330162896050347
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8325517442491319
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8318803575303819
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8325076633029513
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8324500189887153
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.8320414225260416
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.8322277069091797
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.832829475402832
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.8325614929199219
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.8323497772216797
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.832496166229248
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.8331508636474609
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.8323850631713867
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.8327875137329102
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.8338661193847656
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.8314476013183594
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.8310546875
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.8316707611083984
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.826904296875
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.8325519561767578
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.833984375
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.8324305216471354
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.814453125
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.8238932291666666
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.8321783883231026
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.832657950265067
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.8324966430664062
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.8326208932059151
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.8326710292271206
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.8324824741908482
Final sparsity level of 0.8325: 0.8324896914364601
Transfer numbers...
initialize by random pruning
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.8325490525216354
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.8339235773346303
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 0.83984375
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8323398166232638
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8329806857638888
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8333536783854166
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8329891628689237
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8327276441786025
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8324317932128906
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8319871690538194
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8327518039279513
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8317057291666666
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8327094184027778
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8327996995713975
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8324241638183594
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8323415120442709
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8331383599175347
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8321567111545138
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8321516248914931
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8324347601996528
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8320583767361112
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8325076633029513
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8325466579861112
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8323567708333334
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8324652777777778
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8325008816189237
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8325513203938802
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.833282470703125
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8329739040798612
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8326432969835069
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8320990668402778
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8324250115288628
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8327445983886719
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8324534098307291
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8327772352430556
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8318345811631944
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8323025173611112
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8328115675184462
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8326004876030816
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8319786919487847
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8331061469184028
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8325229220920138
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8324618869357638
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8326814439561632
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8329556783040365
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8324873182508681
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8332858615451388
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8323550754123263
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8327043321397569
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8322313096788194
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8322940402560763
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8327043321397569
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8324195014105903
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8330942789713541
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8320465087890625
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.831841786702474
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8322122361924913
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8333214653862847
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8323109944661459
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8319803873697916
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8323906792534722
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8327471415201823
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8326263427734375
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8321211073133681
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8321058485243056
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8324822319878472
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8321533203125
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8323512607150607
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8325958251953125
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8320736355251737
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8329077826605903
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8338962131076388
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8323703342013888
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8325975206163194
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8325229220920138
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.8329993353949653
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.8322982788085938
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.8327951431274414
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.8320178985595703
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.8322858810424805
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.8326878547668457
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.8328456878662109
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.8328619003295898
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.8332395553588867
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.8326911926269531
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.8326625823974609
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.8321647644042969
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.8328113555908203
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.837158203125
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.8331832885742188
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.84375
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.8326034545898438
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.8359375
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.8440755208333334
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.8323113577706474
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.8324846540178572
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.8320083618164062
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.8321282523018974
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.8326525006975447
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.8323538643973214
Final sparsity level of 0.8325: 0.832537706503555
Transfer numbers...
initialize by random pruning
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.832500067351371
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.8329406817444877
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 0.85546875
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8319939507378472
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8328196207682291
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8309885660807291
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8326890733506944
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8327200147840712
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.83259031507704
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8332366943359375
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8328738742404513
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8327840169270834
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8324500189887153
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8326521979437934
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8322991265190972
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8323652479383681
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8331129286024306
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8321261935763888
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8323245578342013
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8324559529622396
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8321558634440104
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8319532606336806
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8329094780815972
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8332400851779513
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8318871392144097
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8324864705403646
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8322809007432725
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8319007025824653
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8321024576822916
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8316786024305556
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8332756890190972
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8325038486056857
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8322249518500434
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8324924045138888
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8326009114583334
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.832550048828125
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.832855224609375
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8326950073242188
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8323872884114584
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8317650689019097
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8323211669921875
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8328840467664931
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8336673312717013
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8326920403374566
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8324385748969184
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8315955268012153
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8324262830946181
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8332722981770834
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8325161404079862
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8324411180284288
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8321766323513455
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8325093587239584
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8326924641927084
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8317837185329862
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8320956759982638
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8325775994194878
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8326852586534288
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8328620062934028
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8329959445529513
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8329925537109375
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8331264919704862
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8328191969129775
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8322452969021268
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8326195610894097
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8325602213541666
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8326127794053819
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8314429389105903
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8326958550347222
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8323071797688802
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8334096272786459
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8324364556206597
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8330349392361112
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8319769965277778
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8330086602105035
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8323114183213975
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.8328026665581597
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.8331368764241537
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.8331384658813477
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.8326409657796224
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.832737922668457
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.8322043418884277
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.8317680358886719
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.8328418731689453
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.8327198028564453
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.832366943359375
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.8331394195556641
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.8327522277832031
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.8327102661132812
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.829833984375
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.8336219787597656
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.822265625
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.8328514099121094
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.8359375
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.8323567708333334
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.8324955531529018
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.8321871076311385
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.8325881958007812
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.8321892874581474
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.8326644897460938
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.8326394217354911
Final sparsity level of 0.8325: 0.832522919856595
wandb: WARNING W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.
2022-09-10 07:39:46,372 | INFO | main_after.py: 96 : main() ::	 Experiment Name: cv_svamp_augmented
2022-09-10 07:39:46,372 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:39:46,372 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:39:47,411 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:39:47,411 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 3932
2022-09-10 07:39:47,412 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 206
2022-09-10 07:39:47,412 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: ['0.01', '12.0', '1.0', '100.0', '0.1', '0.5', '3.0', '4.0', '7.0']
2022-09-10 07:39:47,412 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:39:47,412 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:39:47,987 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 4068 / 4068 = 1.0
2022-09-10 07:39:48,151 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 4071 words in input language, 21 words in output
2022-09-10 07:39:48,160 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:39:51,286 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:39:51,287 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:39:54,607 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:39:54,607 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/svamp/dense/models/cv_svamp_augmented_fold0.
2022-09-10 07:40:00,244 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:40:04,865 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.03398058252427184	
 equation acc epoch: 0.0	
 max val acc: 0.03398058252427184	
 equation acc: 0.0	
2022-09-10 07:40:04,865 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 4s
2022-09-10 07:40:04,867 | INFO | main_after.py: 96 : main() ::	 Experiment Name: cv_svamp_augmented
2022-09-10 07:40:04,867 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:40:04,867 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:40:05,826 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:40:05,826 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 3920
2022-09-10 07:40:05,826 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 218
2022-09-10 07:40:05,826 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: ['0.01', '12.0', '1.0', '100.0', '0.1', '0.5', '3.0', '4.0', '7.0']
2022-09-10 07:40:05,826 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:40:05,826 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:40:06,396 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 4068 / 4068 = 1.0
2022-09-10 07:40:06,559 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 4071 words in input language, 21 words in output
2022-09-10 07:40:06,569 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:40:09,530 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:40:09,530 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:40:09,610 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:40:09,611 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/svamp/dense/models/cv_svamp_augmented_fold1.
2022-09-10 07:40:15,128 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:40:19,539 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.009174311926605505	
 equation acc epoch: 0.0	
 max val acc: 0.009174311926605505	
 equation acc: 0.0	
2022-09-10 07:40:19,539 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 4s
2022-09-10 07:40:19,542 | INFO | main_after.py: 96 : main() ::	 Experiment Name: cv_svamp_augmented
2022-09-10 07:40:19,542 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:40:19,542 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:40:20,500 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:40:20,500 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 3911
2022-09-10 07:40:20,500 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 227
2022-09-10 07:40:20,500 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: ['0.01', '12.0', '1.0', '100.0', '0.1', '0.5', '3.0', '4.0', '7.0']
2022-09-10 07:40:20,500 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:40:20,500 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:40:21,078 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 4068 / 4068 = 1.0
2022-09-10 07:40:21,243 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 4071 words in input language, 21 words in output
2022-09-10 07:40:21,254 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:40:24,092 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:40:24,092 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:40:24,170 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:40:24,170 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/svamp/dense/models/cv_svamp_augmented_fold2.
2022-09-10 07:40:29,652 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:41:06,249 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.0	
 equation acc epoch: 0.0	
 max val acc: 0.0	
 equation acc: 0.0	
2022-09-10 07:41:06,249 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 36s
2022-09-10 07:41:06,252 | INFO | main_after.py: 96 : main() ::	 Experiment Name: cv_svamp_augmented
2022-09-10 07:41:06,252 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:41:06,252 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:41:07,216 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:41:07,216 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 3973
2022-09-10 07:41:07,216 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 165
2022-09-10 07:41:07,216 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: ['0.01', '12.0', '1.0', '100.0', '0.1', '0.5', '3.0', '4.0', '7.0']
2022-09-10 07:41:07,216 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:41:07,216 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:41:07,793 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 4068 / 4068 = 1.0
2022-09-10 07:41:07,962 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 4071 words in input language, 21 words in output
2022-09-10 07:41:07,972 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:41:10,862 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:41:10,862 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:41:10,940 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:41:10,940 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/svamp/dense/models/cv_svamp_augmented_fold3.
2022-09-10 07:41:13,742 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:41:17,605 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.006060606060606061	
 equation acc epoch: 0.0	
 max val acc: 0.006060606060606061	
 equation acc: 0.0	
2022-09-10 07:41:17,605 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 3s
2022-09-10 07:41:17,606 | INFO | main_after.py: 96 : main() ::	 Experiment Name: cv_svamp_augmented
2022-09-10 07:41:17,607 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:41:17,607 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:41:18,561 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:41:18,561 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 3954
2022-09-10 07:41:18,561 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 184
2022-09-10 07:41:18,561 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: ['0.01', '12.0', '1.0', '100.0', '0.1', '0.5', '3.0', '4.0', '7.0']
2022-09-10 07:41:18,561 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:41:18,561 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:41:19,138 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 4068 / 4068 = 1.0
2022-09-10 07:41:19,308 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 4071 words in input language, 21 words in output
2022-09-10 07:41:19,317 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:41:22,175 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:41:22,175 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:41:22,253 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:41:22,253 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/svamp/dense/models/cv_svamp_augmented_fold4.
2022-09-10 07:41:26,513 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:41:30,685 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.005434782608695652	
 equation acc epoch: 0.0	
 max val acc: 0.005434782608695652	
 equation acc: 0.0	
2022-09-10 07:41:30,685 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 4s
Traceback (most recent call last):
  File "/home/sliu/miniconda3/envs/prune_cry/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/sliu/miniconda3/envs/prune_cry/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/gpfs/home6/sliu/Projects/SVAMP/code/gts/src/main_after.py", line 276, in <module>
    main()
  File "/gpfs/home6/sliu/Projects/SVAMP/code/gts/src/main_after.py", line 266, in main
    folds_scores.append(float(best_acc[w][0])/best_acc[w][1])
ZeroDivisionError: float division by zero
Transfer numbers...
initialize by random pruning
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.8659522758546371
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.866782182230869
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 0.85546875
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8660753038194444
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8655276828342013
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8659837510850694
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8657006157769097
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.86615965101454
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.866042243109809
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8664008246527778
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8657040066189237
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8656480577256944
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8665635850694444
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.865899403889974
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8656912909613715
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8669891357421875
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8658820258246528
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8661448160807291
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8660871717664931
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.865897708468967
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8662952846950955
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8658362494574653
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8653310139973959
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8654530843098959
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8653717041015625
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8661753336588541
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8657815721299913
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8653123643663194
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8656989203559028
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8661058213975694
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.864837646484375
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8660210503472222
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8657942877875434
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8662261962890625
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.865509033203125
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8658515082465278
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.866485595703125
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.865960439046224
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.866156260172526
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8658938937717013
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8662397596571181
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8661753336588541
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8661617702907987
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8659477233886719
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8658582899305556
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8660125732421875
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8657972547743056
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8663991292317709
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8658243815104166
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8658409118652344
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8660909864637587
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8662194146050347
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8662990993923612
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8660702175564237
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8657548692491319
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8661270141601562
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8659845987955729
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8655531141493056
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8660176595052084
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8661515977647569
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8654649522569444
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8660613165961372
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8658722771538628
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8664364284939237
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8664109971788194
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8667483859592013
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8661583794487847
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8661888970269097
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8663703070746528
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8663194444444444
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8650868733723959
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8666653103298612
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8654005262586806
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8657942877875434
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8659324645996094
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.8659634060329862
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.8661079406738281
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.8658647537231445
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.8654988606770834
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.8657283782958984
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.8660993576049805
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.8662853240966797
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.8658757209777832
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.8661985397338867
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.8653144836425781
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.8663959503173828
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.8642158508300781
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.8657131195068359
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.870849609375
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.8671150207519531
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.84375
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.8659553527832031
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.875
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.8714192708333334
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.866100856236049
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.8663297380719865
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.865978785923549
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.8654153006417411
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.8661324637276786
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.8660038539341518
Final sparsity level of 0.866: 0.8659756318273327
Transfer numbers...
initialize by random pruning
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.8660293413657615
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.8669569755188067
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 0.8658854166666666
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8658430311414931
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8665313720703125
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8653886583116319
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8661855061848959
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8658370971679688
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8657565646701388
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8668314615885416
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8656209309895834
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8662228054470487
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8654751247829862
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8659799363878038
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8658871120876737
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8658786349826388
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8654073079427084
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8652971055772569
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8656395806206597
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8657913208007812
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8662041558159722
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8657413058810763
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8668619791666666
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8661939832899306
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8654412163628472
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8662423027886285
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8660235934787326
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8663160536024306
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8663160536024306
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8660736083984375
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8651987711588541
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8663092719184028
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.866280025906033
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8655175103081597
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8665127224392362
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8662804497612847
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8659566243489584
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8661850823296441
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8661693996853299
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8659464518229166
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8665568033854166
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8659905327690972
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8664482964409722
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8661367628309462
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8663995530870225
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8657260470920138
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8665093315972222
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8663703070746528
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.86541748046875
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8662914699978299
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8662524753146701
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8658888075086806
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8660396999782987
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8666839599609375
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8659430609809028
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8658862643771701
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8661109076605903
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8657599555121528
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8654836018880209
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8659837510850694
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8660719129774306
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8659477233886719
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8658133612738715
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8658888075086806
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8662312825520834
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8665076361762153
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8654446072048612
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8661045498318143
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.865942637125651
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8665432400173612
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8659413655598959
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8653920491536459
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8660922580295138
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8658362494574653
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8659998575846354
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.8653361002604166
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.8658955891927084
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.8660211563110352
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.8658790588378906
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.8657464981079102
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.8664164543151855
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.8659954071044922
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.8657245635986328
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.8659648895263672
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.8662910461425781
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.8656272888183594
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.8663330078125
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.8666667938232422
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.86279296875
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.8659477233886719
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.876953125
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.8661422729492188
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.859375
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.8714192708333334
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.86602783203125
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.865736825125558
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.8660997663225447
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.8665880475725447
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.8656441824776786
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.866168430873326
Final sparsity level of 0.866: 0.8660308740758069
Transfer numbers...
initialize by random pruning
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.8660071672220564
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.8666555204280155
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 0.8880208333333334
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8657870822482638
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.86639404296875
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8652835422092013
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8659040662977431
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8659112718370225
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.865807851155599
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8655768500434028
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8661922878689237
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8664652506510416
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8652004665798612
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8656756083170573
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8660795423719618
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8662058512369791
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8649953206380209
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8661838107638888
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8660719129774306
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8657103644476997
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8664338853624132
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8657396104600694
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8661465115017362
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8653327094184028
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8661159939236112
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8663063049316406
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8659108479817709
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8664703369140625
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8655412462022569
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8661719428168403
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8665381537543403
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8661384582519531
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8658824496799045
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8653072781032987
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8660159640842013
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8660600450303819
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8667653401692709
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8659261067708334
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8659269544813368
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8659091525607638
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8662448459201388
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8664262559678819
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8666551378038194
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8659659491644965
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8660575018988715
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8661024305555556
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8663126627604166
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8654361300998263
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8656005859375
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8656501770019531
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8658137851291232
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8665330674913194
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8653818766276041
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8663075764973959
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8661159939236112
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8658400641547309
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8660125732421875
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8659328884548612
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8661956787109375
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8658345540364584
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8651614718967013
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.866135491265191
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8658443027072482
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8651614718967013
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8665873209635416
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8664652506510416
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8666839599609375
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.86620118882921
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.865806155734592
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8661922878689237
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8661736382378472
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8663736979166666
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8657006157769097
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8660303751627604
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8659578959147135
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.865631103515625
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.8659966786702474
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.8661880493164062
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.8660831451416016
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.8657455444335938
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.8660974502563477
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.8663816452026367
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.8660387992858887
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.8663349151611328
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.8671951293945312
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.8653392791748047
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.8646392822265625
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.8651847839355469
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.857177734375
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.8663749694824219
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.861328125
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.8657658894856771
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.857421875
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.8626302083333334
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.8656267438616072
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.866382053920201
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.8661727905273438
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.866286141531808
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.865893772670201
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.8659776960100447
Final sparsity level of 0.866: 0.865989989273866
Transfer numbers...
initialize by random pruning
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.8660630170512948
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.8671292355706874
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 0.8736979166666666
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8657446967230903
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8664839002821181
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8668365478515625
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.86614990234375
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8662940131293403
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8660621643066406
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8657667371961806
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8662143283420138
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8655361599392362
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8661075168185763
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8662139044867622
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8658680386013455
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8658277723524306
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8661159939236112
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8657446967230903
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8656582302517362
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8660350375705295
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.865579817030165
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8660566541883681
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8662346733940972
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8660973442925347
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8660007052951388
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8661486307779948
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.865868886311849
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8668738471137153
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8669281005859375
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8658633761935763
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8657396104600694
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8659383985731337
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8661511739095052
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8659566243489584
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8660498725043403
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8656294080946181
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8661532931857638
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8660151163736979
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8659049140082465
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8653835720486112
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8663313123914931
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8659006754557291
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8659040662977431
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8661329481336806
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.866447024875217
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.865936279296875
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8665449354383681
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8660464816623263
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8661719428168403
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8658578660753038
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8656870524088541
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8661566840277778
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8655107286241319
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8663313123914931
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8657379150390625
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.865457746717665
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8657510545518663
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8662940131293403
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8657769097222222
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8657192654079862
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8657294379340278
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8663533528645834
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8660541110568576
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8657396104600694
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8655242919921875
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8657769097222222
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8651055230034722
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.865974850124783
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8659078809950087
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8659651014539931
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8662261962890625
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8671179877387153
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8658955891927084
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8659943474663628
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8660931057400174
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.8661651611328125
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.8659776051839193
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.8663549423217773
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.8654886881510416
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.8654632568359375
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.8664073944091797
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.8662118911743164
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.8663249015808105
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.8666849136352539
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.8662071228027344
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.8662986755371094
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.8659095764160156
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.8662357330322266
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.867431640625
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.8667850494384766
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.865234375
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.866217295328776
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.87109375
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.8763020833333334
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.8657128470284599
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.8658370971679688
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.8655733380998885
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.8655384608677456
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.8659133911132812
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.8659340994698661
Final sparsity level of 0.866: 0.8660266720651774
Transfer numbers...
initialize by random pruning
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.8660418790825293
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.8663160667963683
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 0.8854166666666666
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8658582899305556
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.86627197265625
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8648223876953125
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8661176893446181
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8660753038194444
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8659392462836372
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8666500515407987
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8658243815104166
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8661855061848959
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8659176296657987
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8659278021918403
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8658256530761719
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8657480875651041
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8664872911241319
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8658091227213541
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8657430013020834
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8661253187391493
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8655005560980903
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8655259874131944
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8664805094401041
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8669094509548612
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8651258680555556
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8661075168185763
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8658294677734375
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8656327989366319
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8657328287760416
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8649308946397569
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8664109971788194
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8660524156358507
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8657955593532987
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8661668565538194
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8659837510850694
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8661397298177084
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8663821750217013
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8660570780436198
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8658803304036459
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8656412760416666
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8661770290798612
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8665093315972222
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8668145073784722
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8661215040418837
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8659172058105469
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8652411566840278
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8660464816623263
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8666432698567709
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8659566243489584
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8659413655598959
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8657196892632378
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8658379448784722
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8663109673394097
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8654106987847222
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8656734890407987
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8661558363172743
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8660689459906684
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8664449055989584
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8663330078125
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8663906521267362
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8663143581814237
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8662664625379775
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8657264709472656
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8659837510850694
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8659786648220487
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8662974039713541
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8645155164930556
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8661918640136719
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8658714294433594
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8665754530164931
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8659193250868056
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8662770589192709
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8655954996744791
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8663787841796875
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8658523559570312
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.8661905924479166
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.8664156595865885
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.8663339614868164
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.8661346435546875
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.8659486770629883
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.8658599853515625
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.865483283996582
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.8663110733032227
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.8661279678344727
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.8659172058105469
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.866729736328125
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.8661537170410156
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.8658428192138672
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.864013671875
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.8668651580810547
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.859375
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.8665250142415365
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.861328125
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.8684895833333334
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.8659885951450893
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.8656659807477678
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.8661368233816964
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.8660855974469865
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.8661815098353794
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.8660343715122768
Final sparsity level of 0.866: 0.8660173819527146
wandb: WARNING W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.
2022-09-10 07:41:33,848 | INFO | main_after.py: 96 : main() ::	 Experiment Name: cv_svamp_augmented
2022-09-10 07:41:33,848 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:41:33,848 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:41:34,889 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:41:34,889 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 3932
2022-09-10 07:41:34,889 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 206
2022-09-10 07:41:34,889 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: ['0.01', '12.0', '1.0', '100.0', '0.1', '0.5', '3.0', '4.0', '7.0']
2022-09-10 07:41:34,889 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:41:34,889 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:41:35,467 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 4068 / 4068 = 1.0
2022-09-10 07:41:35,632 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 4071 words in input language, 21 words in output
2022-09-10 07:41:35,640 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:41:38,841 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:41:38,841 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:41:42,167 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:41:42,167 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/svamp/dense/models/cv_svamp_augmented_fold0.
2022-09-10 07:41:47,636 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:41:52,335 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.014563106796116505	
 equation acc epoch: 0.0	
 max val acc: 0.014563106796116505	
 equation acc: 0.0	
2022-09-10 07:41:52,335 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 4s
2022-09-10 07:41:52,337 | INFO | main_after.py: 96 : main() ::	 Experiment Name: cv_svamp_augmented
2022-09-10 07:41:52,337 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:41:52,337 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:41:53,296 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:41:53,296 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 3920
2022-09-10 07:41:53,296 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 218
2022-09-10 07:41:53,296 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: ['0.01', '12.0', '1.0', '100.0', '0.1', '0.5', '3.0', '4.0', '7.0']
2022-09-10 07:41:53,296 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:41:53,296 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:41:53,865 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 4068 / 4068 = 1.0
2022-09-10 07:41:54,027 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 4071 words in input language, 21 words in output
2022-09-10 07:41:54,037 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:41:57,033 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:41:57,033 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:41:57,115 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:41:57,115 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/svamp/dense/models/cv_svamp_augmented_fold1.
2022-09-10 07:42:01,998 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:42:05,905 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.0	
 equation acc epoch: 0.0	
 max val acc: 0.0	
 equation acc: 0.0	
2022-09-10 07:42:05,905 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 3s
2022-09-10 07:42:05,908 | INFO | main_after.py: 96 : main() ::	 Experiment Name: cv_svamp_augmented
2022-09-10 07:42:05,908 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:42:05,908 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:42:06,871 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:42:06,871 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 3911
2022-09-10 07:42:06,871 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 227
2022-09-10 07:42:06,871 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: ['0.01', '12.0', '1.0', '100.0', '0.1', '0.5', '3.0', '4.0', '7.0']
2022-09-10 07:42:06,871 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:42:06,871 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:42:07,450 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 4068 / 4068 = 1.0
2022-09-10 07:42:07,614 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 4071 words in input language, 21 words in output
2022-09-10 07:42:07,626 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:42:10,447 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:42:10,448 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:42:10,527 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:42:10,527 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/svamp/dense/models/cv_svamp_augmented_fold2.
2022-09-10 07:42:15,118 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:42:23,052 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.0	
 equation acc epoch: 0.0	
 max val acc: 0.0	
 equation acc: 0.0	
2022-09-10 07:42:23,052 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 7s
2022-09-10 07:42:23,054 | INFO | main_after.py: 96 : main() ::	 Experiment Name: cv_svamp_augmented
2022-09-10 07:42:23,054 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:42:23,054 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:42:24,116 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:42:24,117 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 3973
2022-09-10 07:42:24,117 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 165
2022-09-10 07:42:24,117 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: ['0.01', '12.0', '1.0', '100.0', '0.1', '0.5', '3.0', '4.0', '7.0']
2022-09-10 07:42:24,117 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:42:24,117 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:42:24,694 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 4068 / 4068 = 1.0
2022-09-10 07:42:24,865 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 4071 words in input language, 21 words in output
2022-09-10 07:42:24,874 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:42:27,715 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:42:27,716 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:42:27,795 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:42:27,795 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/svamp/dense/models/cv_svamp_augmented_fold3.
2022-09-10 07:42:31,585 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:42:35,170 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.006060606060606061	
 equation acc epoch: 0.0	
 max val acc: 0.006060606060606061	
 equation acc: 0.0	
2022-09-10 07:42:35,170 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 3s
2022-09-10 07:42:35,173 | INFO | main_after.py: 96 : main() ::	 Experiment Name: cv_svamp_augmented
2022-09-10 07:42:35,173 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:42:35,173 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:42:36,241 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:42:36,241 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 3954
2022-09-10 07:42:36,241 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 184
2022-09-10 07:42:36,241 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: ['0.01', '12.0', '1.0', '100.0', '0.1', '0.5', '3.0', '4.0', '7.0']
2022-09-10 07:42:36,241 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:42:36,241 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:42:36,820 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 4068 / 4068 = 1.0
2022-09-10 07:42:36,987 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 4071 words in input language, 21 words in output
2022-09-10 07:42:36,997 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:42:39,950 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:42:39,950 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:42:40,029 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:42:40,029 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/svamp/dense/models/cv_svamp_augmented_fold4.
2022-09-10 07:42:42,070 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:42:44,799 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.0	
 equation acc epoch: 0.0	
 max val acc: 0.0	
 equation acc: 0.0	
2022-09-10 07:42:44,799 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 2s
Traceback (most recent call last):
  File "/home/sliu/miniconda3/envs/prune_cry/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/sliu/miniconda3/envs/prune_cry/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/gpfs/home6/sliu/Projects/SVAMP/code/gts/src/main_after.py", line 276, in <module>
    main()
  File "/gpfs/home6/sliu/Projects/SVAMP/code/gts/src/main_after.py", line 266, in main
    folds_scores.append(float(best_acc[w][0])/best_acc[w][1])
ZeroDivisionError: float division by zero
Transfer numbers...
initialize by random pruning
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.8929564454225936
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.8935964858949417
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 0.8802083333333334
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8929019504123263
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8923356797960069
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8929562038845487
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8927815755208334
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8929833306206597
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8931092156304253
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8937581380208334
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8927341037326388
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8926628960503472
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8937852647569444
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8927510579427084
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8928070068359375
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8936123318142362
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8927781846788194
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8928714328342013
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8930155436197916
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8929015265570747
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8932914733886719
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8929629855685763
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8924662272135416
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8927001953125
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8928900824652778
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8930007086859809
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8927023145887587
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8924492730034722
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8927052815755209
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8931749131944444
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8924475775824653
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8929464552137587
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8928489685058594
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8930952284071181
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8925442165798612
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8928680419921875
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8932969835069444
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8929244147406684
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8930269877115885
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.892852783203125
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8928680419921875
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8934919569227431
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8929951985677084
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8930032518174913
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8924547831217448
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.893341064453125
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8927544487847222
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.893463134765625
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8928985595703125
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.892968495686849
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8929557800292969
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8934868706597222
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8933936225043403
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8931121826171875
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8928782145182291
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8932312859429253
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8928722805447049
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8927408854166666
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8928951687282987
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8932579888237847
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8926357693142362
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8931414286295573
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8928574456108941
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8935106065538194
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8932240804036459
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8936225043402778
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8930206298828125
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.893119388156467
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.893394046359592
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8931647406684028
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8922678629557291
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8939005533854166
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8925289577907987
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8929392496744791
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8929913838704427
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.8928629557291666
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.8930104573567709
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.8929443359375
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.8926684061686198
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.8924903869628906
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.8931970596313477
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.8930873870849609
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.8930530548095703
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.8931903839111328
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.8927001953125
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.8932075500488281
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.8918495178222656
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.8929939270019531
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.898681640625
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.8940448760986328
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.873046875
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.8929189046223959
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.89453125
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.9039713541666666
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.892852783203125
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.8933606828962054
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.8931917463030133
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.8922762189592633
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.8932015555245536
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.8931808471679688
Final sparsity level of 0.893: 0.8929773021646322
Transfer numbers...
initialize by random pruning
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.8930481728091118
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.8937611462386511
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 0.8854166666666666
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8927832709418403
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8935207790798612
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8923984103732638
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.893096923828125
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8927959865993924
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8927476671006944
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8936089409722222
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8923102484809028
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8929375542534722
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8924170600043403
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.893043941921658
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8929833306206597
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8927646213107638
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8925730387369791
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8924662272135416
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8926425509982638
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8930193583170573
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8932410346137153
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8925238715277778
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8933936225043403
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8928544786241319
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8924984402126737
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8931927151150174
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8932083977593316
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8933834499782987
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8932952880859375
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8930409749348959
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8922119140625
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8933656480577257
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8929968939887153
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8925594753689237
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8933071560329862
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8931766086154513
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8927290174696181
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8931736416286893
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8930180867513021
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.892669677734375
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8934037950303819
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8932291666666666
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8933868408203125
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8930138481987847
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8932855394151475
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8927866617838541
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8932834201388888
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8933427598741319
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.89239501953125
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8932957119411893
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8932855394151475
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8929189046223959
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.89324951171875
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8935394287109375
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8928561740451388
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8927964104546441
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8931588066948785
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8929273817274306
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8924543592664931
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8933478461371528
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8930816650390625
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8930549621582031
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8929625617133247
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8923712836371528
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8928358289930556
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8936886257595487
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8925357394748263
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8931833902994791
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8930778503417969
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8933241102430556
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8929562038845487
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8927069769965278
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8929799397786459
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8928820292154948
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8929536607530382
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.8928087022569444
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.8929347991943359
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.8933448791503906
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.8928991953531901
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.8930034637451172
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.8935775756835938
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.8933048248291016
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.8927555084228516
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.8928718566894531
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.8935661315917969
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.8927555084228516
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.8933601379394531
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.8934192657470703
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.892822265625
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.8931674957275391
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.900390625
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.8930091857910156
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.884765625
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.9036458333333334
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.8930260794503349
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.8929007393973214
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.8929072788783482
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.8933061872209821
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.8925105503627232
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.8933432442801339
Final sparsity level of 0.893: 0.8930392870133587
Transfer numbers...
initialize by random pruning
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.8929927892585962
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.893317829928664
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 0.9153645833333334
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8927561442057291
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8932834201388888
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8925069173177084
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.89324951171875
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.892982906765408
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8928095499674479
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8926154242621528
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8932342529296875
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8933648003472222
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.892242431640625
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8925798204210069
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8929562038845487
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8934393988715278
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8924848768446181
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8931732177734375
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8931206597222222
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8926946851942275
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8932732476128472
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8926917182074653
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8932139078776041
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8922526041666666
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8930697970920138
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.893257564968533
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8927976820203993
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8934410942925347
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8929341634114584
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8930935329861112
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8934885660807291
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8931460910373263
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8929189046223959
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8922492133246528
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8928087022569444
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8931766086154513
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8936987982855903
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8930604722764757
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8928214179144965
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8925357394748263
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8930630154079862
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8933139377170138
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.893463134765625
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8929269578721788
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8929718865288628
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8929036458333334
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8935614691840278
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8925238715277778
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8927273220486112
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8926277160644531
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8929460313585069
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8933749728732638
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.892425537109375
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8931223551432291
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8930409749348959
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8929867214626737
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8930592007107205
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8928409152560763
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8934699164496528
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8926018608940972
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8922983805338541
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8930795457628038
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8929740058051215
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8924899631076388
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8935055202907987
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8931155734592013
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8936428493923612
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8931914435492622
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8929222954644097
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8926578097873263
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8930308024088541
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8933529324001737
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8925153944227431
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8930452134874132
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8930625915527344
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.8930308024088541
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.8929341634114584
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.8932390213012695
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.8931026458740234
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.8927783966064453
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.8932404518127441
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.8934268951416016
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.8929166793823242
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.8929672241210938
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.8932914733886719
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.8921890258789062
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.8916587829589844
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.8921604156494141
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.882568359375
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.8932018280029297
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.888671875
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.8922615051269531
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.892578125
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.8929036458333334
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.8927078247070312
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.8935067313058036
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.8932843889508929
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.8931012834821429
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.8930162702287946
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.8930773053850446
Final sparsity level of 0.893: 0.8929795381604038
Transfer numbers...
initialize by random pruning
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.8930826774345967
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.893905540693904
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 0.9049479166666666
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8924509684244791
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8933546278211806
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8936496310763888
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8931291368272569
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8932452731662326
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8931405809190538
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8928561740451388
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8932206895616319
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8928036159939237
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8931308322482638
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8932444254557291
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8929977416992188
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8930935329861112
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8931376139322916
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8926544189453125
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8928460015190972
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.892937978108724
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8927845425075955
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8930341932508681
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8933071560329862
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.89312744140625
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8927239312065972
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8932851155598959
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8929316202799479
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8939429389105903
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8937208387586806
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8926442464192709
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.89300537109375
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8929392496744791
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8931897481282552
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8929867214626737
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8934207492404513
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8927459716796875
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8931257459852431
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8929778205023872
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8927760654025607
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8922915988498263
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8936258951822916
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8928646511501737
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8929968939887153
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8931367662217882
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8932715521918403
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8931664360894097
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8938751220703125
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8928544786241319
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8933308919270834
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8929023742675781
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8926798502604166
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8928443060980903
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.89263916015625
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8933156331380209
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8927510579427084
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8924759758843316
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8924920823838975
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8928341335720487
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8926900227864584
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8926917182074653
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8926222059461806
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8932902018229166
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8930028279622396
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8928663465711806
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8927357991536459
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8929528130425347
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8920627170138888
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8929278055826823
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.893104977077908
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8928120930989584
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8933817545572916
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8938530815972222
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8929612901475694
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8927141825358073
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8931299845377604
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.8933478461371528
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.8931636810302734
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.8933467864990234
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.8927078247070312
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.8925857543945312
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.8934135437011719
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.893488883972168
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.8934612274169922
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.893651008605957
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.8930549621582031
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.8931674957275391
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.8927764892578125
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.8934402465820312
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.89111328125
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.8939914703369141
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.88671875
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.8929773966471354
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.8984375
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.9046223958333334
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.8929214477539062
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.8932102748325893
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.8926871163504464
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.8924974714006696
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.8929933820452008
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.8928179059709821
Final sparsity level of 0.893: 0.893042907526636
Transfer numbers...
initialize by random pruning
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.8930304801220199
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.8932266334306096
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 0.90625
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8926645914713541
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8930952284071181
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8921118842230903
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8931630452473959
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8929680718315972
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8930180867513021
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8936258951822916
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8929460313585069
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8933376736111112
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8933258056640625
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8928947448730469
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.892730712890625
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8928544786241319
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8934037950303819
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8929222954644097
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8927120632595487
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8931308322482638
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8925548129611545
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.89263916015625
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8931070963541666
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8934851752387153
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8926561143663194
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8929833306206597
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8928570217556424
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.892608642578125
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8926255967881944
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8923577202690972
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8933207194010416
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8930384318033854
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.892997317843967
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8930257161458334
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8926425509982638
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8930036756727431
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8935648600260416
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8931838141547309
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.892982906765408
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8928866916232638
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8931427001953125
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8930884467230903
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8933495415581597
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8932185702853732
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8929019504123263
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8922695583767362
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8928985595703125
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8937004937065972
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8929511176215278
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8928693135579427
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8929036458333334
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8931223551432291
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8928443060980903
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8923170301649306
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8927290174696181
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8931087917751737
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8931617736816406
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8932749430338541
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8934868706597222
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8935614691840278
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8929935031467013
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8932995266384549
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8928650750054253
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8926476372612847
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8932529025607638
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8931918674045138
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8918253580729166
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8930515713161893
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8926891750759549
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8930036756727431
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8928070068359375
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8933207194010416
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.892822265625
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8934762742784288
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8929044935438368
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.8930714925130209
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.8934644063313802
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.8930377960205078
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.8931655883789062
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.8930587768554688
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.8927512168884277
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.8924484252929688
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.893157958984375
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.8930015563964844
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.892669677734375
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.8936519622802734
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.893310546875
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.8931732177734375
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.892822265625
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.8938446044921875
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.890625
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.8934860229492188
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.892578125
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.8909505208333334
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.8932876586914062
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.8928353445870536
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.8931862967354911
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.8932931082589286
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.8934173583984375
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.8931394304547992
Final sparsity level of 0.893: 0.8930185677088871

JOB STATISTICS
==============
Job ID: 1507128
Cluster: snellius
User/Group: sliu/sliu
State: RUNNING
Nodes: 1
Cores per node: 18
CPU Utilized: 00:00:00
CPU Efficiency: 0.00% of 06:30:00 core-walltime
Job Wall-clock time: 00:21:40
Memory Utilized: 0.00 MB (estimated maximum)
Memory Efficiency: 0.00% of 0.00 MB (0.00 MB/node)
WARNING: Efficiency statistics may be misleading for RUNNING jobs.
