wandb: WARNING W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.
2022-09-10 07:20:15,576 | INFO | main_after.py: 96 : main() ::	 Experiment Name: run_cv_mawps
2022-09-10 07:20:15,576 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:20:15,576 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:20:16,089 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:20:16,089 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 1537
2022-09-10 07:20:16,089 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 384
2022-09-10 07:20:16,089 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: ['0.5', '3.0', '100.0', '12.0', '0.01', '0.1', '1.0', '4.0', '7.0']
2022-09-10 07:20:16,089 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:20:16,089 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:20:16,283 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 2449 / 2449 = 1.0
2022-09-10 07:20:16,326 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 2452 words in input language, 21 words in output
2022-09-10 07:20:16,336 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:20:19,462 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:20:19,462 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:20:23,170 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:20:23,171 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/mawps/dense/models/run_cv_mawps_fold0.
2022-09-10 07:20:30,004 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:20:45,402 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.8802083333333334	
 equation acc epoch: 0.8697916666666666	
 max val acc: 0.8802083333333334	
 equation acc: 0.8697916666666666	
2022-09-10 07:20:45,402 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 15s
2022-09-10 07:20:45,404 | INFO | main_after.py: 96 : main() ::	 Experiment Name: run_cv_mawps
2022-09-10 07:20:45,405 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:20:45,405 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:20:45,833 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:20:45,833 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 1537
2022-09-10 07:20:45,833 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 384
2022-09-10 07:20:45,833 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: ['0.01', '12.0', '1.0', '100.0', '0.1', '0.5', '3.0', '4.0', '7.0']
2022-09-10 07:20:45,833 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:20:45,833 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:20:46,025 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 2449 / 2449 = 1.0
2022-09-10 07:20:46,067 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 2452 words in input language, 21 words in output
2022-09-10 07:20:46,078 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:20:49,016 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:20:49,016 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:20:49,092 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:20:49,093 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/mawps/dense/models/run_cv_mawps_fold1.
2022-09-10 07:20:56,153 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:21:10,261 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.875	
 equation acc epoch: 0.8619791666666666	
 max val acc: 0.875	
 equation acc: 0.8619791666666666	
2022-09-10 07:21:10,262 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 14s
2022-09-10 07:21:10,264 | INFO | main_after.py: 96 : main() ::	 Experiment Name: run_cv_mawps
2022-09-10 07:21:10,264 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:21:10,264 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:21:10,695 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:21:10,695 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 1537
2022-09-10 07:21:10,695 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 384
2022-09-10 07:21:10,695 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: ['0.01', '12.0', '1.0', '100.0', '0.1', '0.5', '3.0', '4.0', '7.0']
2022-09-10 07:21:10,695 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:21:10,695 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:21:10,891 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 2449 / 2449 = 1.0
2022-09-10 07:21:10,933 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 2452 words in input language, 21 words in output
2022-09-10 07:21:10,944 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:21:13,812 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:21:13,812 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:21:13,888 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:21:13,888 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/mawps/dense/models/run_cv_mawps_fold2.
2022-09-10 07:21:20,875 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:21:34,214 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.8645833333333334	
 equation acc epoch: 0.8567708333333334	
 max val acc: 0.8645833333333334	
 equation acc: 0.8567708333333334	
2022-09-10 07:21:34,214 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 13s
2022-09-10 07:21:34,216 | INFO | main_after.py: 96 : main() ::	 Experiment Name: run_cv_mawps
2022-09-10 07:21:34,216 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:21:34,216 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:21:34,644 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:21:34,644 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 1537
2022-09-10 07:21:34,644 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 384
2022-09-10 07:21:34,644 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: ['0.01', '12.0', '1.0', '100.0', '0.1', '0.5', '3.0', '4.0', '7.0']
2022-09-10 07:21:34,644 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:21:34,644 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:21:34,840 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 2449 / 2449 = 1.0
2022-09-10 07:21:34,882 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 2452 words in input language, 21 words in output
2022-09-10 07:21:34,893 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:21:37,757 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:21:37,757 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:21:37,835 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:21:37,835 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/mawps/dense/models/run_cv_mawps_fold3.
2022-09-10 07:21:44,627 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:21:59,349 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.8671875	
 equation acc epoch: 0.859375	
 max val acc: 0.8671875	
 equation acc: 0.859375	
2022-09-10 07:21:59,350 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 14s
2022-09-10 07:21:59,353 | INFO | main_after.py: 96 : main() ::	 Experiment Name: run_cv_mawps
2022-09-10 07:21:59,353 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:21:59,353 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:21:59,783 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:21:59,783 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 1537
2022-09-10 07:21:59,783 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 384
2022-09-10 07:21:59,783 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: ['0.01', '12.0', '1.0', '100.0', '0.1', '0.5', '3.0', '4.0', '7.0']
2022-09-10 07:21:59,783 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:21:59,784 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:21:59,979 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 2449 / 2449 = 1.0
2022-09-10 07:22:00,023 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 2452 words in input language, 21 words in output
2022-09-10 07:22:00,034 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:22:02,877 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:22:02,877 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:22:02,953 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:22:02,953 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/mawps/dense/models/run_cv_mawps_fold4.
2022-09-10 07:22:09,677 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:22:24,142 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.8671875	
 equation acc epoch: 0.8645833333333334	
 max val acc: 0.8671875	
 equation acc: 0.8645833333333334	
2022-09-10 07:22:24,143 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 14s
2022-09-10 07:22:24,143 | INFO | main_after.py: 272 : main() ::	 Final Val score: 0.8708333333333333
Transfer numbers...
initialize by one_shot_gm
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.2003513928263536
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.4301612151426718
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 1.0
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.08282301161024308
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.08543904622395837
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.23011949327256942
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.24969143337673616
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.1599468655056424
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.21307415432400179
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.1116943359375
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.11730278862847221
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.20759243435329866
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.26162211100260413
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.14894188774956596
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.20333989461263025
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.12047153049045134
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.1257697211371528
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.19086201985677087
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.2479790581597222
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.14594311184353304
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.20024151272243929
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.1141510009765625
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.12163628472222221
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.18116929796006942
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.23679606119791663
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.14984851413302946
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.20703252156575525
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.11164686414930558
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.11892191569010413
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.1862860785590278
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.24738905164930558
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.15961964925130212
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.22280587090386283
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.12048000759548616
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.12725660536024308
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.17553202311197913
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.22913953993055558
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.16656324598524308
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.23554950290256071
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.11944919162326384
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.12535603841145837
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.17500813802083337
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.2185516357421875
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.15855450100368929
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.22852155897352433
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.11131625705295134
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.11787584092881942
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.2148810492621528
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.2642584906684028
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.15608257717556429
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.21887461344401038
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.11605495876736116
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.12172105577256942
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.19458177354600692
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.24141438802083337
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.15769237942165804
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.21124521891276038
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.11874220106336808
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.12299940321180558
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.20524936252170134
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.24101765950520837
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.16960186428493929
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.20406807793511283
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.11615159776475692
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.12177530924479163
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.2003106011284722
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.231292724609375
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.18086285061306429
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.21017116970486116
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.11238945855034721
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.11617872450086808
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.17812940809461808
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.211883544921875
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.17267227172851562
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.246911366780599
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.3681776258680556
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.2287731170654297
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.4083290100097656
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.2104167938232422
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.349090576171875
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.2491145133972168
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.203399658203125
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.25389671325683594
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.2132854461669922
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.165618896484375
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.17160415649414062
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.151275634765625
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.1567840576171875
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.190185546875
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.18886756896972656
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.314453125
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.18212381998697913
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.291015625
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.01302083333333337
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.3302001953125
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.3957781110491071
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.26375361851283485
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.2817578996930804
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.4481222970145089
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.34622301374162945
Final sparsity level of 0.2: 0.20001956877042437
Transfer numbers...
initialize by one_shot_gm
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.49924548331343876
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.7827420760376135
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 1.0
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.02835761176215279
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.02954271104600692
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.08122931586371529
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.08781772189670134
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.055298275417751785
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.07457775539822054
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.03853183322482634
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.04043748643663192
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.07257927788628471
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.09214952256944442
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.05140092637803817
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.07140435112847221
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.04120212131076384
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.04293484157986116
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.06625705295138884
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.08715989854600692
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.050150129530164955
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.07034683227539062
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.03904893663194442
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.04199049207899308
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.06309000651041663
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.08347405327690971
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.051512400309244755
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.07286834716796875
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.03835211859809029
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.04105970594618058
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.06472439236111116
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.08793470594618058
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.05495961507161462
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.07855733235677087
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.04133945041232634
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.04360283745659721
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.06162177191840279
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.08052571614583337
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.057563357883029465
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.08367114596896696
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.04116312662760413
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.04306199815538192
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.06205240885416663
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.07590060763888884
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.05471038818359375
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.08057573106553817
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.03830464680989587
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.04032389322916663
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.07654825846354163
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.09369235568576384
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.053604549831814285
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.07707765367296004
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.03993055555555558
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.041839599609375
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.06780497233072913
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.08407762315538192
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.054146236843533035
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.07424121432834196
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.04116990831163192
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.04212273491753471
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.07137552897135413
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.08370463053385413
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.05843946668836808
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.07096057467990446
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.03952195909288192
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.04203287760416663
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.06990220811631942
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.07961527506510413
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.06263309054904509
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.07278781467013884
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.03868950737847221
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.03984578450520837
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.06277635362413192
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.07361687554253471
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.059764014350043415
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.08592054578993058
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.12904018825954866
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.226996103922526
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.44977378845214844
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.2157745361328125
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.42514991760253906
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.19256067276000977
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.10590076446533203
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.19150590896606445
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.10758590698242188
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.04599761962890625
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.05389976501464844
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.04290771484375
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.0517730712890625
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.035400390625
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.13507080078125
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.314453125
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.059185028076171875
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.076171875
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.0546875
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.16925593784877235
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.1920645577566964
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.15207018171037945
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.166534423828125
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.22680555071149555
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.2154410226004464
Final sparsity level of 0.2: 0.20000001246065757
Transfer numbers...
initialize by one_shot_gm
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.39916411767631554
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.7219342777237354
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 1.0
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.05095757378472221
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.05254109700520837
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.14355638292100692
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.15564982096354163
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.0987570020887587
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.1327735053168403
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.06880527072482634
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.07221476236979163
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.12908766004774308
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.16279941134982634
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.09202957153320312
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.12706714206271696
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.07376946343315971
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.07717556423611116
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.11828443739149308
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.15482584635416663
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.08983612060546875
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.12482961018880212
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.07010057237413192
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.07518513997395837
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.11208936903211808
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.14759657118055558
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.09231948852539062
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.12935638427734375
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.06872219509548616
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.07329135470920134
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.11590576171875
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.1553870307074653
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.09830390082465279
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.13934283786349821
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.07405938042534721
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.07825046115451384
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.10937669542100692
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.14275783962673616
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.1025979783799913
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.14797676934136283
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.07381863064236116
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.076812744140625
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.10947672526041663
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.13560655381944442
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.09782875908745658
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.14298884073893225
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.0684814453125
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.07237752278645837
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.13502841525607634
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.16563585069444442
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.09596930609809029
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.13677469889322913
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.07116529676649308
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.07470533582899308
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.1208953857421875
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.1501312255859375
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.09718153211805558
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.13169394599066842
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.073211669921875
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.07518853081597221
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.1271582709418403
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.1494377983940972
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.10437181260850692
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.12655893961588538
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.0711212158203125
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.07477823893229163
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.12468126085069442
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.1427154541015625
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.11168501112196183
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.12996080186631942
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.06911892361111116
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.07148912217881942
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.11056179470486116
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.1312476264105903
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.10675303141276038
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.15331013997395837
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.23051961263020837
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.15946896870930993
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.3023195266723633
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.16069094340006507
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.30692100524902344
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.1757655143737793
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.1347026824951172
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.16804981231689453
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.12989139556884766
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.07530593872070312
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.07718658447265625
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.06904220581054688
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.07521820068359375
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.071044921875
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.14088058471679688
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.265625
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.08037948608398438
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.1328125
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.03515625
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.19785308837890625
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.2279815673828125
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.1567404610770089
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.16884177071707585
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.2479117257254464
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.2100655691964286
Final sparsity level of 0.2: 0.20000003322842008
Transfer numbers...
initialize by one_shot_gm
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.23848286373885075
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.49968587872892345
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 1.0
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.07733493381076384
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.07983737521701384
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.21562025282118058
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.233673095703125
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.14942974514431429
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.19955359564887154
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.10417005750868058
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.10960557725694442
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.19396803114149308
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.24507310655381942
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.1392322116427951
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.19037373860677087
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.11236911349826384
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.11741468641493058
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.17857869466145837
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.23243374294704866
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.13628175523546004
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.18748558892144096
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.10652330186631942
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.11367458767361116
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.16920301649305558
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.22209676106770837
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.1400078667534722
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.19414138793945312
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.10443962944878471
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.11102464463975692
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.17441813151041663
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.23202853732638884
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.14899868435329866
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.20884746975368929
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.11239284939236116
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.11870998806423616
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.16437106662326384
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.2146759033203125
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.15569135877821183
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.221045176188151
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.11181810167100692
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.11707390679253471
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.16404385036892366
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.20475429958767366
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.14818276299370658
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.21422788831922746
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.10397677951388884
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.10988023546006942
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.20137532552083337
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.24779764811197913
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.1458324856228299
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.2051828172471788
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.10827297634548616
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.11363389756944442
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.18212890625
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.22602674696180558
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.14731640285915804
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.19775644938151038
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.11081949869791663
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.11473253038194442
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.19200473361545134
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.22545878092447913
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.1584540473090278
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.19121763441297746
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.10858154296875
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.11369154188368058
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.1876610649956597
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.21633402506510413
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.16908857557508683
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.1966747707790799
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.10503133138020837
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.10845608181423616
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.1665259467230903
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.19814554850260413
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.16141128540039062
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.23120837741427946
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.3452368842230903
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.21732266743977868
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.4044313430786133
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.2007033030192057
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.3471794128417969
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.23090219497680664
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.17720413208007812
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.22745895385742188
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.17893028259277344
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.1328277587890625
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.14202308654785156
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.12331771850585938
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.13162803649902344
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.1533203125
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.15814781188964844
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.2734375
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.14170583089192712
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.236328125
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.01334635416666663
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.31092943464006695
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.35555049351283485
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.2229200090680804
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.2483106340680804
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.3951677594866071
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.3046722412109375
Final sparsity level of 0.2: 0.20000001246065757
Transfer numbers...
initialize by one_shot_gm
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.41206936569514907
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.7181065580415045
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 1.0
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.04723782009548616
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.04856703016493058
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.13305155436197913
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.14416842990451384
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.0912954542371962
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.12276670667860246
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.06383090549045134
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.06686062282986116
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.11975606282552087
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.1508551703559028
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.08513938056098092
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.117516835530599
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.06847974989149308
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.07116360134548616
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.10955641004774308
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.14312744140625
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.08296373155381942
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.11557430691189241
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.06487698025173616
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.069610595703125
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.10372585720486116
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.136627197265625
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.08540386623806429
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.11974716186523438
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.0636138916015625
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.06776089138454866
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.10720316569010413
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.1442955864800347
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.09100214640299475
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.129180908203125
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.06850009494357634
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.07232666015625
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.10098097059461808
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.1323174370659722
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.09485922919379342
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.13705190022786462
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.06827460394965279
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.07095506456163192
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.10146077473958337
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.1254035101996528
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.09043078952365446
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.13244925604926217
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.06344265407986116
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.06686231825086808
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.1250695122612847
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.15344577365451384
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.0887362162272135
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.12665769788953996
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.065765380859375
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.06909688313802087
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.1120758056640625
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.1386328803168403
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.08973778618706596
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.121917724609375
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.06737263997395837
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.06968519422743058
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.11777072482638884
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.1383141411675347
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.09651438395182288
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.11695098876953125
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.06550259060329866
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.06902567545572913
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.1154327392578125
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.13188680013020837
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.10333548651801217
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.12003707885742188
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.06386142306857634
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.06591966417100692
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.10256449381510413
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.12129889594184029
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.09883075290256071
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.14188130696614587
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.213470458984375
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.20509020487467444
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.39739322662353516
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.17971293131510413
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.3347654342651367
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.1872267723083496
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.1273670196533203
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.18205833435058594
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.12950515747070312
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.0699005126953125
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.07447624206542969
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.06283187866210938
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.06999015808105469
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.069580078125
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.12150764465332031
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.248046875
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.07609939575195312
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.12109375
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.03548177083333337
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.19501495361328125
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.2170802525111607
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.14674050467354915
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.16721997942243305
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.239837646484375
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.21340179443359375
Final sparsity level of 0.2: 0.20000001938324508
wandb: WARNING W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.
2022-09-10 07:22:27,265 | INFO | main_after.py: 96 : main() ::	 Experiment Name: run_cv_mawps
2022-09-10 07:22:27,265 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:22:27,265 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:22:27,770 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:22:27,771 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 1537
2022-09-10 07:22:27,771 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 384
2022-09-10 07:22:27,771 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: ['0.5', '3.0', '100.0', '12.0', '0.01', '0.1', '1.0', '4.0', '7.0']
2022-09-10 07:22:27,771 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:22:27,771 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:22:27,965 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 2449 / 2449 = 1.0
2022-09-10 07:22:28,010 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 2452 words in input language, 21 words in output
2022-09-10 07:22:28,020 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:22:31,163 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:22:31,164 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:22:34,567 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:22:34,567 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/mawps/dense/models/run_cv_mawps_fold0.
2022-09-10 07:22:41,350 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:23:16,287 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.203125	
 equation acc epoch: 0.203125	
 max val acc: 0.203125	
 equation acc: 0.203125	
2022-09-10 07:23:16,288 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 34s
2022-09-10 07:23:16,290 | INFO | main_after.py: 96 : main() ::	 Experiment Name: run_cv_mawps
2022-09-10 07:23:16,291 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:23:16,291 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:23:16,811 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:23:16,811 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 1537
2022-09-10 07:23:16,811 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 384
2022-09-10 07:23:16,811 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: ['0.01', '12.0', '1.0', '100.0', '0.1', '0.5', '3.0', '4.0', '7.0']
2022-09-10 07:23:16,811 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:23:16,811 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:23:17,003 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 2449 / 2449 = 1.0
2022-09-10 07:23:17,045 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 2452 words in input language, 21 words in output
2022-09-10 07:23:17,056 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:23:19,963 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:23:19,963 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:23:20,040 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:23:20,040 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/mawps/dense/models/run_cv_mawps_fold1.
2022-09-10 07:23:23,769 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:23:37,887 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.8125	
 equation acc epoch: 0.8046875	
 max val acc: 0.8125	
 equation acc: 0.8046875	
2022-09-10 07:23:37,887 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 14s
2022-09-10 07:23:37,889 | INFO | main_after.py: 96 : main() ::	 Experiment Name: run_cv_mawps
2022-09-10 07:23:37,889 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:23:37,889 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:23:38,320 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:23:38,320 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 1537
2022-09-10 07:23:38,320 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 384
2022-09-10 07:23:38,320 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: ['0.01', '12.0', '1.0', '100.0', '0.1', '0.5', '3.0', '4.0', '7.0']
2022-09-10 07:23:38,320 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:23:38,320 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:23:38,514 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 2449 / 2449 = 1.0
2022-09-10 07:23:38,556 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 2452 words in input language, 21 words in output
2022-09-10 07:23:38,567 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:23:41,430 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:23:41,430 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:23:41,508 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:23:41,508 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/mawps/dense/models/run_cv_mawps_fold2.
2022-09-10 07:23:47,278 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:24:00,005 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.3723958333333333	
 equation acc epoch: 0.3645833333333333	
 max val acc: 0.3723958333333333	
 equation acc: 0.3645833333333333	
2022-09-10 07:24:00,005 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 12s
2022-09-10 07:24:00,008 | INFO | main_after.py: 96 : main() ::	 Experiment Name: run_cv_mawps
2022-09-10 07:24:00,008 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:24:00,008 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:24:00,437 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:24:00,437 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 1537
2022-09-10 07:24:00,437 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 384
2022-09-10 07:24:00,437 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: ['0.01', '12.0', '1.0', '100.0', '0.1', '0.5', '3.0', '4.0', '7.0']
2022-09-10 07:24:00,437 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:24:00,437 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:24:00,631 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 2449 / 2449 = 1.0
2022-09-10 07:24:00,762 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 2452 words in input language, 21 words in output
2022-09-10 07:24:00,773 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:24:03,600 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:24:03,600 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:24:03,677 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:24:03,678 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/mawps/dense/models/run_cv_mawps_fold3.
2022-09-10 07:24:09,518 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:24:22,430 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.21875	
 equation acc epoch: 0.21354166666666666	
 max val acc: 0.21875	
 equation acc: 0.21354166666666666	
2022-09-10 07:24:22,430 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 12s
2022-09-10 07:24:22,432 | INFO | main_after.py: 96 : main() ::	 Experiment Name: run_cv_mawps
2022-09-10 07:24:22,433 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:24:22,433 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:24:22,861 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:24:22,862 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 1537
2022-09-10 07:24:22,862 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 384
2022-09-10 07:24:22,862 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: ['0.01', '12.0', '1.0', '100.0', '0.1', '0.5', '3.0', '4.0', '7.0']
2022-09-10 07:24:22,862 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:24:22,862 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:24:23,057 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 2449 / 2449 = 1.0
2022-09-10 07:24:23,099 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 2452 words in input language, 21 words in output
2022-09-10 07:24:23,110 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:24:25,967 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:24:25,967 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:24:26,046 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:24:26,046 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/mawps/dense/models/run_cv_mawps_fold4.
2022-09-10 07:24:32,048 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:24:47,653 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.4765625	
 equation acc epoch: 0.4713541666666667	
 max val acc: 0.4765625	
 equation acc: 0.4713541666666667	
2022-09-10 07:24:47,653 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 15s
2022-09-10 07:24:47,653 | INFO | main_after.py: 272 : main() ::	 Final Val score: 0.4166666666666667
Transfer numbers...
initialize by one_shot_gm
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.277802464645711
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.5947354288261997
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 1.0
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.17416212293836808
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.17827351888020837
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.45371500651041663
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.4868638780381944
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.32718912760416663
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.4213388231065538
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.23380533854166663
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.2448950873480903
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.4154069688585069
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.503173828125
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.30527920193142366
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.4032355414496528
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.2515190972222222
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.2620561387803819
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.38568623860677087
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.4797821044921875
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.30035697089301217
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.39529715643988717
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.23903232150607634
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.2526533338758681
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.36784871419270837
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.45603434244791663
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.30854670206705725
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.4067420959472656
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.23359849717881942
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.2482452392578125
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.3758867051866319
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.4699384901258681
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.32778422037760413
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.43270111083984375
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.2501848008897569
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.2650146484375
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.3523847791883681
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.4436509874131944
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.3401141696506076
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.45428975423177087
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.24952189127604163
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.26135592990451384
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.35213724772135413
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.4283667670355903
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.3259798685709635
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.44313346015082467
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.23310343424479163
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.2454138861762153
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.4252031114366319
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5051252577039931
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.32094828287760413
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.42909961276584196
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.24350992838541663
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.25487772623697913
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.39150492350260413
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.4752197265625
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.32453791300455725
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.41730499267578125
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.24800448947482634
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.2563340928819444
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.4121856689453125
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.47839694552951384
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.34832678900824654
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.40990659925672746
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.24280802408854163
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.25497775607638884
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.40388149685329866
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.4632941351996528
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.3708212110731337
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.42254765828450525
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.236846923828125
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.24364217122395837
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.3602074517144097
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.43085734049479163
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.35518646240234375
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.48979483710394967
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.6916775173611112
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.4181404113769531
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.6307792663574219
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.38821729024251306
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.56011962890625
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.4581298828125
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.4124784469604492
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.46580076217651367
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.42844295501708984
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.3449897766113281
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.3536224365234375
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.31388092041015625
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.32491111755371094
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.40185546875
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.3730049133300781
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.45703125
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.36952972412109375
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.4296875
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.02766927083333337
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.5966099330357143
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.6549737112862724
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.5147628784179688
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.537792750767299
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.7164350237165178
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.6316746303013393
Final sparsity level of 0.36: 0.3600000196601486
Transfer numbers...
initialize by one_shot_gm
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.5597831752130376
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.8385821984435797
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 1.0
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.11080593532986116
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.11417812771267366
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.30201382107204866
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.32745022243923616
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.21249728732638884
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.28043619791666663
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.14915805392795134
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.15704684787326384
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.2739020453559028
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.3410661485460069
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.1977386474609375
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.267645517985026
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.1604461669921875
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.16762797037760413
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.2525482177734375
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.32418314615885413
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.19415749443901908
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.26297505696614587
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.1521759033203125
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.16215006510416663
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.24001905653211808
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.309051513671875
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.1996264987521701
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.2715013292100694
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.149566650390625
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.1586388481987847
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.24600558810763884
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.3216264512803819
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.2124154832628038
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.29125383165147567
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.16088358561197913
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.16995578342013884
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.23088412814670134
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.29960971408420134
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.22106467352973092
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.30736965603298616
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.1597222222222222
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.16729566786024308
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.23107571072048616
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.2870008680555556
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.21112526787651908
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.29875903659396696
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.1482391357421875
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.15732998318142366
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.28215535481770837
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.3441229926215278
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.20761701795789933
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.2873124016655816
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.15547010633680558
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.16273668077256942
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.2564697265625
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.31694369845920134
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.20988337198893225
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.27766291300455725
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.15874905056423616
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.16412183973524308
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.2708282470703125
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.31787109375
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.22566180759006071
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.2700954013400607
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.15520053439670134
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.16260274251302087
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.2651807996961806
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.3057691786024306
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.2405874464246962
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.2779655456542969
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.1506873236762153
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.15515475802951384
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.23542616102430558
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.280975341796875
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.2298562791612413
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.32581329345703125
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.47794596354166663
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.48034540812174475
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.7319717407226562
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.4534123738606771
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.6915159225463867
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.4904451370239258
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.3800945281982422
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.4860353469848633
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.37589550018310547
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.18022537231445312
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.2089710235595703
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.16617202758789062
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.19832229614257812
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.14306640625
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.4303092956542969
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.501953125
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.21587753295898438
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.18359375
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.15592447916666663
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.4892229352678571
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.548657008579799
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.44070325578962055
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.48760441371372765
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.6264103480747768
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.6016028267996651
Final sparsity level of 0.36: 0.3600000196601486
Transfer numbers...
initialize by one_shot_gm
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.47074398914917603
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.8113853761348897
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 1.0
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.13917202419704866
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.14302402072482634
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.3734198676215278
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.40289645724826384
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.2652570936414931
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.34664832221137154
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.1868981255425347
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.19662136501736116
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.340118408203125
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.417938232421875
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.24718475341796875
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.3312416076660156
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.2014227973090278
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.2103814019097222
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.3143937852647569
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.39853413899739587
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.24296654595269096
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.32503255208333337
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.1914520263671875
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.20323011610243058
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.2998725043402778
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.3792792426215278
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.24952867296006942
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.33555687798394096
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.18806796603732634
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.19944424099392366
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.3066168891059028
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.3932206895616319
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.26571994357638884
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.3586315578884549
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.20162455240885413
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.213165283203125
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.28779093424479163
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.36878628200954866
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.27612813313802087
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.3775617811414931
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.20051405164930558
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.21013217502170134
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.2875993516710069
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.3540276421440972
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.2640643649631076
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.3678237067328559
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.18660990397135413
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.19758436414930558
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.3497483995225694
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.4223192003038194
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.259881337483724
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.35495800442165804
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.19536336263020837
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.20452880859375
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.3199700249565972
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.39246622721354163
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.26291910807291663
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.3439598083496094
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.1989373101128472
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.2063819037543403
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.3369055853949653
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.39330715603298616
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.2825537787543403
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.33594258626302087
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.1946326361762153
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.20453219943576384
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.33004252115885413
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.38025919596354163
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.3008842468261719
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.34629694620768225
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.1897498236762153
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.19577534993489587
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.29323323567708337
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.3507859971788194
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.2879664103190104
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.4043252733018663
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.5831587049696181
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.3389606475830078
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.5367794036865234
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.337921142578125
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.5339775085449219
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.3869624137878418
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.3526792526245117
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.3761777877807617
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.33940792083740234
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.20721817016601562
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.2119140625
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.18927383422851562
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.20592117309570312
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.195068359375
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.356689453125
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.44921875
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.217583974202474
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.244140625
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.080078125
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.4768589564732143
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.5279584612165178
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.3997650146484375
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.4275970458984375
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.571366446358817
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.5126974923270089
Final sparsity level of 0.36: 0.3600000404279111
Transfer numbers...
initialize by one_shot_gm
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.3179792153668888
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.6521512240596627
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 1.0
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.1685265435112847
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.1727142333984375
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.44098239474826384
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.47388712565104163
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.3174396091037326
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.40970653957790804
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.22635904947916663
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.23706563313802087
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.4036390516493056
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.4899275037977431
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.2961141798231337
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.39211018880208337
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.2434624565972222
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.25401645236545134
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.3746015760633681
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.4672987196180556
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.2912724812825521
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.38435702853732634
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.23151991102430558
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.24472554524739587
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.3570556640625
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.44414096408420134
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.29913202921549475
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.39557393391927087
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.22645229763454866
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.24072435167100692
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.3650987413194444
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.4581587049696181
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.318023681640625
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.42113961113823783
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.24242824978298616
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.25671725802951384
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.34225802951388884
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.43198818630642366
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.330084482828776
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.44244681464301217
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.24205186631944442
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.25334506564670134
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.3420121934678819
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.416717529296875
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.3163015577528212
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.4315058390299479
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.22583177354600692
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.2379082573784722
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.4136522081163194
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.49247402615017366
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.31140009562174475
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.41758812798394096
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.23591783311631942
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.24687703450520837
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.38037109375
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.46242947048611116
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.3148545159233941
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.40582148234049475
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.2401970757378472
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.2485283745659722
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.40037197536892366
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.46541849772135413
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.3379495408799913
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.3984116448296441
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.2350989447699653
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.24686686197916663
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.39226786295572913
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.45059712727864587
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.3597234090169271
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.41056484646267366
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.22947353786892366
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.2359619140625
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.3495635986328125
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.4184722900390625
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.3445913526746962
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.4764624701605903
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.6753556993272569
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.4040788014729818
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.6220664978027344
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.3734722137451172
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.5505771636962891
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.43457603454589844
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.3733205795288086
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.4283275604248047
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.3740119934082031
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.28855133056640625
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.3067607879638672
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.2660331726074219
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.28559303283691406
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.3349609375
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.33330726623535156
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.451171875
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.30437215169270837
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.38671875
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.03515625
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.5663037981305803
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.616194588797433
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.45657893589564735
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.4987749372209821
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.6663077218191964
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.5857947213309151
Final sparsity level of 0.36: 0.3600385023242172
Transfer numbers...
initialize by one_shot_gm
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.48006666231473194
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.8039579280155642
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 1.0
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.1347130669487847
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.13843282063802087
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.3624352349175347
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.39116414388020837
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.25678465101453996
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.33627022637261283
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.18075731065538192
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.1901923285590278
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.32961527506510413
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.4060228135850694
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.2393120659722222
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.32125981648763025
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.19465806749131942
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.2033843994140625
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.3047620985243056
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.3870680067274306
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.23524644639756942
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.315375010172526
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.1850348578559028
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.19645182291666663
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.2905442979600694
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.36877780490451384
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.2416729397243924
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.32566706339518225
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.18211025661892366
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.19290500217013884
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.2974175347222222
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.38230387369791663
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.25730090671115446
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.3483123779296875
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.19511752658420134
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.20629374186197913
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.27907986111111116
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.35821194118923616
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.2673967149522569
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.36696794297960067
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.19417148166232634
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.2032318115234375
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.2790086534288194
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.34373135036892366
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.2557699415418837
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.35719384087456596
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.1804131401909722
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.19082132975260413
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.3389977349175347
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.41046142578125
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.2515436808268229
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.3445671929253472
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.18883260091145837
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.19758775499131942
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.31011962890625
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.3808305528428819
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.2544704013400607
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.3337448967827691
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.19215562608506942
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.19940524631076384
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.3266143798828125
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.3814629448784722
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.2735269334581163
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.32595019870334196
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.1882476806640625
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.19757249620225692
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.32002597384982634
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.36861165364583337
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.29142464531792533
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.3358573913574219
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.1835564507378472
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.18904283311631942
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.28426784939236116
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.34002176920572913
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.27880944146050346
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.392724355061849
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.5673234727647569
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.4090843200683594
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.63995361328125
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.370764414469401
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.5670700073242188
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.4161663055419922
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.3477144241333008
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.4073820114135742
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.34944820404052734
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.20175933837890625
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.21178436279296875
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.18322372436523438
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.2005138397216797
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.199462890625
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.31897544860839844
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.390625
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.214691162109375
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.265625
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.09798177083333337
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.48145948137555805
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.5204511369977678
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.38913072858537945
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.4333953857421875
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.5757511683872768
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.5334091186523438
Final sparsity level of 0.36: 0.3600000127375611
wandb: WARNING W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.
2022-09-10 07:24:50,714 | INFO | main_after.py: 96 : main() ::	 Experiment Name: run_cv_mawps
2022-09-10 07:24:50,715 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:24:50,715 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:24:51,220 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:24:51,220 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 1537
2022-09-10 07:24:51,220 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 384
2022-09-10 07:24:51,220 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: ['0.5', '3.0', '100.0', '12.0', '0.01', '0.1', '1.0', '4.0', '7.0']
2022-09-10 07:24:51,221 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:24:51,221 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:24:51,415 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 2449 / 2449 = 1.0
2022-09-10 07:24:51,461 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 2452 words in input language, 21 words in output
2022-09-10 07:24:51,471 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:24:54,534 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:24:54,534 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:24:57,928 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:24:57,928 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/mawps/dense/models/run_cv_mawps_fold0.
2022-09-10 07:25:04,399 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:27:09,471 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.026041666666666668	
 equation acc epoch: 0.026041666666666668	
 max val acc: 0.026041666666666668	
 equation acc: 0.026041666666666668	
2022-09-10 07:27:09,471 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 2m 5s
2022-09-10 07:27:09,473 | INFO | main_after.py: 96 : main() ::	 Experiment Name: run_cv_mawps
2022-09-10 07:27:09,474 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:27:09,474 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:27:09,901 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:27:09,901 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 1537
2022-09-10 07:27:09,901 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 384
2022-09-10 07:27:09,901 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: ['0.01', '12.0', '1.0', '100.0', '0.1', '0.5', '3.0', '4.0', '7.0']
2022-09-10 07:27:09,901 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:27:09,901 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:27:10,092 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 2449 / 2449 = 1.0
2022-09-10 07:27:10,135 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 2452 words in input language, 21 words in output
2022-09-10 07:27:10,146 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:27:13,057 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:27:13,057 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:27:13,137 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:27:13,137 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/mawps/dense/models/run_cv_mawps_fold1.
2022-09-10 07:27:17,504 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:27:34,143 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.171875	
 equation acc epoch: 0.16666666666666666	
 max val acc: 0.171875	
 equation acc: 0.16666666666666666	
2022-09-10 07:27:34,143 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 16s
2022-09-10 07:27:34,146 | INFO | main_after.py: 96 : main() ::	 Experiment Name: run_cv_mawps
2022-09-10 07:27:34,146 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:27:34,146 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:27:34,574 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:27:34,574 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 1537
2022-09-10 07:27:34,574 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 384
2022-09-10 07:27:34,574 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: ['0.01', '12.0', '1.0', '100.0', '0.1', '0.5', '3.0', '4.0', '7.0']
2022-09-10 07:27:34,574 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:27:34,574 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:27:34,769 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 2449 / 2449 = 1.0
2022-09-10 07:27:34,812 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 2452 words in input language, 21 words in output
2022-09-10 07:27:34,823 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:27:37,707 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:27:37,707 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:27:37,787 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:27:37,787 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/mawps/dense/models/run_cv_mawps_fold2.
2022-09-10 07:27:42,849 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:27:54,989 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.12760416666666666	
 equation acc epoch: 0.12760416666666666	
 max val acc: 0.12760416666666666	
 equation acc: 0.12760416666666666	
2022-09-10 07:27:54,989 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 12s
2022-09-10 07:27:54,991 | INFO | main_after.py: 96 : main() ::	 Experiment Name: run_cv_mawps
2022-09-10 07:27:54,991 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:27:54,991 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:27:55,419 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:27:55,419 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 1537
2022-09-10 07:27:55,419 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 384
2022-09-10 07:27:55,420 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: ['0.01', '12.0', '1.0', '100.0', '0.1', '0.5', '3.0', '4.0', '7.0']
2022-09-10 07:27:55,420 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:27:55,420 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:27:55,614 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 2449 / 2449 = 1.0
2022-09-10 07:27:55,656 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 2452 words in input language, 21 words in output
2022-09-10 07:27:55,667 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:27:58,450 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:27:58,450 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:27:58,530 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:27:58,530 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/mawps/dense/models/run_cv_mawps_fold3.
2022-09-10 07:28:03,422 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:28:15,393 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.19270833333333334	
 equation acc epoch: 0.19010416666666666	
 max val acc: 0.19270833333333334	
 equation acc: 0.19010416666666666	
2022-09-10 07:28:15,393 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 11s
2022-09-10 07:28:15,396 | INFO | main_after.py: 96 : main() ::	 Experiment Name: run_cv_mawps
2022-09-10 07:28:15,396 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:28:15,396 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:28:15,824 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:28:15,824 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 1537
2022-09-10 07:28:15,824 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 384
2022-09-10 07:28:15,824 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: ['0.01', '12.0', '1.0', '100.0', '0.1', '0.5', '3.0', '4.0', '7.0']
2022-09-10 07:28:15,824 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:28:15,824 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:28:16,019 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 2449 / 2449 = 1.0
2022-09-10 07:28:16,061 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 2452 words in input language, 21 words in output
2022-09-10 07:28:16,073 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:28:18,914 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:28:18,914 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:28:18,993 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:28:18,993 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/mawps/dense/models/run_cv_mawps_fold4.
2022-09-10 07:28:22,001 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:28:39,120 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.09114583333333333	
 equation acc epoch: 0.08333333333333333	
 max val acc: 0.09114583333333333	
 equation acc: 0.08333333333333333	
2022-09-10 07:28:39,121 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 17s
2022-09-10 07:28:39,121 | INFO | main_after.py: 272 : main() ::	 Final Val score: 0.121875
Transfer numbers...
initialize by one_shot_gm
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.34792143307138834
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.7114061486705577
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 1.0
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.2571699354383681
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.26313273111979163
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.623291015625
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6572791205512153
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.4674703809950087
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5825788709852431
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.34250895182291663
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.35825602213541663
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5799679226345487
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6710951063368056
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.4383498297797309
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5599271986219618
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.3682691786024306
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.38250223795572913
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5436164008246528
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6441023084852431
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.4320483737521701
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.548911624484592
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.35006374782986116
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.37018839518229163
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5223015679253472
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6127438015407987
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.4435640970865885
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5611737569173176
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.34363132052951384
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.3633863661024306
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5308363172743056
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6241285536024306
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.46873770819769967
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5906978183322482
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.3655683729383681
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.38499959309895837
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5004085964626737
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5982157389322917
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.48520406087239587
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6160002814398872
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.3657243516710069
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.38222757975260413
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5011172824435763
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.586761474609375
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.467193603515625
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6032799614800347
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.34235805935329866
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.35983615451388884
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5887061225043403
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6707407633463542
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.46155760023328996
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5897322760687934
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.3576795789930556
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.37400309244791663
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5520155164930556
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6452501085069444
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.46682824028862846
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5783945719401042
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.36460367838541663
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.37722269694010413
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5793745252821181
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6552022298177083
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5002772013346355
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5742263793945312
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.3573387993706597
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.3742353651258681
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5707668728298612
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6417897542317708
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5313474867078993
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.592659420437283
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.3499365912543403
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.3596564398871528
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5147688123914931
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6050177680121528
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5116471184624566
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6717809041341145
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.8760511610243056
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.5619277954101562
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.7637557983398438
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.5266036987304688
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.6973733901977539
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.6109118461608887
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.5819158554077148
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.6186699867248535
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.600398063659668
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.5018157958984375
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.5095481872558594
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.4568595886230469
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.4722423553466797
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.570556640625
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.5195274353027344
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.556640625
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.5244738260904949
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.544921875
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.0390625
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.7618408203125
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.8043343680245536
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.6974193028041294
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.7165320260184151
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.867966788155692
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.8092313494001115
Final sparsity level of 0.488: 0.4880000198816714
Transfer numbers...
initialize by one_shot_gm
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.6050873081004012
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.8777358949416343
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 1.0
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.1930609809027778
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.19758436414930558
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.49325052897135413
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.527435302734375
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.35933897230360246
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.45945612589518225
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.2583685980902778
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.2707960340711806
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.4539099799262153
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5437062581380208
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.3355055914984809
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.43995157877604163
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.2777947319878472
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.28915744357638884
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.42203267415364587
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5194549560546875
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.33035575018988717
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.4307428995768229
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.2640092637803819
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.2789086235894097
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.4026201036241319
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.4932878282335069
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.3391075134277344
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.44263203938802087
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.25798373752170134
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.2740325927734375
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.41063944498697913
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5066240098741319
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.3599590725368924
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.46988635592990446
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.27593994140625
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.29193454318576384
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.3854997422960069
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.4802178276909722
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.37305450439453125
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.4922574361165365
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.27547030978732634
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.28801981608072913
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.3852403428819444
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.46527438693576384
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.3578758239746094
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.48089048597547746
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.25734965006510413
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.2710639105902778
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.4627465142144097
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5450914171006944
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.35284423828125
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.4666158888075087
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.2693939208984375
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.28175862630208337
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.4287855360243056
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5157623291015625
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.35669453938802087
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.4546487596299913
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.2739715576171875
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.28361850314670134
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.4508683946397569
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5204518636067708
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.3829087151421441
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.44776323106553817
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.26831563313802087
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.281524658203125
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.4416436089409722
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5048726399739583
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.40774408976236975
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.4616440667046441
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.26226467556423616
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.2697601318359375
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.3951856825086806
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.47128804524739587
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.39095094468858504
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5328915913899739
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.7399224175347222
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.5992902119954426
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.8245229721069336
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.5678424835205078
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.7858829498291016
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.6313896179199219
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.5833759307861328
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.6260967254638672
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.5688600540161133
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.3103141784667969
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.3545665740966797
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.2883262634277344
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.3388195037841797
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.255126953125
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.5867900848388672
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.55859375
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.34714508056640625
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.251953125
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.21126302083333337
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.687030247279576
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.7532969883510044
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.6402566092354911
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.7001964024135044
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.829345703125
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.812457493373326
Final sparsity level of 0.488: 0.4880000268042589
Transfer numbers...
initialize by one_shot_gm
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.5239933819506615
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.8656523589494163
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 1.0
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.2237616644965278
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.2293311225043403
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5578392876519097
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5927530924479167
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.4122551812065972
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5203289455837674
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.2986433241102431
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.3132544623480903
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.51593017578125
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6083662245008681
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.38545523749457467
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.49907472398546004
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.3216925726996528
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.33439975314670134
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.4818284776475694
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5820990668402778
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.3796454535590278
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.48877292209201384
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.30535888671875
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.32294209798177087
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.46048990885416663
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5526970757378472
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.3896937900119357
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5010664198133681
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.29914347330729163
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.31700473361545134
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.46924167209201384
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5649787055121528
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.41295369466145837
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5292074415418837
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.3186713324652778
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.3365258110894097
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.4406958685980903
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5383165147569444
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.42769198947482634
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5530654059516059
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.3183152940538194
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.3334977891710069
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.44079081217447913
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5245954725477431
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.41090053982204866
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5411961873372395
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.29771762424045134
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.3135460747612847
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5247158474392362
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6083424886067708
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.4053959316677518
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5269775390625
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.3115302191840278
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.3262939453125
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.48863728841145837
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5802273220486112
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.40989557902018225
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5150638156467013
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.31720309787326384
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.32847934299045134
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5132412380642362
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5875430636935763
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.4399545457628038
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5090955098470051
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.3111860487196181
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.32593960232204866
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5039842393663194
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5726352267795138
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.4675441318088107
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5248985290527344
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.3038550482855903
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.3125288221571181
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.4529096815321181
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5367160373263888
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.44901529947916663
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6010526021321614
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.8123677571614584
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.4647712707519531
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.6687602996826172
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.46129417419433594
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.6598014831542969
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.5294771194458008
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.5264043807983398
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.5190110206604004
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.5064525604248047
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.3289947509765625
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.3355388641357422
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.3020591735839844
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.32716941833496094
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.311767578125
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.515533447265625
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.529296875
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.3400052388509115
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.33203125
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.11458333333333337
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.6643404279436385
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.7149505615234375
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.5861729213169643
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.621596200125558
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.7651192801339286
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.7154497419084822
Final sparsity level of 0.488: 0.4880000198816714
Transfer numbers...
initialize by one_shot_gm
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.38842514879472134
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.7524623054474708
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 1.0
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.25168185763888884
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.25764804416232634
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6133863661024306
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6474931504991319
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.4588801066080729
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5729310777452257
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.33543735080295134
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.3507300482855903
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5700412326388888
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6613871256510417
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.43007024129231775
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5504256354437934
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.3607550726996528
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.3746422661675347
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5339185926649306
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6346316867404513
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.4238539801703559
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5394766065809462
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.3427564832899306
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.36250813802083337
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5125003390842013
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6034681532118056
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.43505096435546875
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5515645345052083
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.3364189995659722
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.35589260525173616
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5212215847439237
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6149037679036458
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.45994440714518225
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5810890197753906
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.3578711615668403
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.37723286946614587
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.4908735487196181
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5887468126085069
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.4761500888400607
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6060189141167535
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.3579474555121528
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.3744184705946181
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.49135843912760413
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5766923692491319
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.45839860704210067
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5933210584852431
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.334869384765625
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.35216946072048616
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5787336561414931
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6610107421875
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.45268842909071183
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5797865125868056
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.3500891791449653
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.3662482367621528
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5419989691840278
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6354251437717013
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.4578904045952691
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5682144165039062
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.35660129123263884
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.3693016899956597
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5688730875651042
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6448482937282987
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.49063873291015625
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5637758043077257
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.34955851236979163
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.3660430908203125
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5601874457465278
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6310831705729167
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5213868882921007
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5815883212619357
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.3423478868272569
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.3518235948350694
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5045827229817708
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5944552951388888
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.501625484890408
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.660257551405165
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.8662618001302084
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.5407053629557292
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.7491378784179688
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.5045420328776042
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.6793451309204102
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.5802793502807617
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.5353021621704102
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.5733332633972168
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.5335931777954102
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.42603302001953125
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.45062828063964844
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.39450836181640625
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.4227581024169922
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.49169921875
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.4780254364013672
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.546875
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.44371159871419275
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.46875
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.0517578125
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.7285450526646206
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.773700169154576
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.632676260811942
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.6788842337472099
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.8257805960518974
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.7675977434430803
Final sparsity level of 0.488: 0.4880000268042589
Transfer numbers...
initialize by one_shot_gm
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.5329162988245632
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.8590583454928664
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 1.0
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.2190941704644097
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.2245941162109375
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5480719672309028
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5831332736545138
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.4041985405815972
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5109939575195312
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.2925092909071181
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.30660502115885413
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5065290662977431
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5983598497178819
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.3777033487955729
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.4900902642144097
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.31503634982638884
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.3272230360243056
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.4725511338975694
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5727216932508681
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.3719630771213107
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.4799452887641059
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.2992112901475694
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.3159349229600694
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.45157368977864587
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5437367757161458
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.3818401760525174
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.4921048482259115
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.2926313612196181
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.3104637993706597
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.4596727159288194
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5559861924913194
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.4047868516710069
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5201314290364583
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.31212192111545134
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.3292914496527778
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.43216281467013884
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5292714436848958
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.4191152784559462
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5436774359809028
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.3115166558159722
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.32628885904947913
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.43215433756510413
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5155605740017362
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.4026006062825521
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5320040384928386
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.2913360595703125
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.30694071451822913
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5150010850694444
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5988498263888888
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.3972278171115451
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5176239013671875
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.3048790825737847
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.31912570529513884
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.47949727376302087
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5705227322048612
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.401458740234375
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5056169297960069
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.31029256184895837
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.32130771213107634
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5035196940104167
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5770907931857638
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.43112055460611975
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.4996337890625
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.30434841579861116
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.31909518771701384
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.4943695068359375
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5620914035373263
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.4582146538628472
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5150557623969184
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.2972700330946181
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.30564541286892366
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.4441613091362847
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5267418755425347
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.4398918151855469
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.590629153781467
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.8016815185546875
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.5332291920979817
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.7531223297119141
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.4913113911946615
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.6858291625976562
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.5579433441162109
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.5219240188598633
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.5481352806091309
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.5195550918579102
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.3240776062011719
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.3374214172363281
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.29457855224609375
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.32132530212402344
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.323974609375
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.46598243713378906
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.46484375
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.3368415832519531
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.357421875
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.13313802083333337
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.6688929966517857
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.7090170724051339
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.5740596226283482
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.6295460292271206
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.773345947265625
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.7409907749720982
Final sparsity level of 0.488: 0.4880000268042589
wandb: WARNING W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.
2022-09-10 07:28:42,397 | INFO | main_after.py: 96 : main() ::	 Experiment Name: run_cv_mawps
2022-09-10 07:28:42,397 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:28:42,397 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:28:42,905 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:28:42,905 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 1537
2022-09-10 07:28:42,905 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 384
2022-09-10 07:28:42,905 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: ['0.5', '3.0', '100.0', '12.0', '0.01', '0.1', '1.0', '4.0', '7.0']
2022-09-10 07:28:42,905 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:28:42,905 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:28:43,101 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 2449 / 2449 = 1.0
2022-09-10 07:28:43,145 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 2452 words in input language, 21 words in output
2022-09-10 07:28:43,156 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:28:46,473 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:28:46,473 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:28:49,863 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:28:49,863 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/mawps/dense/models/run_cv_mawps_fold0.
2022-09-10 07:28:53,177 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:29:29,949 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.140625	
 equation acc epoch: 0.140625	
 max val acc: 0.140625	
 equation acc: 0.140625	
2022-09-10 07:29:29,950 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 36s
2022-09-10 07:29:29,952 | INFO | main_after.py: 96 : main() ::	 Experiment Name: run_cv_mawps
2022-09-10 07:29:29,952 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:29:29,952 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:29:30,382 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:29:30,382 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 1537
2022-09-10 07:29:30,382 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 384
2022-09-10 07:29:30,382 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: ['0.01', '12.0', '1.0', '100.0', '0.1', '0.5', '3.0', '4.0', '7.0']
2022-09-10 07:29:30,382 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:29:30,382 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:29:30,574 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 2449 / 2449 = 1.0
2022-09-10 07:29:30,616 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 2452 words in input language, 21 words in output
2022-09-10 07:29:30,716 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:29:33,925 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:29:33,925 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:29:34,003 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:29:34,003 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/mawps/dense/models/run_cv_mawps_fold1.
2022-09-10 07:29:38,967 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:29:56,311 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.046875	
 equation acc epoch: 0.046875	
 max val acc: 0.046875	
 equation acc: 0.046875	
2022-09-10 07:29:56,311 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 17s
2022-09-10 07:29:56,315 | INFO | main_after.py: 96 : main() ::	 Experiment Name: run_cv_mawps
2022-09-10 07:29:56,315 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:29:56,315 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:29:56,746 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:29:56,746 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 1537
2022-09-10 07:29:56,746 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 384
2022-09-10 07:29:56,746 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: ['0.01', '12.0', '1.0', '100.0', '0.1', '0.5', '3.0', '4.0', '7.0']
2022-09-10 07:29:56,746 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:29:56,746 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:29:56,941 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 2449 / 2449 = 1.0
2022-09-10 07:29:56,983 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 2452 words in input language, 21 words in output
2022-09-10 07:29:56,994 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:29:59,875 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:29:59,876 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:29:59,954 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:29:59,955 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/mawps/dense/models/run_cv_mawps_fold2.
2022-09-10 07:30:05,800 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:30:18,751 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.13541666666666666	
 equation acc epoch: 0.13541666666666666	
 max val acc: 0.13541666666666666	
 equation acc: 0.13541666666666666	
2022-09-10 07:30:18,751 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 12s
2022-09-10 07:30:18,754 | INFO | main_after.py: 96 : main() ::	 Experiment Name: run_cv_mawps
2022-09-10 07:30:18,754 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:30:18,754 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:30:19,183 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:30:19,183 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 1537
2022-09-10 07:30:19,183 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 384
2022-09-10 07:30:19,183 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: ['0.01', '12.0', '1.0', '100.0', '0.1', '0.5', '3.0', '4.0', '7.0']
2022-09-10 07:30:19,183 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:30:19,183 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:30:19,378 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 2449 / 2449 = 1.0
2022-09-10 07:30:19,510 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 2452 words in input language, 21 words in output
2022-09-10 07:30:19,521 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:30:22,392 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:30:22,392 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:30:22,469 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:30:22,469 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/mawps/dense/models/run_cv_mawps_fold3.
2022-09-10 07:30:27,640 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:30:39,474 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.16927083333333334	
 equation acc epoch: 0.16145833333333334	
 max val acc: 0.16927083333333334	
 equation acc: 0.16145833333333334	
2022-09-10 07:30:39,474 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 11s
2022-09-10 07:30:39,477 | INFO | main_after.py: 96 : main() ::	 Experiment Name: run_cv_mawps
2022-09-10 07:30:39,478 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:30:39,478 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:30:39,909 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:30:39,909 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 1537
2022-09-10 07:30:39,909 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 384
2022-09-10 07:30:39,909 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: ['0.01', '12.0', '1.0', '100.0', '0.1', '0.5', '3.0', '4.0', '7.0']
2022-09-10 07:30:39,909 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:30:39,909 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:30:40,103 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 2449 / 2449 = 1.0
2022-09-10 07:30:40,145 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 2452 words in input language, 21 words in output
2022-09-10 07:30:40,156 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:30:43,047 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:30:43,047 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:30:43,127 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:30:43,127 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/mawps/dense/models/run_cv_mawps_fold4.
2022-09-10 07:30:46,259 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:31:03,581 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.07291666666666667	
 equation acc epoch: 0.0703125	
 max val acc: 0.07291666666666667	
 equation acc: 0.0703125	
2022-09-10 07:31:03,581 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 17s
2022-09-10 07:31:03,581 | INFO | main_after.py: 272 : main() ::	 Final Val score: 0.11302083333333333
Transfer numbers...
initialize by one_shot_gm
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.412758784691137
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.7937464534695201
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 1.0
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.33866373697916663
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.34513515896267366
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7453748914930556
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7755618625217013
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5859451293945312
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.704728020562066
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.4456091986762153
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.46360439724392366
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7059851752387153
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7864786783854166
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5525101555718316
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6813663906521268
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.477294921875
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.4947272406684028
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6677805582682292
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7585228814019097
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5459815131293403
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6677135891384549
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.4549730088975694
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.4783935546875
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6462792290581597
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7243703206380208
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5587459140353732
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6787923177083333
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.4461958143446181
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.46990458170572913
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6527659098307292
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7306671142578125
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5872599283854167
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.706984625922309
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.47231716579861116
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.4945390489366319
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6182166205512153
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7078280978732638
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.605875227186415
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.731919182671441
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.47302754720052087
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.49166361490885413
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6214803059895833
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7007276746961806
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5863355000813801
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7190441555447049
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.4447920057508681
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.46496242947048616
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7090572781032987
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7806328667534722
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5800933837890625
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7074936760796441
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.46497768825954866
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.48328823513454866
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6775919596354167
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7655741373697916
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5870535108778212
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6984024047851562
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.472900390625
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.4881150987413194
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7069227430555556
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7789425320095487
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6266924540201824
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6972902086046007
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.4641265869140625
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.4846869574652778
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6997443305121528
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7690531412760416
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6624421013726128
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7184727986653645
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.45543246799045134
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.4658372667100694
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6419491238064237
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7379370795355903
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6404618157280816
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7936269972059462
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.9589589436848959
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.6716543833414714
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.8481235504150391
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.6348565419514973
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.7908143997192383
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.7220563888549805
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.7118959426879883
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.7291293144226074
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.7289800643920898
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.63311767578125
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.636932373046875
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.5811653137207031
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.5962982177734375
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.70654296875
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.6347007751464844
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.654296875
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.6451950073242188
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.642578125
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.0498046875
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.8590720040457589
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.8889116559709821
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.8168814522879464
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.8321380615234375
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.940779549734933
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.908158438546317
Final sparsity level of 0.59: 0.5900000135682715
Transfer numbers...
initialize by one_shot_gm
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.6426154143456348
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.906728781614786
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 1.0
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.26995340983072913
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.2757093641493056
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6417185465494792
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6756303575303819
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.4852973090277778
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.601456536187066
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.3585137261284722
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.37453206380208337
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5993109809027778
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.689605712890625
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.45536465115017366
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5788353814019097
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.3849402533637153
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.40006001790364587
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5625695122612847
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6624925401475694
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.449120839436849
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5670483907063801
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.36588541666666663
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.3869849310980903
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.54071044921875
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6299150254991319
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.46048990885416663
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.579388936360677
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.3591223822699653
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.3795928955078125
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5488179524739583
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6405775282118056
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.4861475626627604
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.608380635579427
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.381591796875
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.4016893174913194
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.517578125
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6146731906467013
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5027817620171441
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6337309943305122
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.3820563422309028
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.39882236056857634
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5181528727213542
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.603363037109375
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.4847496880425347
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6210907830132378
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.3581102159288194
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.37611897786458337
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6064554850260417
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6876407199435763
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.4790306091308594
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6077952914767795
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.37443033854166663
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.3910200330946181
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5705888536241319
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6638760036892362
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.48457887437608504
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5966356065538194
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.38132052951388884
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.3940412733289931
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5984734429253472
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6740841335720487
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5188590155707465
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5927954779730903
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.3734452989366319
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.3911827935112847
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5895741780598958
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6608920627170138
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5510762532552083
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6118443806966145
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.36630588107638884
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.37614101833767366
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5331743028428819
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6250254313151042
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.530967288547092
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6907034979926215
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.8908589680989584
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.6851590474446614
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.8789634704589844
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.6539471944173176
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.8453912734985352
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.7247614860534668
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.72064208984375
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.7190241813659668
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.7004270553588867
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.4279022216796875
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.48100852966308594
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.39865875244140625
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.46383094787597656
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.351806640625
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.6760158538818359
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.5859375
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.4545682271321615
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.314453125
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.251953125
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.8062471662248885
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.864758082798549
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.7726058959960938
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.8310917445591518
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.9228384835379464
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.9169692993164062
Final sparsity level of 0.59: 0.5900000135682715
Transfer numbers...
initialize by one_shot_gm
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.5700858626363606
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.903744629539559
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 1.0
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.30304294162326384
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.3090667724609375
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6941036648220487
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7272440592447917
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5349595811631944
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6533593071831597
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.4008500840928819
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.41785685221354163
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6530541314019097
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7399563259548612
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5030581156412761
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6302553812662761
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.4300774468315972
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.4467247856987847
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6155005560980903
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7121531168619792
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.49655490451388884
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6173053317599826
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.4097374810112847
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.43191867404513884
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5930650499131944
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6779412163628472
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.508809831407335
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6290986802842882
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.4015977647569444
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.42337375217013884
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6005622016059028
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6863420274522569
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5358263651529949
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6577135721842449
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.4258355034722222
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.4466874864366319
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5670572916666667
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6616363525390625
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5536278618706597
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6832101609971788
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.42643398708767366
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.4445122612847222
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5688561333550347
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6519402398003472
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.534828609890408
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6701613532172309
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.40011935763888884
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.41949971516927087
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6579555935329862
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7355906168619792
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5287301805284288
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6578424241807725
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.4186028374565972
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.43588765462239587
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6238657633463542
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7157982720269097
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5349405076768663
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6476804945203993
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.4260406494140625
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.43983798556857634
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6528354220920138
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7276509602864583
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5721138848198785
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6448449028862847
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.4176771375868056
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.4367845323350694
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6442430284288194
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7158203125
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6061859130859375
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.664960225423177
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.409393310546875
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.41986423068576384
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5867123074001737
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6823442247178819
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5851470099555122
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7431399027506511
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.9300859239366319
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.5642801920572917
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.756861686706543
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.559109369913737
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.7451009750366211
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.6359114646911621
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.6574125289916992
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.6252894401550293
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.6346206665039062
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.4399223327636719
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.4466075897216797
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.4068450927734375
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.4361228942871094
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.412109375
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.6306400299072266
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.591796875
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.44600931803385413
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.4140625
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.1484375
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.7838058471679688
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.8278459821428572
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.7210442679268974
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.7563672746930803
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.8724801199776786
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.8412148611886161
Final sparsity level of 0.59: 0.5900000204908591
Transfer numbers...
initialize by one_shot_gm
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.4480536748980404
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.8274258268482491
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 1.0
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.33324008517795134
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.3396386040581597
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7381083170572917
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7689751519097222
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5785581800672743
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6974491543240018
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.4388648139105903
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.45690578884548616
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6985236273871528
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.78021240234375
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5454779730902778
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6741909450954862
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.47049967447916663
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.4877539740668403
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6602613661024306
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7522803412543403
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5389200846354167
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6606309678819444
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.44836086697048616
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.47170511881510413
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6387447781032987
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7178175184461806
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5516615973578559
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6717202928331163
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.43961758083767366
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.4633856879340278
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6453094482421875
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7244686550564237
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5800726148817275
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7000791761610243
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.46560329861111116
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.48783535427517366
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6109720865885417
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7014957004123263
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5985404120551215
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7252044677734375
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.4662458631727431
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.4847412109375
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6140323215060763
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6939409044053819
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5788709852430556
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.712267557779948
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.4384172227647569
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.45843844943576384
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7019975450303819
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7746700710720487
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5727759467230903
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7005021837022569
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.4579755995008681
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.47633700900607634
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6699863009982638
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7588144938151041
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5795991685655382
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6913295321994357
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.46617465549045134
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.48097568088107634
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6992136637369792
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7718014187282987
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6190117730034722
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6899566650390625
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.45743306477864587
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.47783915201822913
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6919097900390625
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7618984646267362
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6545223659939237
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7110197279188368
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.44888814290364587
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.4592827690972222
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6337602403428819
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7301873101128472
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6326446533203125
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7867618136935763
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.9555019802517362
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.6468982696533203
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.8316383361816406
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.6097100575764973
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.7695798873901367
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.6898584365844727
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.6650409698486328
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.6828246116638184
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.6602773666381836
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.5488853454589844
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.5739250183105469
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.5110931396484375
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.5434780120849609
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.618408203125
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.5956039428710938
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.62890625
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.5619595845540364
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.560546875
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.072265625
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.8313217163085938
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.8682294573102678
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.7599574497767857
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.8022548130580357
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.9118728637695312
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.8771111624581474
Final sparsity level of 0.59: 0.5900000274134466
Transfer numbers...
initialize by one_shot_gm
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.578327157730694
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.8983817688067445
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 1.0
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.2982042100694444
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.3043229844835069
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6862589518229167
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7198452419704862
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5275989108615451
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6456493801540799
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.39457194010416663
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.4115990532769097
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6451602511935763
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7328457302517362
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.49585554334852433
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6226505703396268
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.4233551025390625
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.43988037109375
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6078016493055556
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7049713134765625
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.4894235399034288
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.609945085313585
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.40306599934895837
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.4253607855902778
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5850626627604167
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6708458794487847
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5015814039442275
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6217994689941406
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.3953331841362847
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.41689724392361116
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5926767985026042
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6794179280598958
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5284029642740886
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.650469462076823
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.41936238606770837
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.4400770399305556
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5596533881293403
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6547987196180556
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5460726420084636
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6759253607855903
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.41968790690104163
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.4377763536241319
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5612962510850694
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6447804768880208
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5272068447536893
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.662819332546658
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.39349365234375
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.41309780544704866
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6500328911675347
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7286427815755208
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5213177998860676
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6503283182779949
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.41189066569010413
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.42908223470052087
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6157803005642362
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7082248263888888
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5272475348578559
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6400544908311632
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.41920132107204866
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.43286641438802087
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6445058186848958
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7197791205512153
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5640085008409288
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6369951036241319
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.41110568576388884
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.42976718478732634
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6359354654947917
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7075330946180556
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5978804694281684
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6568603515625
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.4028303358289931
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.41306728786892366
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5783572726779513
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6736670600043403
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.576851314968533
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7351807488335503
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.9247351752387153
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.6263078053792317
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.8247241973876953
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.5834592183430989
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.763890266418457
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.6589055061340332
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.652979850769043
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.6491808891296387
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.6470603942871094
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.43416595458984375
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.4507770538330078
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.3979835510253906
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.4304046630859375
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.42724609375
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.5755748748779297
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.53125
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.44390106201171875
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.41796875
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.16796875
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.7865044730050224
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.8232269287109375
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.7086857386997768
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.7628228323800224
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.8802457536969867
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.8621651785714286
Final sparsity level of 0.59: 0.5900000204908591
wandb: WARNING W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.
2022-09-10 07:31:06,726 | INFO | main_after.py: 96 : main() ::	 Experiment Name: run_cv_mawps
2022-09-10 07:31:06,726 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:31:06,726 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:31:07,228 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:31:07,228 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 1537
2022-09-10 07:31:07,228 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 384
2022-09-10 07:31:07,228 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: ['0.5', '3.0', '100.0', '12.0', '0.01', '0.1', '1.0', '4.0', '7.0']
2022-09-10 07:31:07,228 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:31:07,229 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:31:07,422 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 2449 / 2449 = 1.0
2022-09-10 07:31:07,468 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 2452 words in input language, 21 words in output
2022-09-10 07:31:07,479 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:31:10,651 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:31:10,651 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:31:13,992 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:31:13,993 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/mawps/dense/models/run_cv_mawps_fold0.
2022-09-10 07:31:17,265 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:31:27,970 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.1484375	
 equation acc epoch: 0.140625	
 max val acc: 0.1484375	
 equation acc: 0.140625	
2022-09-10 07:31:27,970 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 10s
2022-09-10 07:31:27,972 | INFO | main_after.py: 96 : main() ::	 Experiment Name: run_cv_mawps
2022-09-10 07:31:27,972 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:31:27,973 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:31:28,403 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:31:28,404 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 1537
2022-09-10 07:31:28,404 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 384
2022-09-10 07:31:28,404 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: ['0.01', '12.0', '1.0', '100.0', '0.1', '0.5', '3.0', '4.0', '7.0']
2022-09-10 07:31:28,404 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:31:28,404 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:31:28,597 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 2449 / 2449 = 1.0
2022-09-10 07:31:28,640 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 2452 words in input language, 21 words in output
2022-09-10 07:31:28,651 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:31:31,600 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:31:31,600 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:31:31,677 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:31:31,678 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/mawps/dense/models/run_cv_mawps_fold1.
2022-09-10 07:31:34,916 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:31:59,651 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.020833333333333332	
 equation acc epoch: 0.018229166666666668	
 max val acc: 0.020833333333333332	
 equation acc: 0.018229166666666668	
2022-09-10 07:31:59,652 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 24s
2022-09-10 07:31:59,655 | INFO | main_after.py: 96 : main() ::	 Experiment Name: run_cv_mawps
2022-09-10 07:31:59,655 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:31:59,655 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:32:00,084 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:32:00,084 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 1537
2022-09-10 07:32:00,084 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 384
2022-09-10 07:32:00,084 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: ['0.01', '12.0', '1.0', '100.0', '0.1', '0.5', '3.0', '4.0', '7.0']
2022-09-10 07:32:00,084 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:32:00,084 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:32:00,278 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 2449 / 2449 = 1.0
2022-09-10 07:32:00,321 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 2452 words in input language, 21 words in output
2022-09-10 07:32:00,333 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:32:03,174 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:32:03,175 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:32:03,252 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:32:03,252 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/mawps/dense/models/run_cv_mawps_fold2.
2022-09-10 07:32:06,252 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:32:26,442 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.0026041666666666665	
 equation acc epoch: 0.0	
 max val acc: 0.0026041666666666665	
 equation acc: 0.0	
2022-09-10 07:32:26,442 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 20s
2022-09-10 07:32:26,444 | INFO | main_after.py: 96 : main() ::	 Experiment Name: run_cv_mawps
2022-09-10 07:32:26,444 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:32:26,444 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:32:26,872 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:32:26,872 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 1537
2022-09-10 07:32:26,872 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 384
2022-09-10 07:32:26,872 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: ['0.01', '12.0', '1.0', '100.0', '0.1', '0.5', '3.0', '4.0', '7.0']
2022-09-10 07:32:26,872 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:32:26,873 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:32:27,067 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 2449 / 2449 = 1.0
2022-09-10 07:32:27,109 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 2452 words in input language, 21 words in output
2022-09-10 07:32:27,121 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:32:30,103 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:32:30,103 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:32:30,181 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:32:30,181 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/mawps/dense/models/run_cv_mawps_fold3.
2022-09-10 07:32:33,220 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:32:44,924 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.17708333333333334	
 equation acc epoch: 0.1640625	
 max val acc: 0.17708333333333334	
 equation acc: 0.1640625	
2022-09-10 07:32:44,924 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 11s
2022-09-10 07:32:44,928 | INFO | main_after.py: 96 : main() ::	 Experiment Name: run_cv_mawps
2022-09-10 07:32:44,928 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:32:44,928 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:32:45,356 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:32:45,356 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 1537
2022-09-10 07:32:45,356 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 384
2022-09-10 07:32:45,356 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: ['0.01', '12.0', '1.0', '100.0', '0.1', '0.5', '3.0', '4.0', '7.0']
2022-09-10 07:32:45,356 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:32:45,356 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:32:45,553 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 2449 / 2449 = 1.0
2022-09-10 07:32:45,596 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 2452 words in input language, 21 words in output
2022-09-10 07:32:45,607 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:32:48,468 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:32:48,468 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:32:48,546 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:32:48,546 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/mawps/dense/models/run_cv_mawps_fold4.
2022-09-10 07:32:51,527 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:33:12,780 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.044270833333333336	
 equation acc epoch: 0.044270833333333336	
 max val acc: 0.044270833333333336	
 equation acc: 0.044270833333333336	
2022-09-10 07:33:12,780 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 21s
2022-09-10 07:33:12,780 | INFO | main_after.py: 272 : main() ::	 Final Val score: 0.07864583333333333
Transfer numbers...
initialize by one_shot_gm
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.4663033319241354
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.8587011592088197
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 1.0
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.4164343939887153
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.4244622124565972
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8353593614366319
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8581153021918403
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6865840488009982
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7973361545138888
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5400916205512153
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5597669813368056
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8020714653862847
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8642205132378472
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6515634324815538
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7760081821017795
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5755343967013888
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5948350694444444
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7659047444661459
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8402184380425347
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.644854227701823
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7615521748860677
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5509287516276042
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5755123562282987
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7479163275824653
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8079511854383681
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.658251444498698
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7713483174641927
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5411970350477431
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5659637451171875
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7509104410807291
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8107367621527778
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6869413587782118
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7961383395724826
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5695699055989583
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5931006537543403
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7162645128038194
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7916446261935763
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7065972222222222
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8189307318793403
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5709957546657987
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5904320610894097
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7225392659505208
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7898101806640625
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6872329711914062
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8061676025390625
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5395575629340278
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5611911349826388
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8003404405381944
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8571573893229166
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6817232767740886
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7970907423231337
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5638105604383681
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5833079020182292
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7774861653645834
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8515947129991319
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6897866990831163
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7907871670193143
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5719248453776042
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.588409423828125
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8042466905381944
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8668619791666666
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7320056491427951
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7929886711968316
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5634935167100694
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5853152804904513
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8011033799913194
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8605363633897569
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7697033352322049
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8149613274468316
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5535074869791667
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5639767116970487
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7466023763020833
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8364427354600694
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7479718526204426
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8778915405273438
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.9898715549045138
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.7614974975585938
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.9052972793579102
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.7259871164957683
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.8588743209838867
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.8076863288879395
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.8127956390380859
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.8136458396911621
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.8277750015258789
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.7438774108886719
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.7441635131835938
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.6888465881347656
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.7042350769042969
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.8173828125
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.7302989959716797
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.728515625
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.7450853983561199
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.720703125
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.06184895833333337
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.9197202410016742
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.9393637520926339
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.8966304234095982
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.9072636195591518
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.9756349836077008
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.9607336861746651
Final sparsity level of 0.672: 0.6720000357759324
Transfer numbers...
initialize by one_shot_gm
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.6907595732202659
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.9287375364785992
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 1.0
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.34065585666232634
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.3469017876519097
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7461090087890625
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7764163547092013
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5876587761773003
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7063407897949219
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.4475250244140625
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.4656914605034722
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7070092095269097
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7874348958333334
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5541636149088542
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6829770406087239
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.47921413845486116
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.49681430392795134
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6689368353949653
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7593722873263888
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5475828382703993
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6691924201117622
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.4566972520616319
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.4802686903211806
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6473914252387153
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7253146701388888
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5602840847439237
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6802096896701388
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.44778951009114587
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.471771240234375
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6539238823784722
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7314995659722222
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.588879903157552
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7082201639811199
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.4738311767578125
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.4963446723090278
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6193593343098958
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7086588541666667
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.60735109117296
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7331420050726997
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.4747297498914931
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.49312845865885413
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6226094563802083
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7016923692491319
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5879915025499132
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7203309800889757
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.44672139485677087
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.4668867323133681
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7101999918619792
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7813466389973959
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5817765129937066
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7088936699761285
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.4669562445746528
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.4854346381293403
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6787431504991319
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7664082845052084
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.588798099093967
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6998956468370225
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.47504848904079866
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.4900733100043403
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7080908881293403
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7800225151909722
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.628373040093316
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6988385518391926
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.4660729302300347
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.4867028130425347
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7008514404296875
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7702162000868056
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6643481784396701
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7201131184895833
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.4575653076171875
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.46787685818142366
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6432427300347222
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7391594780815972
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6425615946451824
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7950312296549479
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.9592403835720487
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.7488517761230469
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.9134292602539062
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.7196515401204426
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.8852834701538086
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.7893199920654297
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.8078336715698242
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.7836155891418457
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.7862215042114258
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.5256233215332031
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.582794189453125
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.4931793212890625
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.56640625
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.4375
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.7347335815429688
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.607421875
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.5388946533203125
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.365234375
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.28450520833333337
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.8749487740652901
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.922745840890067
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.8545902797154018
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.9047742571149554
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.9637124197823661
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.9632012503487724
Final sparsity level of 0.672: 0.6720000150081697
Transfer numbers...
initialize by one_shot_gm
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.6112608124854936
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.9304702699416343
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 1.0
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.37832302517361116
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.3851335313585069
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7938435872395834
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8204939100477431
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6387244330512153
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.754510243733724
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.4936981201171875
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5129275851779513
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7573157416449653
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8294033474392362
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6039326985677083
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7321400112575955
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5279812282986112
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5463748508029513
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7197418212890625
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8027631971571181
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5974252488878038
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7175754970974393
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5038062201605903
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5282643636067708
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6999138726128472
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7692582872178819
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6103180779351128
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7280972798665364
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.4949629041883681
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5193566216362847
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7050323486328125
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7734883626302084
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6395564609103732
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7548849317762587
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5220608181423612
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5451439751519097
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6696929931640625
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7524261474609375
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6585973103841145
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7787356906467013
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5231374104817708
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5420430501302083
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6743011474609375
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7476365831163194
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6392402648925781
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7657809787326388
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.4931877983940972
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5146365695529513
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7582617865668403
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8222859700520834
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6334071689181857
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.755515628390842
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5155249701605903
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5349443223741319
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7306094699435763
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8122168646918403
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6410081651475694
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7480608622233074
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5240732828776042
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5398322211371528
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7591518825954862
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8272043863932291
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6823933919270833
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7486979166666667
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5153283013237847
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.536651611328125
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7536790635850694
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8192104763454862
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7194684346516926
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7704908582899306
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5054304334852431
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5162319607204862
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6969672309027778
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7915632459852431
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6973287794325087
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8402184380425347
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.9789174397786459
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.6469440460205078
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.8210134506225586
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.6414947509765625
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.8086576461791992
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.7191591262817383
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.7552709579467773
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.7094030380249023
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.7337722778320312
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.5405654907226562
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.5457000732421875
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.5023956298828125
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.5357818603515625
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.5048828125
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.7153663635253906
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.638671875
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.539258321126302
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.490234375
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.17708333333333337
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.8611243111746651
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.89642333984375
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.815948486328125
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.8486306326729911
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.9320079258510044
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.9149453299386161
Final sparsity level of 0.672: 0.6720000150081697
Transfer numbers...
initialize by one_shot_gm
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.4972042704913956
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.8814141536964981
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 1.0
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.4112328423394097
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.419097900390625
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8301612006293403
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8534427218967013
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6804419623480903
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7920010884602865
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5338592529296875
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5533548990885417
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7962476942274306
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8598259819878472
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6453314887152778
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7705069647894965
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5693037245008681
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5882602267795138
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7599334716796875
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8356306287977431
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.638777838812934
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7559899224175347
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5445590549045138
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5692630343967013
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7417093912760417
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8030056423611112
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.652083502875434
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7658581203884549
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5348358154296875
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5599856906467013
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7450629340277778
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8060726589626737
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6809281243218316
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7910088433159722
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5634138319227431
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5870530870225694
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7102728949652778
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7867550320095487
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7004458109537761
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8139538235134549
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5646837022569444
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5842098659939237
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7164272732204862
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7844492594401041
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6809925503200955
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8011733161078559
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5336880154079862
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5552435980902778
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7950846354166666
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8528238932291666
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.675493876139323
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.791923099093967
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5574832492404513
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5771009657118056
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7717234293619791
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8466288248697916
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6835000779893663
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7854042053222656
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5657484266493056
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5819871690538194
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7986874050564237
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8619723849826388
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7257520887586806
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7874336242675781
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5572882758246528
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5791422526041667
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7952033148871528
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8554331461588541
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7633756001790365
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8094834221733941
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5471767849392362
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5577494303385417
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7404056125217013
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8309682210286459
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7414546542697482
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8733016120062934
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.9888034396701388
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.7337779998779297
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.8892784118652344
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.6984710693359375
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.8367176055908203
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.7761311531066895
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.7689762115478516
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.7691707611083984
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.763026237487793
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.6577377319335938
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.6816196441650391
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.6187362670898438
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.6500205993652344
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.728515625
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.6938304901123047
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.7109375
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.6646817525227864
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.65625
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.08626302083333337
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.8983067103794643
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.925950186593192
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.8507461547851562
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.8857062203543526
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.9580721173967633
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.9405779157366071
Final sparsity level of 0.672: 0.6720000080855822
Transfer numbers...
initialize by one_shot_gm
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.6183812771477835
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.9262498986705577
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 1.0
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.37373691134982634
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.380462646484375
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7880859375
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8152431911892362
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6325064765082465
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7486771477593316
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.4878777398003472
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.507171630859375
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7512427435980903
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8244272867838541
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5976473490397136
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7262280782063801
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5217641194661458
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5400831434461806
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7134145100911458
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7975785997178819
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5913001166449653
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7116665310329862
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.497894287109375
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5223812527126737
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6933661566840278
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7638465033637153
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6041793823242188
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7222599453396268
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.4890594482421875
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5133683946397569
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6987253824869792
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7683732774522569
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6332083808051215
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7493095397949219
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5160912407769097
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5390811496310763
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6633165147569444
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7470364040798612
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6522513495551215
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7731598748101128
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5171390109592013
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5359734429253472
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6678483751085069
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7421586778428819
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6328370836046007
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7603560553656684
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.48713175455729163
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5085500081380208
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7523668077256944
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8172810872395834
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.626965840657552
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.749802483452691
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5092654758029513
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5284356011284722
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7242550320095487
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8067389594184028
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6344176398383247
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7420433892144097
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5175882975260417
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5333370632595487
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7529127332899306
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8215230305989584
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6756629943847656
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7425092061360676
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5088517930772569
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5300682915581597
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7468990749782987
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8132544623480903
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7126206292046441
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7643296983506944
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.49934048122829866
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5099368625217013
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6903245713975694
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7851901584201388
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6905394660101997
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8347710503472222
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.9769134521484375
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.7020060221354167
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.8744630813598633
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.660525639851888
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.8212728500366211
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.7370419502258301
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.7510900497436523
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.727597713470459
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.7435922622680664
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.5345535278320312
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.5510120391845703
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.49378204345703125
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.5298347473144531
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.530029296875
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.6614704132080078
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.5859375
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.5365155537923176
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.46875
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.19759114583333337
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.861968994140625
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.892622811453683
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.80487060546875
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.853015354701451
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.9373964582170758
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.9300286429268974
Final sparsity level of 0.672: 0.6720000150081697
wandb: WARNING W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.
2022-09-10 07:33:15,858 | INFO | main_after.py: 96 : main() ::	 Experiment Name: run_cv_mawps
2022-09-10 07:33:15,858 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:33:15,858 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:33:16,362 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:33:16,362 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 1537
2022-09-10 07:33:16,362 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 384
2022-09-10 07:33:16,362 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: ['0.5', '3.0', '100.0', '12.0', '0.01', '0.1', '1.0', '4.0', '7.0']
2022-09-10 07:33:16,362 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:33:16,362 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:33:16,557 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 2449 / 2449 = 1.0
2022-09-10 07:33:16,603 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 2452 words in input language, 21 words in output
2022-09-10 07:33:16,613 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:33:19,911 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:33:19,911 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:33:23,363 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:33:23,363 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/mawps/dense/models/run_cv_mawps_fold0.
2022-09-10 07:33:27,349 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:34:19,915 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.013020833333333334	
 equation acc epoch: 0.0	
 max val acc: 0.013020833333333334	
 equation acc: 0.0	
2022-09-10 07:34:19,915 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 52s
2022-09-10 07:34:19,918 | INFO | main_after.py: 96 : main() ::	 Experiment Name: run_cv_mawps
2022-09-10 07:34:19,918 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:34:19,918 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:34:20,346 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:34:20,346 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 1537
2022-09-10 07:34:20,346 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 384
2022-09-10 07:34:20,347 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: ['0.01', '12.0', '1.0', '100.0', '0.1', '0.5', '3.0', '4.0', '7.0']
2022-09-10 07:34:20,347 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:34:20,347 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:34:20,538 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 2449 / 2449 = 1.0
2022-09-10 07:34:20,580 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 2452 words in input language, 21 words in output
2022-09-10 07:34:20,592 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:34:23,506 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:34:23,506 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:34:23,584 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:34:23,584 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/mawps/dense/models/run_cv_mawps_fold1.
2022-09-10 07:34:28,632 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:34:55,553 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.0	
 equation acc epoch: 0.0	
 max val acc: 0.0	
 equation acc: 0.0	
2022-09-10 07:34:55,553 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 26s
2022-09-10 07:34:55,557 | INFO | main_after.py: 96 : main() ::	 Experiment Name: run_cv_mawps
2022-09-10 07:34:55,557 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:34:55,557 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:34:55,987 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:34:55,987 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 1537
2022-09-10 07:34:55,987 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 384
2022-09-10 07:34:55,987 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: ['0.01', '12.0', '1.0', '100.0', '0.1', '0.5', '3.0', '4.0', '7.0']
2022-09-10 07:34:55,987 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:34:55,987 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:34:56,185 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 2449 / 2449 = 1.0
2022-09-10 07:34:56,228 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 2452 words in input language, 21 words in output
2022-09-10 07:34:56,240 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:34:59,111 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:34:59,111 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:34:59,188 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:34:59,189 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/mawps/dense/models/run_cv_mawps_fold2.
2022-09-10 07:35:02,225 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:35:21,536 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.0390625	
 equation acc epoch: 0.0390625	
 max val acc: 0.0390625	
 equation acc: 0.0390625	
2022-09-10 07:35:21,537 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 19s
2022-09-10 07:35:21,539 | INFO | main_after.py: 96 : main() ::	 Experiment Name: run_cv_mawps
2022-09-10 07:35:21,539 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:35:21,539 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:35:21,967 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:35:21,967 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 1537
2022-09-10 07:35:21,967 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 384
2022-09-10 07:35:21,967 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: ['0.01', '12.0', '1.0', '100.0', '0.1', '0.5', '3.0', '4.0', '7.0']
2022-09-10 07:35:21,967 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:35:21,967 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:35:22,162 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 2449 / 2449 = 1.0
2022-09-10 07:35:22,204 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 2452 words in input language, 21 words in output
2022-09-10 07:35:22,215 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:35:25,082 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:35:25,082 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:35:25,159 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:35:25,159 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/mawps/dense/models/run_cv_mawps_fold3.
2022-09-10 07:35:28,237 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:35:42,709 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.03125	
 equation acc epoch: 0.015625	
 max val acc: 0.03125	
 equation acc: 0.015625	
2022-09-10 07:35:42,709 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 14s
2022-09-10 07:35:42,711 | INFO | main_after.py: 96 : main() ::	 Experiment Name: run_cv_mawps
2022-09-10 07:35:42,711 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:35:42,711 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:35:43,142 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:35:43,142 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 1537
2022-09-10 07:35:43,142 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 384
2022-09-10 07:35:43,142 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: ['0.01', '12.0', '1.0', '100.0', '0.1', '0.5', '3.0', '4.0', '7.0']
2022-09-10 07:35:43,142 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:35:43,142 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:35:43,338 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 2449 / 2449 = 1.0
2022-09-10 07:35:43,381 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 2452 words in input language, 21 words in output
2022-09-10 07:35:43,393 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:35:46,244 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:35:46,244 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:35:46,321 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:35:46,321 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/mawps/dense/models/run_cv_mawps_fold4.
2022-09-10 07:35:49,421 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:36:03,420 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.125	
 equation acc epoch: 0.12239583333333333	
 max val acc: 0.125	
 equation acc: 0.12239583333333333	
2022-09-10 07:36:03,420 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 13s
Traceback (most recent call last):
  File "/home/sliu/miniconda3/envs/prune_cry/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/sliu/miniconda3/envs/prune_cry/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/gpfs/home6/sliu/Projects/SVAMP/code/gts/src/main_after.py", line 276, in <module>
    main()
  File "/gpfs/home6/sliu/Projects/SVAMP/code/gts/src/main_after.py", line 266, in main
    folds_scores.append(float(best_acc[w][0])/best_acc[w][1])
ZeroDivisionError: float division by zero
Transfer numbers...
initialize by one_shot_gm
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.5172709379870022
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.9055330941958495
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 1.0
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.4895460340711806
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.49789937337239587
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8979254828559028
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9119008382161459
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7660149468315972
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8631218804253472
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.624786376953125
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6438361273871528
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8721126980251737
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9147728814019097
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7322832743326824
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8456700642903646
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6621500651041667
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6795671251085069
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8409440782335069
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8963470458984375
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7254965040418837
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8326407538519965
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6372036404079862
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6620229085286458
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8279520670572916
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8696407741970487
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7387953864203559
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8411793178982205
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6275380452473958
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6527659098307292
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8270399305555556
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8713802761501737
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7657907274034288
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8624424404568143
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6559363471137153
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6797366672092013
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7962375217013888
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8564130995008681
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7855326334635416
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8818626403808594
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6587761773003472
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6786448160807292
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8045705159505209
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8579576280381944
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7679265340169271
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8703859117296007
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6264122856987847
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6475711398654513
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8675214979383681
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9103529188368056
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7636159261067709
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8640560574001737
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6534678141276042
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6732991536458333
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8533613416883681
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9105360243055556
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7730467054578993
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8594474792480469
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6631401909722222
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6791653103298612
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8756510416666666
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9242757161458334
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8140962388780382
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8646943834092882
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6548512776692708
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6758338080512153
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8755611843532987
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9207611083984375
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8507355584038628
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.885284423828125
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6443447536892362
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6552564832899306
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.829986572265625
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9033152262369791
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8325000339084201
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9326502482096354
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.9982401529947916
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.8339112599690756
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.9438619613647461
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.8024406433105469
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.9088001251220703
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.8731770515441895
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.8866920471191406
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.8772506713867188
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.8980293273925781
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.8336944580078125
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.8305130004882812
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.7813034057617188
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.7962398529052734
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.88623046875
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.8092021942138672
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.8046875
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.8240712483723959
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.802734375
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.07649739583333337
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.956679207938058
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.9686072213309151
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.9454323904854911
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.9531642368861607
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.991164071219308
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.9855575561523438
Final sparsity level of 0.738: 0.7380000475720214
Transfer numbers...
initialize by one_shot_gm
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.7190435224559832
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.945864745460441
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 1.0
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.41106160481770837
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.4187537299262153
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8282962375217013
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8519727918836806
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6791983710394965
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7908155653211806
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5331370035807292
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5526309543185763
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7946641710069444
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8585154215494791
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6440035502115886
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7692057291666666
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5680135091145833
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5872175428602431
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7581329345703125
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8340437147352431
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6372210184733074
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7546187506781684
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.543426513671875
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5682254367404513
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7396799723307292
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8014780680338541
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6505889892578125
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7644471062554253
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5336337619357638
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5587531195746528
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.74310302734375
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8045247395833334
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6793458726671007
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7895486619737413
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5618201361762153
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5853627522786458
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7082587348090278
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.78515625
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.698813968234592
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8124826219346788
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5632849799262153
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5826076931423612
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7142011854383681
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7825419108072916
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.679644266764323
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7997817993164062
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5324198404947917
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.553863525390625
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7932518853081597
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8512912326388888
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6739633348253038
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7906061808268229
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5564303927951388
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5758107503255208
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7695346408420138
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8450690375434028
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6820661756727431
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7840465969509549
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5648091634114583
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5809478759765625
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7969852023654513
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8603803846571181
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7241859436035156
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7859552171495225
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5558336046006944
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5778130425347222
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7932552761501737
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8536580403645834
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7619862026638455
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8080592685275607
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5461917453342013
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.556671142578125
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7383507622612847
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8292219373914931
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7402364942762587
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8720097011990018
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.9883405897352431
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.8034197489420573
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.9389801025390625
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.7774143218994141
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.9161624908447266
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.8411073684692383
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.8719635009765625
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.8361563682556152
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.8515176773071289
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.6175460815429688
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.6727714538574219
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.5845870971679688
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.6600437164306641
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.521484375
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.7844467163085938
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.640625
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.6158498128255208
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.388671875
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.3095703125
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.9213180541992188
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.957413809640067
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.9114565168108258
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.9497495378766742
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.9838943481445312
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.9852632795061383
Final sparsity level of 0.738: 0.7380000129590838
Transfer numbers...
initialize by one_shot_gm
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.6480042752578004
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.9490388902399481
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 1.0
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.44913736979166663
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.45750935872395837
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8658328586154513
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8846571180555556
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.724718729654948
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.829369862874349
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5790574815538194
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.598480224609375
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8356526692708334
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8889380560980903
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.68980958726671
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8095275031195747
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6152292887369792
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6339789496527778
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8010609944661459
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8673536512586806
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6829477945963542
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7950965033637153
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5900031195746528
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6149766710069444
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7847951253255209
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8367496066623263
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6962827046712239
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8042954338921441
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5804375542534722
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.605224609375
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.78631591796875
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8389892578125
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7244843377007378
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.827484130859375
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6088731553819444
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6326090494791667
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7527245415581597
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8215993245442709
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7444288465711806
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8487921820746528
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6106448703342013
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6302066379123263
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7601403130425347
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8216824001736112
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.725569831000434
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8364817301432291
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5786421034071181
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.599884033203125
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8323228624131944
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8825480143229166
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7203403049045138
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8287374708387587
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6038733588324653
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.623931884765625
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8133053249782987
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8802846272786459
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7291679382324219
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8229870266384549
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.613006591796875
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6291215684678819
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.838165283203125
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8952484130859375
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7712364196777344
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8266919453938802
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6041920979817708
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6260206434461806
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8366275363498263
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8903910319010416
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8089786105685763
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8483721415201823
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5940110948350694
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6044158935546875
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7853749593098959
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8693678114149306
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7882084316677518
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9046440124511719
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.9949256049262153
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.7171351114908855
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.8692407608032227
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.711657206217448
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.8572683334350586
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.7858304977416992
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.829340934753418
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.776587963104248
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.8095502853393555
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.6303482055664062
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.6348075866699219
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.5902900695800781
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.6254291534423828
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.603515625
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.7805538177490234
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.671875
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.6212399800618489
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.548828125
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.20052083333333337
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.9116101946149554
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.9386051722935268
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.8820386614118304
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.9098260062081474
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.9646268572126117
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.9567184448242188
Final sparsity level of 0.738: 0.7380000198816714
Transfer numbers...
initialize by one_shot_gm
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.5462382445953116
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.9200130714980544
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 1.0
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.482879638671875
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.4911634657118056
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8938683403862847
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9083964029947916
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7602734035915799
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8584179348415799
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6174502902560763
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6366136338975694
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8670908610026041
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.91119384765625
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7262331644694011
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8405778672960069
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6544443766276042
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6722344292534722
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8352016872829862
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.89227294921875
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7194120619032118
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8272518581814237
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6297760009765625
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6542527940538194
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8218485514322916
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8649020724826388
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7327537536621094
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8358823988172743
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6199561225043403
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6451755099826388
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8215281168619791
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8670450846354166
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7601131863064237
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8574846055772569
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6487155490451388
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6721971299913194
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.790069580078125
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8515302870008681
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7799648708767362
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8771866692437066
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6512247721354167
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6711747911241319
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7982889811197916
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8530409071180556
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7620073954264323
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8654598659939237
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6187455919053819
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6397518581814237
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8629302978515625
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9067043728298612
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7575721740722656
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8589566548665365
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6453874376085069
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6652509901258681
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8479139539930556
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9066297743055556
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7668974134657118
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8541683620876737
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6548122829861112
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6711459689670138
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8708614773220487
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9205593532986112
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8083928426106771
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8592618306477865
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6468149820963542
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6676042344835069
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8704054090711806
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9169837103949653
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8452652825249566
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8801761203342013
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6361287434895833
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6470370822482638
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8240610758463541
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8989410400390625
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8265270657009549
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9290182325575087
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.9979044596354166
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.8046639760335287
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.9291410446166992
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.7726624806722006
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.8870296478271484
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.8427233695983887
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.8487176895141602
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.8361544609069824
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.8423519134521484
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.7504081726074219
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.7724075317382812
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.7122383117675781
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.7430362701416016
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.81640625
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.7734642028808594
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.7734375
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.7504730224609375
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.72265625
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.10416666666666663
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.9403719220842633
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.960235595703125
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.911834716796875
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.938307625906808
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.9814398629324776
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.9739368983677456
Final sparsity level of 0.738: 0.738000040649434
Transfer numbers...
initialize by one_shot_gm
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.6589171661029876
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.9453403655966277
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 1.0
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.44318644205729163
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.451385498046875
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8601735432942709
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8797149658203125
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7175869411892362
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8234507242838541
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.57208251953125
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5913187662760417
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8293677435980903
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8844078911675347
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6825281778971355
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8032285902235243
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6077999538845487
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6269005669487847
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7944352891710069
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8623894585503472
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.67572021484375
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.788797590467665
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5827619764539931
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6073235405815972
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7776980929904513
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8312717013888888
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6891712612575955
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7980020311143663
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5731404622395833
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5977342393663194
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7794901529947916
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8335757785373263
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7174580891927083
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8216463724772135
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6014251708984375
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6250440809461806
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7454647488064237
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8158213297526041
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7372656928168403
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8432019551595052
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6031121148003472
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6227145724826388
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7527635362413194
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8157263861762153
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7182350158691406
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8308003743489584
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5709872775607638
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5923360188802083
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8263465033637153
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8777652316623263
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7130008273654513
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8228251139322916
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5960439046223958
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6160939534505208
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8065728081597222
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8749745686848959
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7215491400824653
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8169288635253906
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6049686008029513
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6210564507378472
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8316921657986112
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8897755940755209
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7636701795789931
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8202896118164062
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5960506863064237
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6178063286675347
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8296322292751737
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.884765625
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8015865749782987
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8421270582411025
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5862528483072917
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5964135064019097
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7778438991970487
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8631066216362847
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7805052863226997
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8997222052680122
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.9941016303168403
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.7630869547526041
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.9092674255371094
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.7243207295735676
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.8640995025634766
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.7973814010620117
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.8233270645141602
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.7888350486755371
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.8151159286499023
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.6220855712890625
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.6374225616455078
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.5803413391113281
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.6178455352783203
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.613525390625
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.7292747497558594
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.634765625
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.6156857808430989
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.5234375
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.22265625
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.909712655203683
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.9345005580357143
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.8709095546177456
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.9107469831194196
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.967620849609375
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.9656273978097099
Final sparsity level of 0.738: 0.7380000198816714
wandb: WARNING W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.
2022-09-10 07:36:06,917 | INFO | main_after.py: 96 : main() ::	 Experiment Name: run_cv_mawps
2022-09-10 07:36:06,917 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:36:06,917 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:36:07,416 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:36:07,416 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 1537
2022-09-10 07:36:07,416 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 384
2022-09-10 07:36:07,416 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: ['0.5', '3.0', '100.0', '12.0', '0.01', '0.1', '1.0', '4.0', '7.0']
2022-09-10 07:36:07,416 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:36:07,416 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:36:07,612 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 2449 / 2449 = 1.0
2022-09-10 07:36:07,655 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 2452 words in input language, 21 words in output
2022-09-10 07:36:07,665 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:36:10,621 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:36:10,621 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:36:14,264 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:36:14,264 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/mawps/dense/models/run_cv_mawps_fold0.
2022-09-10 07:36:17,989 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:37:31,826 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.018229166666666668	
 equation acc epoch: 0.0	
 max val acc: 0.018229166666666668	
 equation acc: 0.0	
2022-09-10 07:37:31,827 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 1m 13s
2022-09-10 07:37:31,829 | INFO | main_after.py: 96 : main() ::	 Experiment Name: run_cv_mawps
2022-09-10 07:37:31,829 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:37:31,829 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:37:32,258 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:37:32,258 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 1537
2022-09-10 07:37:32,258 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 384
2022-09-10 07:37:32,258 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: ['0.01', '12.0', '1.0', '100.0', '0.1', '0.5', '3.0', '4.0', '7.0']
2022-09-10 07:37:32,259 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:37:32,259 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:37:32,451 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 2449 / 2449 = 1.0
2022-09-10 07:37:32,493 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 2452 words in input language, 21 words in output
2022-09-10 07:37:32,505 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:37:35,429 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:37:35,429 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:37:35,506 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:37:35,506 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/mawps/dense/models/run_cv_mawps_fold1.
2022-09-10 07:37:42,215 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:38:05,222 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.018229166666666668	
 equation acc epoch: 0.005208333333333333	
 max val acc: 0.018229166666666668	
 equation acc: 0.005208333333333333	
2022-09-10 07:38:05,222 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 23s
2022-09-10 07:38:05,225 | INFO | main_after.py: 96 : main() ::	 Experiment Name: run_cv_mawps
2022-09-10 07:38:05,225 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:38:05,225 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:38:05,655 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:38:05,655 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 1537
2022-09-10 07:38:05,655 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 384
2022-09-10 07:38:05,655 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: ['0.01', '12.0', '1.0', '100.0', '0.1', '0.5', '3.0', '4.0', '7.0']
2022-09-10 07:38:05,655 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:38:05,655 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:38:05,850 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 2449 / 2449 = 1.0
2022-09-10 07:38:05,892 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 2452 words in input language, 21 words in output
2022-09-10 07:38:05,903 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:38:08,769 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:38:08,769 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:38:08,846 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:38:08,846 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/mawps/dense/models/run_cv_mawps_fold2.
2022-09-10 07:38:15,397 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:38:35,627 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.0	
 equation acc epoch: 0.0	
 max val acc: 0.0	
 equation acc: 0.0	
2022-09-10 07:38:35,627 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 20s
2022-09-10 07:38:35,630 | INFO | main_after.py: 96 : main() ::	 Experiment Name: run_cv_mawps
2022-09-10 07:38:35,630 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:38:35,630 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:38:36,061 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:38:36,061 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 1537
2022-09-10 07:38:36,061 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 384
2022-09-10 07:38:36,061 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: ['0.01', '12.0', '1.0', '100.0', '0.1', '0.5', '3.0', '4.0', '7.0']
2022-09-10 07:38:36,061 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:38:36,061 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:38:36,257 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 2449 / 2449 = 1.0
2022-09-10 07:38:36,300 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 2452 words in input language, 21 words in output
2022-09-10 07:38:36,312 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:38:39,120 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:38:39,120 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:38:39,197 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:38:39,197 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/mawps/dense/models/run_cv_mawps_fold3.
2022-09-10 07:38:45,859 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:39:24,939 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.0	
 equation acc epoch: 0.0	
 max val acc: 0.0	
 equation acc: 0.0	
2022-09-10 07:39:24,939 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 39s
2022-09-10 07:39:24,942 | INFO | main_after.py: 96 : main() ::	 Experiment Name: run_cv_mawps
2022-09-10 07:39:24,942 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:39:24,942 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:39:25,371 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:39:25,371 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 1537
2022-09-10 07:39:25,371 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 384
2022-09-10 07:39:25,371 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: ['0.01', '12.0', '1.0', '100.0', '0.1', '0.5', '3.0', '4.0', '7.0']
2022-09-10 07:39:25,371 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:39:25,371 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:39:25,569 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 2449 / 2449 = 1.0
2022-09-10 07:39:25,610 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 2452 words in input language, 21 words in output
2022-09-10 07:39:25,622 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:39:28,507 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:39:28,507 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:39:28,585 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:39:28,586 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/mawps/dense/models/run_cv_mawps_fold4.
2022-09-10 07:39:35,106 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:40:07,880 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.005208333333333333	
 equation acc epoch: 0.0	
 max val acc: 0.005208333333333333	
 equation acc: 0.0	
2022-09-10 07:40:07,880 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 32s
Traceback (most recent call last):
  File "/home/sliu/miniconda3/envs/prune_cry/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/sliu/miniconda3/envs/prune_cry/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/gpfs/home6/sliu/Projects/SVAMP/code/gts/src/main_after.py", line 276, in <module>
    main()
  File "/gpfs/home6/sliu/Projects/SVAMP/code/gts/src/main_after.py", line 266, in main
    folds_scores.append(float(best_acc[w][0])/best_acc[w][1])
ZeroDivisionError: float division by zero
Transfer numbers...
initialize by one_shot_gm
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.5656042765012103
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.9379965142671854
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 1.0
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5648040771484375
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5736270480685763
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9398854573567709
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9475487603081597
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8335227966308594
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9104792277018229
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.705078125
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7236616346571181
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9206051296657987
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9475419786241319
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8022893269856771
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8977055019802518
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7434132893880208
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7592553032769097
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8952111138237847
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9344363742404513
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7958522372775607
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8865335252549913
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7183753119574653
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7415890163845487
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8867696126302084
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9141286214192709
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8079427083333334
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8931202358669705
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7075025770399306
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7325880262586806
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8827870686848959
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9144727918836806
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8316904703776041
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.910087161593967
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7356160481770833
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7572801378038194
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8569776746961806
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9023691813151041
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8497729831271701
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9254180060492622
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7386898464626737
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7565697564019097
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8652275933159722
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9068332248263888
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8342704772949219
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9154802958170573
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7072482638888888
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7274610731336806
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9139404296875
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9457482231987847
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8311470879448785
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9108615451388888
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7365230984157987
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7544453938802084
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9068874782986112
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9486236572265625
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.840683831108941
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9079102410210503
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7455122205946181
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7594638400607638
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9238060845269097
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9590894911024306
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8775706821017795
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9135314093695747
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7383999294704862
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7578192816840278
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9251166449652778
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9574873182508681
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.9091317918565538
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9308107164171007
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7275916205512153
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7376166449652778
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8903520372178819
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9464653862847222
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8947363959418403
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.96343019273546
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.9997728135850694
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.8869806925455729
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.9672565460205078
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.8607769012451172
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.9419536590576172
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.9181027412414551
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.9346466064453125
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.9210572242736816
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.9426946640014648
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.8966102600097656
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.8915309906005859
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.8522415161132812
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.8650035858154297
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.938720703125
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.8670940399169922
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.875
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.8812573750813802
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.869140625
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.08854166666666663
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.9767586844308036
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.9839292253766742
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.9720731462751117
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.977522713797433
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.9968795776367188
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.9951269967215401
Final sparsity level of 0.791: 0.7910261196426926
Transfer numbers...
initialize by one_shot_gm
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.7451637829918101
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.9587513172827496
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 1.0
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.4763115776909722
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.4842003716362847
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8875342475043403
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9032304551866319
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7531522115071615
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.852486080593533
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6098056369357638
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.62884521484375
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8601413302951388
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9064483642578125
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7185753716362847
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8339932759602865
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.64605712890625
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6642235649956597
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8275672064887153
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8867136637369791
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.711700439453125
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8202641805013021
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6212632921006944
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.645782470703125
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8131578233506944
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8583509657118056
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7251735263400607
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8288485209147135
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6110941569010417
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6363542344835069
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8135189480251737
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8602922227647569
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7525948418511285
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8507715861002604
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.639739990234375
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6634063720703125
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7811940511067709
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8445722791883681
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7724092271592882
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8707322014702691
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6420372856987847
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6620466444227431
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.78912353515625
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8455674913194444
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7543652852376302
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8588676452636719
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6100701226128472
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6308220757378472
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8559197319878472
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9011620415581597
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7496609157986112
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8521016438802084
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6360422770182292
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6560855441623263
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8402201334635416
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.90087890625
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7589005364312066
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8471900092230903
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.645721435546875
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6619737413194444
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8636474609375
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9149254692925347
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8006935119628906
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8518795437282987
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6370595296223958
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6582522922092013
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8628217909071181
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9111904568142362
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8376346164279513
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8729994032118056
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6271006266276042
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6374359130859375
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8154076470269097
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8925866021050347
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8185272216796875
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9234877692328559
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.9973161485460069
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.8484859466552734
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.956878662109375
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.8256104787190756
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.9394092559814453
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.881617546081543
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.9163179397583008
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.8772287368774414
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.8983545303344727
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.69903564453125
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.7489452362060547
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.6666679382324219
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.7411022186279297
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.593017578125
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.8258724212646484
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.66796875
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.683013916015625
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.4296875
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.34049479166666663
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.9502639770507812
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.9769875662667411
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.9476089477539062
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.9747717721121651
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.9930103846958706
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.9945340837751117
Final sparsity level of 0.791: 0.7910000076425366
Transfer numbers...
initialize by one_shot_gm
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.6965635258131901
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.9615201442931258
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 1.0
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5117407904730903
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5198092990451388
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9107089572482638
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9232398139105903
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7859043545193143
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8778788248697916
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6485273573133681
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6678398980034722
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8870815700954862
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9255218505859375
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7527054680718316
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.86187744140625
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6868065728081597
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7034488254123263
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8574998643663194
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9086066351996528
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7462056477864583
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.849379645453559
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6615278455946181
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.685760498046875
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8457624647352431
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8832855224609375
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7591862148708768
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8573430379231771
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6513315836588542
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6764916314019097
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8435143364800347
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8842824300130209
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7853033277723525
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8771989610460069
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6796993679470487
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7029673258463542
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8139088948567709
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8700646294487847
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8044094509548612
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8953399658203125
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6825239393446181
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7020280626085069
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8220299614800347
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8722788492838541
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7873700459798177
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8843583001030816
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6507924397786458
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6717970106336806
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8812323676215278
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9212358262803819
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7835879855685763
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8785451253255209
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6785447862413194
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6979488796657987
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8695916069878472
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9224175347222222
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7930514017740885
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8744739956325955
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6882205539279513
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7036353217230903
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8900570339626737
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9353264702690972
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8331625196668837
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8798582288953993
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6800960964626737
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.701141357421875
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8902757432725694
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9322543674045138
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8685828314887153
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8993793063693576
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6695777045355903
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6799146864149306
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8477834065755209
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9169057210286459
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.851432376437717
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9422802395290799
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.9988776312934028
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.7715593973795573
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.9021387100219727
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.7664953867594401
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.891566276550293
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.834251880645752
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.8795366287231445
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.8260221481323242
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.8627223968505859
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.7025871276855469
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.7062244415283203
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.6628913879394531
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.6978511810302734
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.68310546875
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.8267631530761719
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.708984375
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.6871350606282551
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.599609375
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.2197265625
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.9422040666852679
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.9622682843889508
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.9229736328125
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.9452732631138393
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.98101806640625
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.9776872907366071
Final sparsity level of 0.791: 0.7910000145651241
Transfer numbers...
initialize by one_shot_gm
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.5918011880781856
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.9464929880025941
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 1.0
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5569881863064237
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5655924479166667
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9360249837239584
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9444936116536459
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8269454108344184
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9061414930555556
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6968621148003472
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7157609727647569
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9163106282552084
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9446597629123263
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7953580220540365
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8930015563964844
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7354651557074653
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7514512803819444
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8900316026475694
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9309488932291666
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7888844807942709
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8815409342447916
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7103763156467013
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7337714301215278
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8812204996744791
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.909942626953125
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.801163567437066
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8882967631022135
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6997307671440972
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7247517903645833
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8775516086154513
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9102681477864584
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8253690931532118
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9058138529459635
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7277035183376737
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7495286729600694
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8511827256944444
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8980289035373263
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8436249627007378
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9214146931966146
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7308671739366319
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7490336100260417
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8592258029513888
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9023454454210069
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8278774685329862
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.911391364203559
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6992984347873263
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7195078531901042
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9096171061197916
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.942535400390625
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8247104220920138
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9067183600531684
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7282206217447917
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7466379801432292
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9020250108506944
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9452921549479166
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.834206051296658
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.90345213148329
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7374199761284722
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7517615424262153
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9193505181206597
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9560750325520834
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8716074625651041
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9090567694769965
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7303110758463542
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7497812906901042
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9205695258246528
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9543660481770834
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.9037666320800781
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9267302619086372
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7193959554036458
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7293955485026042
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8845130072699653
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9426625569661459
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8889088100857205
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9608133104112413
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.9997016059027778
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.8590825398763021
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.9552707672119141
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.8311812082926432
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.9230613708496094
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.8909850120544434
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.9047126770019531
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.8859419822692871
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.8983535766601562
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.8241653442382812
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.8426856994628906
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.7898712158203125
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.8169002532958984
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.87841796875
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.8349876403808594
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.810546875
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.8175710042317709
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.7890625
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.12272135416666663
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.9653047834123883
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.978851318359375
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.9490269252232143
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.9677723475864956
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.9920719691685268
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.9892185756138393
Final sparsity level of 0.791: 0.791038407235555
Transfer numbers...
initialize by one_shot_gm
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.7023466771942041
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.9592351653696498
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 1.0
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5070919460720487
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5151468912760417
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9076436360677084
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9205000135633681
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7813758850097656
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8745375739203559
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6434750027126737
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6626112196180556
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8837432861328125
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9229583740234375
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7479684617784288
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8581954108344184
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6812862820095487
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6981743706597222
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8535953097873263
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9057430691189237
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7414466010199653
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8456077575683594
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6562398274739583
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6806165907118056
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8415408664279513
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8801930745442709
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7545399136013455
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8536881340874566
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6462995741102431
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6713138156467013
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8396352132161459
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8810916476779513
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7808113098144531
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.873787350124783
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6741417778862847
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6976437038845487
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8095584445529513
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8666568332248263
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.799971686469184
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8922407362196181
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6771053738064237
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6969587537977431
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8177541097005209
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8689405653211806
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7827165391710069
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8811310662163628
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6450432671440972
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6661834716796875
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8779262966579862
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9187486436631944
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7788213094075521
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8751712375217013
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6727125379774306
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6922675238715278
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8656870524088541
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9196641710069444
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.788407219780816
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8709826999240451
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6823493109809028
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6979014078776042
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8866153293185763
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9326375325520834
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8286361694335938
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8763304816351997
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6740637885199653
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6951972113715278
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8867763943142362
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9295399983723959
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.864264170328776
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8960622151692709
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6639183892144097
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6742587619357638
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8434261745876737
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9136912027994791
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8469501071506076
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9400367736816406
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.9987352159288194
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.8123207092285156
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.9346809387207031
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.7767779032389323
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.8961496353149414
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.8438706398010254
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.8749141693115234
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.8363604545593262
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.867283821105957
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.6963348388671875
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.7100582122802734
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.6552391052246094
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.6920280456542969
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.684814453125
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.7825565338134766
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.685546875
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.682769775390625
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.58203125
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.24609375
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.9405081612723214
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.9594704764229911
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.9151524135044643
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.9462825230189732
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.9831662859235492
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.9832796369280133
Final sparsity level of 0.791: 0.7910000076425366
wandb: WARNING W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.
2022-09-10 07:40:10,996 | INFO | main_after.py: 96 : main() ::	 Experiment Name: run_cv_mawps
2022-09-10 07:40:10,997 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:40:10,997 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:40:11,504 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:40:11,504 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 1537
2022-09-10 07:40:11,504 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 384
2022-09-10 07:40:11,504 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: ['0.5', '3.0', '100.0', '12.0', '0.01', '0.1', '1.0', '4.0', '7.0']
2022-09-10 07:40:11,504 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:40:11,504 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:40:11,697 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 2449 / 2449 = 1.0
2022-09-10 07:40:11,742 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 2452 words in input language, 21 words in output
2022-09-10 07:40:11,753 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:40:15,060 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:40:15,060 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:40:18,566 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:40:18,567 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/mawps/dense/models/run_cv_mawps_fold0.
2022-09-10 07:40:24,323 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:40:58,724 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.013020833333333334	
 equation acc epoch: 0.0	
 max val acc: 0.013020833333333334	
 equation acc: 0.0	
2022-09-10 07:40:58,724 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 34s
2022-09-10 07:40:58,727 | INFO | main_after.py: 96 : main() ::	 Experiment Name: run_cv_mawps
2022-09-10 07:40:58,727 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:40:58,727 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:40:59,156 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:40:59,156 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 1537
2022-09-10 07:40:59,156 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 384
2022-09-10 07:40:59,156 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: ['0.01', '12.0', '1.0', '100.0', '0.1', '0.5', '3.0', '4.0', '7.0']
2022-09-10 07:40:59,156 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:40:59,156 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:40:59,347 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 2449 / 2449 = 1.0
2022-09-10 07:40:59,389 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 2452 words in input language, 21 words in output
2022-09-10 07:40:59,400 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:41:02,311 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:41:02,311 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:41:02,388 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:41:02,389 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/mawps/dense/models/run_cv_mawps_fold1.
2022-09-10 07:41:08,933 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:41:27,978 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.03125	
 equation acc epoch: 0.018229166666666668	
 max val acc: 0.03125	
 equation acc: 0.018229166666666668	
2022-09-10 07:41:27,978 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 19s
2022-09-10 07:41:27,982 | INFO | main_after.py: 96 : main() ::	 Experiment Name: run_cv_mawps
2022-09-10 07:41:27,982 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:41:27,982 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:41:28,414 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:41:28,414 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 1537
2022-09-10 07:41:28,414 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 384
2022-09-10 07:41:28,414 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: ['0.01', '12.0', '1.0', '100.0', '0.1', '0.5', '3.0', '4.0', '7.0']
2022-09-10 07:41:28,414 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:41:28,414 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:41:28,608 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 2449 / 2449 = 1.0
2022-09-10 07:41:28,651 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 2452 words in input language, 21 words in output
2022-09-10 07:41:28,662 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:41:31,580 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:41:31,580 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:41:31,658 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:41:31,658 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/mawps/dense/models/run_cv_mawps_fold2.
2022-09-10 07:41:38,377 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:42:03,540 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.0	
 equation acc epoch: 0.0	
 max val acc: 0.0	
 equation acc: 0.0	
2022-09-10 07:42:03,540 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 25s
2022-09-10 07:42:03,543 | INFO | main_after.py: 96 : main() ::	 Experiment Name: run_cv_mawps
2022-09-10 07:42:03,543 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:42:03,543 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:42:03,974 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:42:03,974 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 1537
2022-09-10 07:42:03,974 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 384
2022-09-10 07:42:03,974 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: ['0.01', '12.0', '1.0', '100.0', '0.1', '0.5', '3.0', '4.0', '7.0']
2022-09-10 07:42:03,974 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:42:03,975 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:42:04,168 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 2449 / 2449 = 1.0
2022-09-10 07:42:04,209 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 2452 words in input language, 21 words in output
2022-09-10 07:42:04,220 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:42:07,066 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:42:07,066 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:42:07,143 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:42:07,143 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/mawps/dense/models/run_cv_mawps_fold3.
2022-09-10 07:42:13,981 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:42:23,920 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.0	
 equation acc epoch: 0.0	
 max val acc: 0.0	
 equation acc: 0.0	
2022-09-10 07:42:23,920 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 9s
2022-09-10 07:42:23,923 | INFO | main_after.py: 96 : main() ::	 Experiment Name: run_cv_mawps
2022-09-10 07:42:23,923 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:42:23,923 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:42:24,353 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:42:24,353 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 1537
2022-09-10 07:42:24,354 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 384
2022-09-10 07:42:24,354 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: ['0.01', '12.0', '1.0', '100.0', '0.1', '0.5', '3.0', '4.0', '7.0']
2022-09-10 07:42:24,354 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:42:24,354 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:42:24,548 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 2449 / 2449 = 1.0
2022-09-10 07:42:24,591 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 2452 words in input language, 21 words in output
2022-09-10 07:42:24,602 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:42:27,474 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:42:27,474 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:42:27,552 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:42:27,552 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/mawps/dense/models/run_cv_mawps_fold4.
2022-09-10 07:42:34,394 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:43:11,464 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.010416666666666666	
 equation acc epoch: 0.0	
 max val acc: 0.010416666666666666	
 equation acc: 0.0	
2022-09-10 07:43:11,464 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 37s
Traceback (most recent call last):
  File "/home/sliu/miniconda3/envs/prune_cry/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/sliu/miniconda3/envs/prune_cry/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/gpfs/home6/sliu/Projects/SVAMP/code/gts/src/main_after.py", line 276, in <module>
    main()
  File "/gpfs/home6/sliu/Projects/SVAMP/code/gts/src/main_after.py", line 266, in main
    folds_scores.append(float(best_acc[w][0])/best_acc[w][1])
ZeroDivisionError: float division by zero
Transfer numbers...
initialize by one_shot_gm
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.6123238761646606
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.9590654385538262
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 1.0
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6322648790147569
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6406097412109375
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9665968153211806
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9690823025173612
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8848749796549479
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9418411254882812
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7722913953993056
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7884046766493056
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9523468017578125
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9672580295138888
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8570429484049479
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9328134324815538
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8082953559027778
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8220248752170138
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9323374430338541
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9585316975911459
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8511729770236545
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9239773220486112
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7849748399522569
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8056504991319444
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9282735188802084
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9443308512369791
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8614675733778212
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9289576212565104
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7746463351779513
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7969631618923612
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9225226508246528
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9447496202256944
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8813790215386285
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.942374759250217
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8005625406901041
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8188713921440972
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9028405083550347
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9359486897786459
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8969985114203559
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9537010192871094
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.803497314453125
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8192087809244791
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9099595811631944
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9411977132161459
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8842281765407987
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9456367492675781
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7743309868706597
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7923363579644097
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9454922146267362
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9682668050130209
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.88200929429796
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9421183268229166
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8031972249348959
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8189002143012153
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9428151448567709
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9719390869140625
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8911268446180556
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9407708909776475
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8126983642578125
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8241475423177084
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9549492730034722
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9790089925130209
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.9225489298502604
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9460868835449219
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8054843478732638
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8230183919270834
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9568905300564237
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9783749050564237
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.9474860297309028
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9597011142306857
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7962086995442709
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8045586480034722
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9327002631293403
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9715287950303819
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.937294430202908
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9806459214952257
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.9999847412109375
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.9265689849853516
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.9817953109741211
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.9057337443033854
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.9647693634033203
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.9494848251342773
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.9648666381835938
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.951383113861084
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.9699306488037109
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.9410171508789062
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.9349498748779297
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.9059333801269531
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.9157962799072266
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.9677734375
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.9121799468994141
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.931640625
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.9228858947753906
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.9140625
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.10384114583333337
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.9879673549107143
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.9922757829938617
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.9865482875279018
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.9900076729910714
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.998978751046317
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.9985220772879464
Final sparsity level of 0.8325: 0.832500019937052
Transfer numbers...
initialize by one_shot_gm
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.7689621050101131
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.9684890766861219
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 1.0
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5406460232204862
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5490332709418403
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9266679551866319
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9366590711805556
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8119392395019531
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8960711161295573
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.67950439453125
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6985117594401042
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.905426025390625
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9377560085720487
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7794337802463107
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8818711174858941
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7178412543402778
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7337053087022569
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8779076470269097
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9227752685546875
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7729988098144531
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8699527316623263
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6924099392361112
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7162000868055556
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8678165011935763
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8998243543836806
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7856080796983507
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8770412868923612
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.681640625
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7067769368489583
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8647206624348959
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9004194471571181
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8105744255913628
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8951911926269531
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7096320258246528
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7322794596354167
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8366682264539931
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8872748480902778
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8290820651584201
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9118927849663628
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7124684651692708
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7317928738064237
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8447655571831597
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8912031385633681
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8128768073187934
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9016109042697482
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6813184950086806
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7017262776692708
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8988749186197916
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9344872368706597
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8094668918185763
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.896460215250651
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7099355061848958
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7285987006293403
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8897535536024306
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9367624918619791
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8189820183648003
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8929125467936198
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7195519341362847
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7339867485894097
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9083472357855903
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9485151502821181
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8573875427246094
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.898398929172092
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7114342583550347
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7317148844401042
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9092458089192709
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9464908175998263
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8909912109375
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9168239169650607
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7008819580078125
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7111765543619792
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8706393771701388
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9333173963758681
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8753551907009549
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9542066786024306
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.9994455973307291
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.8827120463053385
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.9693880081176758
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.863274892171224
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.9556951522827148
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.911186695098877
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.9445676803588867
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.9076113700866699
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.9300632476806641
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.7651901245117188
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.8093585968017578
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.7354736328125
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.8043403625488281
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.659912109375
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.8584938049316406
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.689453125
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.7377535502115886
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.48828125
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.361328125
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.9681418282645089
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.9874616350446429
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.9691728864397321
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.9877308436802456
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.9969656808035714
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.9979498726981026
Final sparsity level of 0.8325: 0.832534757481268
Transfer numbers...
initialize by one_shot_gm
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.7257145980470174
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.9705384646562906
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 1.0
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5753801133897569
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5842030843098958
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9446512858072916
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9514058430989584
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8423122829861112
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9160647922092013
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7159644232855903
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7343020968967013
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9263203938802084
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9511040581597222
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8115454779730903
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9038895501030816
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7539723714192709
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7697991265190972
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9016571044921875
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9386461046006944
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.805267333984375
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8930045233832465
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7291819254557292
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7523990207248263
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8939853244357638
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9193149142795138
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8170072767469618
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8993983798556857
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7184668646918403
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7430131700303819
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8894534640842013
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9195946587456597
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8401315477159288
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9157994588216146
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7460666232638888
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7671610514322916
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8646714952256944
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9081336127387153
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8578970167371962
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.930440690782335
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7494286431206597
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7668575710720487
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.872772216796875
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9127841525607638
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8428005642361112
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.920922597249349
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7182193332248263
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7382558186848958
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9193335639105903
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9497612847222222
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.839927249484592
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.91649415757921
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7475975884331597
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7654571533203125
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9132910834418403
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9529452853732638
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8495479159884982
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.913895501030816
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7571122911241319
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7705451117621528
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9293636745876737
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9627872043185763
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.885721418592665
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9194378323025174
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7496829562717013
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7693617078993056
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9308878580729166
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9614461263020834
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.9161830478244357
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9362733629014757
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7394816080729167
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7489590115017362
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8976508246527778
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.951141357421875
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.9025666978624132
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9667345682779948
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.9998389350043403
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.8207518259684244
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.9290275573730469
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.8159618377685547
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.919917106628418
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.8754019737243652
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.918766975402832
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.8685402870178223
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.9049749374389648
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.7705535888671875
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.7715625762939453
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.7318763732910156
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.7654342651367188
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.752685546875
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.8664588928222656
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.736328125
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.7480430603027344
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.662109375
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.24088541666666663
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.9638192313058036
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.9781319754464286
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.952254159109933
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.9692927769252232
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.990586417061942
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.9894300188337054
Final sparsity level of 0.8325: 0.8325000268596396
Transfer numbers...
initialize by one_shot_gm
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.6344115510709905
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.9634479369325551
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 1.0
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6233740912543403
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6318257649739583
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9637841118706597
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9667341444227431
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8788011338975694
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9383332994249132
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7636396620008681
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7801428900824653
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9487660725911459
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9650726318359375
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8504562377929688
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9287308586968316
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8002387152777778
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8144039577907987
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9280548095703125
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9558732774522569
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.844400617811415
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9197129143608941
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7764858669704862
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7977006700303819
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9235009087456597
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9409501817491319
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.85504150390625
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9248771667480469
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7663353814019097
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7889065212673612
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9178042941623263
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9412180582682291
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8755137125651041
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9387279086642795
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7924228244357638
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8115132649739584
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.89752197265625
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9320899115668403
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.891493903266059
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9505505032009549
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7955678304036459
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8115776909722222
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9048411051432291
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9373626708984375
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8783158196343316
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9422717624240451
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7660335964626737
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7844424777560763
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9419826931423612
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9658152262369791
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8760486178927951
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9386656019422743
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7949269612630209
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.811065673828125
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9387546115451388
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.969573974609375
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8853060404459635
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9371066623263888
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8043670654296875
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8161824544270834
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9514736599392362
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9768727620442709
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.9174584282769097
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9425328572591146
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7970191107855903
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8149583604600694
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9534708658854166
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9762895372178819
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.94328859117296
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.956648932562934
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7878604465060763
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7963104248046875
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9277903238932291
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9689534505208334
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.9326612684461806
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9788894653320312
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.9999745686848959
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.9017569224039713
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.9725732803344727
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.8785692850748698
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.948969841003418
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.9271535873413086
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.9434690475463867
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.9232354164123535
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.9376811981201172
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.8826942443847656
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.8962688446044922
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.8533401489257812
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.8757514953613281
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.9228515625
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.8829231262207031
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.87109375
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.8713595072428385
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.861328125
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.13671875
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.9805428641183036
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.9890975952148438
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.9718126569475446
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.9844458443777901
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.9967771257672992
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.9959684099469867
Final sparsity level of 0.8325: 0.8325000130144645
Transfer numbers...
initialize by one_shot_gm
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.7305674197586127
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.9689982571335928
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 1.0
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5711313883463542
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5801764594184028
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9426099989149306
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9497341579861112
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8385789659288194
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9137458801269531
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7115546332465278
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7299329969618056
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9237653944227431
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9495001898871528
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8075739542643229
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9012396070692275
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7496117485894097
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7653316921657987
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8987884521484375
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9367896185980903
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8012877570258247
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8902638753255209
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.724609375
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7478688557942708
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8907521565755209
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9170159233940972
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8131158616807725
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.89673826429579
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7137807210286458
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7386830647786458
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8866611056857638
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9173651801215278
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8364910549587674
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9133368598090278
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7415737575954862
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7629123263888888
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8612857394748263
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9055260552300347
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8543947007921007
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.92827394273546
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7446492513020833
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7623799641927084
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8693559434678819
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9101274278428819
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8390799628363715
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9185837639702691
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.71343994140625
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7334696451822917
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9169498019748263
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9479946560329862
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8361388312445747
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9140697055392795
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7427351209852431
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7606489393446181
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9103630913628472
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9509616427951388
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8456560770670573
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9112989637586806
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.752044677734375
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7658047146267362
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9268171522352431
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9611714680989584
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8821389940049913
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9168306986490885
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7447255452473958
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7643839518229166
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9282633463541666
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9597371419270834
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.9131173027886285
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9338582356770834
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7346225314670138
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7439422607421875
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8942837185329862
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9490695529513888
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8992818196614584
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9653150770399306
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.9998050265842013
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.8544133504231771
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.9538078308105469
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.8226655324300131
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.9221706390380859
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.8823027610778809
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.9141626358032227
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.8759231567382812
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.9080772399902344
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.7628097534179688
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.7748394012451172
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.7237777709960938
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.7593803405761719
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.755126953125
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.8281688690185547
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.734375
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.7428868611653645
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.6484375
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.271484375
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.9615914481026786
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.9759597778320312
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.9464002336774554
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.9693875994001117
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.9915989467075893
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.9923913138253349
Final sparsity level of 0.8325: 0.8325000476274023
wandb: WARNING W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.
2022-09-10 07:43:14,589 | INFO | main_after.py: 96 : main() ::	 Experiment Name: run_cv_mawps
2022-09-10 07:43:14,590 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:43:14,590 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:43:15,098 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:43:15,098 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 1537
2022-09-10 07:43:15,098 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 384
2022-09-10 07:43:15,098 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: ['0.5', '3.0', '100.0', '12.0', '0.01', '0.1', '1.0', '4.0', '7.0']
2022-09-10 07:43:15,098 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:43:15,098 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:43:15,291 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 2449 / 2449 = 1.0
2022-09-10 07:43:15,336 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 2452 words in input language, 21 words in output
2022-09-10 07:43:15,347 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:43:18,577 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:43:18,577 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:43:22,049 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:43:22,049 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/mawps/dense/models/run_cv_mawps_fold0.
2022-09-10 07:43:28,805 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:43:35,301 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.020833333333333332	
 equation acc epoch: 0.0	
 max val acc: 0.020833333333333332	
 equation acc: 0.0	
2022-09-10 07:43:35,302 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 6s
2022-09-10 07:43:35,304 | INFO | main_after.py: 96 : main() ::	 Experiment Name: run_cv_mawps
2022-09-10 07:43:35,304 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:43:35,304 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:43:35,735 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:43:35,735 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 1537
2022-09-10 07:43:35,735 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 384
2022-09-10 07:43:35,735 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: ['0.01', '12.0', '1.0', '100.0', '0.1', '0.5', '3.0', '4.0', '7.0']
2022-09-10 07:43:35,735 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:43:35,735 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:43:35,930 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 2449 / 2449 = 1.0
2022-09-10 07:43:35,972 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 2452 words in input language, 21 words in output
2022-09-10 07:43:35,983 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:43:38,919 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:43:38,919 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:43:38,994 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:43:38,994 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/mawps/dense/models/run_cv_mawps_fold1.
2022-09-10 07:43:42,137 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:43:53,343 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.020833333333333332	
 equation acc epoch: 0.0	
 max val acc: 0.020833333333333332	
 equation acc: 0.0	
2022-09-10 07:43:53,344 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 11s
2022-09-10 07:43:53,347 | INFO | main_after.py: 96 : main() ::	 Experiment Name: run_cv_mawps
2022-09-10 07:43:53,347 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:43:53,347 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:43:53,781 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:43:53,781 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 1537
2022-09-10 07:43:53,781 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 384
2022-09-10 07:43:53,781 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: ['0.01', '12.0', '1.0', '100.0', '0.1', '0.5', '3.0', '4.0', '7.0']
2022-09-10 07:43:53,781 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:43:53,781 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:43:53,978 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 2449 / 2449 = 1.0
2022-09-10 07:43:54,019 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 2452 words in input language, 21 words in output
2022-09-10 07:43:54,030 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:43:56,901 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:43:56,901 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:43:56,976 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:43:56,976 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/mawps/dense/models/run_cv_mawps_fold2.
2022-09-10 07:43:59,977 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:44:29,781 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.0	
 equation acc epoch: 0.0	
 max val acc: 0.0	
 equation acc: 0.0	
2022-09-10 07:44:29,782 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 29s
2022-09-10 07:44:29,785 | INFO | main_after.py: 96 : main() ::	 Experiment Name: run_cv_mawps
2022-09-10 07:44:29,785 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:44:29,785 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:44:30,218 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:44:30,219 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 1537
2022-09-10 07:44:30,219 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 384
2022-09-10 07:44:30,219 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: ['0.01', '12.0', '1.0', '100.0', '0.1', '0.5', '3.0', '4.0', '7.0']
2022-09-10 07:44:30,219 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:44:30,219 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:44:30,416 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 2449 / 2449 = 1.0
2022-09-10 07:44:30,548 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 2452 words in input language, 21 words in output
2022-09-10 07:44:30,559 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:44:33,434 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:44:33,434 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:44:33,512 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:44:33,512 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/mawps/dense/models/run_cv_mawps_fold3.
2022-09-10 07:44:36,367 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:44:49,337 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.036458333333333336	
 equation acc epoch: 0.0	
 max val acc: 0.036458333333333336	
 equation acc: 0.0	
2022-09-10 07:44:49,338 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 12s
2022-09-10 07:44:49,341 | INFO | main_after.py: 96 : main() ::	 Experiment Name: run_cv_mawps
2022-09-10 07:44:49,341 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:44:49,341 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:44:49,773 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:44:49,773 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 1537
2022-09-10 07:44:49,773 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 384
2022-09-10 07:44:49,773 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: ['0.01', '12.0', '1.0', '100.0', '0.1', '0.5', '3.0', '4.0', '7.0']
2022-09-10 07:44:49,774 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:44:49,774 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:44:49,971 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 2449 / 2449 = 1.0
2022-09-10 07:44:50,013 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 2452 words in input language, 21 words in output
2022-09-10 07:44:50,025 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:44:52,912 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:44:52,912 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:44:52,988 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:44:52,988 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/mawps/dense/models/run_cv_mawps_fold4.
2022-09-10 07:44:58,350 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:46:26,748 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.013020833333333334	
 equation acc epoch: 0.0	
 max val acc: 0.013020833333333334	
 equation acc: 0.0	
2022-09-10 07:46:26,748 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 1m 28s
Traceback (most recent call last):
  File "/home/sliu/miniconda3/envs/prune_cry/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/sliu/miniconda3/envs/prune_cry/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/gpfs/home6/sliu/Projects/SVAMP/code/gts/src/main_after.py", line 276, in <module>
    main()
  File "/gpfs/home6/sliu/Projects/SVAMP/code/gts/src/main_after.py", line 266, in main
    folds_scores.append(float(best_acc[w][0])/best_acc[w][1])
ZeroDivisionError: float division by zero
Transfer numbers...
initialize by one_shot_gm
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.656983715474651
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.9719140118352788
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 1.0
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6963958740234375
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7043728298611112
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9827711317274306
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9823794894748263
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.9241133795844184
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9629813300238715
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8308444552951388
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8440890842013888
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9733378092447916
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9796091715494791
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.9010480244954427
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9574127197265625
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8634389241536459
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8735673692491319
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9586605495876737
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9745279947916666
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8956693013509115
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.950943840874566
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8433108859592013
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8598734537760416
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9572821723090278
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9656711154513888
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.903794182671441
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9545343187120225
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8331705729166666
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8520931667751737
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9515482584635416
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9661237928602431
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.9196217854817709
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9647288852267795
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8560638427734375
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8706885443793403
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9375729031032987
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9602644178602431
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.9328736199273003
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9726549784342448
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8578694661458334
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8714870876736112
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9433542887369791
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9651251898871528
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.9225362141927084
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9662840101453993
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8331315782335069
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8480411105685763
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9670834011501737
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.982452392578125
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.921301523844401
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9637870788574219
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8606787787543403
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8732164171006944
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9668562147352431
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9857906765407987
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.9290758768717448
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9633449978298612
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8695576985677084
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8784230550130209
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9749348958333334
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9901292588975694
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.9538375006781684
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9679391649034288
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8629947238498263
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8780975341796875
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9769015842013888
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9898512098524306
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.9721828036838107
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9778145684136285
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8556942409939237
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8623335096571181
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9613681369357638
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9855211046006944
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.9658821953667535
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9902017381456163
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 1.0
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.9553540547688802
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.9906797409057617
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.9401410420735677
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.9798583984375
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.9711260795593262
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.9830503463745117
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.972069263458252
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.9859161376953125
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.9693641662597656
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.9645957946777344
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.9457588195800781
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.9523334503173828
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.985595703125
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.9452552795410156
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.958984375
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.9532279968261719
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.94921875
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.1162109375
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.9942561558314732
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.9966746738978794
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.9939782278878349
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.996171133858817
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.999725341796875
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.9996370588030133
Final sparsity level of 0.866: 0.8660000201031942
Transfer numbers...
initialize by one_shot_gm
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.7920553617908419
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.9755998702983139
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 1.0
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5994906955295138
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6079678005642362
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9543084038628472
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9592047797309028
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8606724209255643
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9275889926486545
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7400377061631944
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7570393880208334
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9375745985243056
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9582061767578125
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8309652540418837
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9165615505642362
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7769232855902778
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7921210394965278
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9149831136067709
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.94732666015625
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8247532314724393
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9064873589409722
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7527041965060763
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7750108506944444
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9086422390407987
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9300214979383681
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8359705607096354
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9123030768500434
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7421129014756944
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7658962673611112
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9034491644965278
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9304216172960069
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8579432169596354
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9273991054958768
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7688954671223959
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7888387044270834
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8807610405815972
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.919921875
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8748147752549913
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9407030741373698
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7718387179904513
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7888217502170138
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8882904052734375
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9249216715494791
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8607046339246962
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9318245781792535
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7416347927517362
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7611914740668403
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9305284288194444
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9578755696614584
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8580746120876737
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9278386433919271
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7711368136935763
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7880622016059028
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9261084662543403
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9613969590928819
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8675660027398003
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9257452223036025
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7806226942274306
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7934943305121528
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9403957790798612
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9700571695963541
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.9017350938585069
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9312099880642362
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7728525797526041
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7918955485026041
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9422505696614584
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9691416422526041
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.9301872253417969
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9467909071180556
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.763458251953125
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7724507649739584
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9127197265625
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9604051378038194
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.918044196234809
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9731110466851128
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.99993896484375
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.9106610616048177
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.9783382415771484
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.8945439656575521
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.9682445526123047
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.9344377517700195
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.9641962051391602
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.9314608573913574
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.9529438018798828
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.8201560974121094
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.8581962585449219
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.7946815490722656
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.8562850952148438
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.719482421875
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.8870182037353516
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.7109375
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.7858874003092448
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.546875
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.38606770833333337
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.9798028128487724
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.9932970319475446
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.9824262346540179
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.9944370814732143
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.9987607683454242
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.9992566789899554
Final sparsity level of 0.866: 0.8660212032210302
Transfer numbers...
initialize by one_shot_gm
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.7531165551742431
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.9772186081387808
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 1.0
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6359524197048612
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6444091796875
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9675937228732638
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9700419108072916
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8873320685492622
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9432084825303819
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7757415771484375
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7916904025607638
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9536997477213541
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9680582682291666
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.859718746609158
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9343978034125434
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8115590413411459
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8252495659722222
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9339447021484375
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9594709608289931
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8538661532931857
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9257185194227431
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7884996202256944
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8089955647786459
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9300791422526041
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9456227620442709
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8640225728352865
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9305911593967013
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7780778672960069
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8002827962239584
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9242587619357638
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9460957845052084
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8837640550401475
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9438425699869791
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8039466010199653
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8219163682725694
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9048055013020834
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9372982449001737
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8992818196614584
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9549179077148438
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8068779839409722
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8222198486328125
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9119110107421875
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9425947401258681
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8866403367784288
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9469672309027778
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7779557969835069
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7957729763454862
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9468129475911459
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9691636827256944
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8846278720431857
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9435628255208334
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8068695068359375
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8224521213107638
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9443630642361112
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9727969699435763
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.893652598063151
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.942281511094835
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8163469102647569
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8277418348524306
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9561733669704862
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9797380235460069
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.924697028266059
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9475775824652778
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8093973795572916
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8266821967230903
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9582638210720487
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9791531032986112
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.9492602878146701
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.961012946234809
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8000861273871528
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8081563313802084
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9344601101345487
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9724985758463541
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.9393950568305122
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.981363508436415
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.9999847412109375
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.8624451955159506
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.9497652053833008
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.858489990234375
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.9419965744018555
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.9086952209472656
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.9472017288208008
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.9030060768127441
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.936457633972168
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.8289070129394531
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.8284053802490234
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.7931938171386719
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.8244266510009766
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.810302734375
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.8986873626708984
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.771484375
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.8013496398925781
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.716796875
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.26790364583333337
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.9778660365513393
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.9877003261021206
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.9716077532087054
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.9837297712053571
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.9955237252371651
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.9954071044921875
Final sparsity level of 0.866: 0.8660000131806067
Transfer numbers...
initialize by one_shot_gm
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.6848442577257203
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.9728842412451362
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 1.0
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6783091227213542
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6863488091362847
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9790208604600694
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9792751736111112
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.9140845404730903
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9578505622016059
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8153177897135416
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8291219075520834
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9685092502170138
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9767337375217013
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8897154066297743
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9513498942057291
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8488210042317709
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8601142035590278
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9523095024956597
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9707149929470487
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8843010796440972
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9441990322536893
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8278350830078125
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8457014295789931
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9504191080729166
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9605051676432291
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8929752773708768
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9482430352105035
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8175845675998263
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8376380072699653
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9442918565538194
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9608696831597222
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.9100006951226128
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9592781066894531
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8415391710069444
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8572099473741319
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9287499321831597
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9541236029730903
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.9238963656955295
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9680430094401041
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8436008029513888
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8580135769314237
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9349500868055556
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.95928955078125
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.912872314453125
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9612935384114584
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8177066379123263
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8334011501736112
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9618004692925347
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9790768093532987
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.9114061991373698
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.958534664577908
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8456200493706597
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8590969509548612
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9610426161024306
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9827084011501737
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.9196544223361545
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9578768412272135
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8546956380208334
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8644527859157987
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9702284071180556
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9877115885416666
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.9463513692220052
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9626998901367188
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8479631212022569
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8641866048177084
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9722663031684028
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9873131646050347
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.9664539761013455
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9736277262369791
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.840057373046875
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8470662434895834
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9544660780164931
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9824540879991319
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.9591551886664497
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9881345960828993
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.9999983045789931
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.9310760498046875
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.9829683303833008
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.9121952056884766
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.9657306671142578
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.9507746696472168
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.9660501480102539
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.9477920532226562
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.9617528915405273
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.9219703674316406
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.9320049285888672
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.8987197875976562
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.9165534973144531
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.953857421875
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.9166679382324219
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.900390625
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.9085235595703125
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.91015625
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.15299479166666663
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.9887281145368304
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.994093758719308
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.9840044294084821
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.9923836844308036
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.9986855643136161
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.9984588623046875
Final sparsity level of 0.866: 0.8660825858046417
Transfer numbers...
initialize by one_shot_gm
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.757390206903412
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.976306643158236
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 1.0
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6318918863932292
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6401892768012153
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9661797417534722
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9687398274739584
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8842790391710069
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9415376451280382
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.77178955078125
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7879214816623263
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9519517686631944
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9669901529947916
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8564046223958334
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9324167039659288
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8076595730251737
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8214704725477431
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.93170166015625
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9581502278645834
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.85052490234375
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9235967000325521
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7842746310763888
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8049485948350694
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9276529947916666
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9438442654079862
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8608813815646701
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9285481770833334
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7739054361979166
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7962392171223959
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9218834771050347
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9442240397135416
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8808229234483507
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9419890509711372
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.799774169921875
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8181372748480903
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9020250108506944
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9353298611111112
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8964114718967013
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9533441331651475
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8027106391059028
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8183407253689237
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9092780219184028
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9406110975477431
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8836207919650607
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9452531602647569
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7735460069444444
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7915768093532987
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9449632432725694
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9679107666015625
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8814197116427951
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.941780514187283
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8024987114800347
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8181542290581597
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9422827826605903
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9716050889756944
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8905376858181424
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9404097663031684
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8118676079644097
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8232625325520834
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.954437255859375
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9787394205729166
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.9219868977864584
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9457388983832465
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8046993679470487
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8222774929470487
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9563785129123263
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9780748155381944
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.9470956590440538
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9593823750813802
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7956153021918403
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8038397894965278
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9318762885199653
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.971221923828125
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.9369786580403646
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9804602728949653
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.9999813503689237
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.8900597890218099
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.9680862426757812
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.8618545532226562
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.9428310394287109
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.9132051467895508
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.9432268142700195
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.9080171585083008
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.9382104873657227
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.8200874328613281
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.8303394317626953
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.7851409912109375
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.8172626495361328
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.81689453125
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.8667430877685547
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.78125
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.7957255045572916
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.703125
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.29524739583333337
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.9757646833147321
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.9862725394112724
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.9674366542271206
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.9834082467215401
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.9960359845842633
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.9967771257672992
Final sparsity level of 0.866: 0.8660000131806067
wandb: WARNING W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.
2022-09-10 07:46:29,872 | INFO | main_after.py: 96 : main() ::	 Experiment Name: run_cv_mawps
2022-09-10 07:46:29,872 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:46:29,872 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:46:30,375 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:46:30,375 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 1537
2022-09-10 07:46:30,375 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 384
2022-09-10 07:46:30,375 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: ['0.5', '3.0', '100.0', '12.0', '0.01', '0.1', '1.0', '4.0', '7.0']
2022-09-10 07:46:30,375 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:46:30,375 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:46:30,568 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 2449 / 2449 = 1.0
2022-09-10 07:46:30,611 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 2452 words in input language, 21 words in output
2022-09-10 07:46:30,621 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:46:33,930 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:46:33,930 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:46:37,390 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:46:37,391 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/mawps/dense/models/run_cv_mawps_fold0.
2022-09-10 07:46:40,766 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:46:45,819 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.0	
 equation acc epoch: 0.0	
 max val acc: 0.0	
 equation acc: 0.0	
2022-09-10 07:46:45,819 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 5s
2022-09-10 07:46:45,823 | INFO | main_after.py: 96 : main() ::	 Experiment Name: run_cv_mawps
2022-09-10 07:46:45,823 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:46:45,823 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:46:46,251 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:46:46,251 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 1537
2022-09-10 07:46:46,251 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 384
2022-09-10 07:46:46,251 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: ['0.01', '12.0', '1.0', '100.0', '0.1', '0.5', '3.0', '4.0', '7.0']
2022-09-10 07:46:46,251 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:46:46,251 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:46:46,442 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 2449 / 2449 = 1.0
2022-09-10 07:46:46,485 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 2452 words in input language, 21 words in output
2022-09-10 07:46:46,496 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:46:49,456 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:46:49,456 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:46:49,533 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:46:49,534 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/mawps/dense/models/run_cv_mawps_fold1.
2022-09-10 07:46:55,630 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:47:04,779 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.03125	
 equation acc epoch: 0.0	
 max val acc: 0.03125	
 equation acc: 0.0	
2022-09-10 07:47:04,779 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 9s
2022-09-10 07:47:04,782 | INFO | main_after.py: 96 : main() ::	 Experiment Name: run_cv_mawps
2022-09-10 07:47:04,782 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:47:04,782 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:47:05,213 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:47:05,213 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 1537
2022-09-10 07:47:05,213 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 384
2022-09-10 07:47:05,213 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: ['0.01', '12.0', '1.0', '100.0', '0.1', '0.5', '3.0', '4.0', '7.0']
2022-09-10 07:47:05,213 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:47:05,213 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:47:05,408 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 2449 / 2449 = 1.0
2022-09-10 07:47:05,451 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 2452 words in input language, 21 words in output
2022-09-10 07:47:05,462 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:47:08,345 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:47:08,345 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:47:08,423 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:47:08,423 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/mawps/dense/models/run_cv_mawps_fold2.
2022-09-10 07:47:13,898 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:47:39,508 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.0026041666666666665	
 equation acc epoch: 0.0	
 max val acc: 0.0026041666666666665	
 equation acc: 0.0	
2022-09-10 07:47:39,508 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 25s
2022-09-10 07:47:39,511 | INFO | main_after.py: 96 : main() ::	 Experiment Name: run_cv_mawps
2022-09-10 07:47:39,511 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:47:39,511 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:47:39,942 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:47:39,942 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 1537
2022-09-10 07:47:39,942 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 384
2022-09-10 07:47:39,942 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: ['0.01', '12.0', '1.0', '100.0', '0.1', '0.5', '3.0', '4.0', '7.0']
2022-09-10 07:47:39,942 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:47:39,942 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:47:40,136 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 2449 / 2449 = 1.0
2022-09-10 07:47:40,268 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 2452 words in input language, 21 words in output
2022-09-10 07:47:40,279 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:47:43,160 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:47:43,160 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:47:43,237 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:47:43,238 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/mawps/dense/models/run_cv_mawps_fold3.
2022-09-10 07:47:49,843 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:49:04,590 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.018229166666666668	
 equation acc epoch: 0.0	
 max val acc: 0.018229166666666668	
 equation acc: 0.0	
2022-09-10 07:49:04,590 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 1m 14s
2022-09-10 07:49:04,592 | INFO | main_after.py: 96 : main() ::	 Experiment Name: run_cv_mawps
2022-09-10 07:49:04,592 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:49:04,592 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:49:05,020 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:49:05,020 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 1537
2022-09-10 07:49:05,020 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 384
2022-09-10 07:49:05,020 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: ['0.01', '12.0', '1.0', '100.0', '0.1', '0.5', '3.0', '4.0', '7.0']
2022-09-10 07:49:05,020 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:49:05,021 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:49:05,215 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 2449 / 2449 = 1.0
2022-09-10 07:49:05,257 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 2452 words in input language, 21 words in output
2022-09-10 07:49:05,268 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:49:08,148 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:49:08,148 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:49:08,226 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:49:08,226 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/mawps/dense/models/run_cv_mawps_fold4.
2022-09-10 07:49:14,666 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:51:49,700 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.005208333333333333	
 equation acc epoch: 0.0	
 max val acc: 0.005208333333333333	
 equation acc: 0.0	
2022-09-10 07:51:49,701 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 2m 35s
Traceback (most recent call last):
  File "/home/sliu/miniconda3/envs/prune_cry/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/sliu/miniconda3/envs/prune_cry/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/gpfs/home6/sliu/Projects/SVAMP/code/gts/src/main_after.py", line 276, in <module>
    main()
  File "/gpfs/home6/sliu/Projects/SVAMP/code/gts/src/main_after.py", line 266, in main
    folds_scores.append(float(best_acc[w][0])/best_acc[w][1])
ZeroDivisionError: float division by zero
Transfer numbers...
initialize by one_shot_gm
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.7102637531499718
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.9785814891374838
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 1.0
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7445458306206597
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7520260281032987
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9906802707248263
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9888966878255209
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.9476475185818143
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9746873643663194
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8704342312282987
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8810543484157987
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9839630126953125
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9859381781684028
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.9287384880913628
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9711053636338975
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.9001007080078125
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.9072079128689237
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9734039306640625
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9830254448784722
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.9239256117078993
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9664764404296875
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8824615478515625
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8957282172309028
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9733293321397569
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9773813883463541
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.9303016662597656
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.969230228000217
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8735114203559028
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.88848876953125
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9682685004340278
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9781256781684028
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.9429308573404948
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9769727918836806
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8932105170355903
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.9047580295138888
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9581960042317709
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9740447998046875
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.9539964463975694
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9826982286241319
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8946668836805556
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.9064144558376737
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9629313151041666
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9783053927951388
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.9457596672905816
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.977453867594401
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8730265299479166
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8849199083116319
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9791446261935763
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9897037082248263
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.9448165893554688
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.975754631890191
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8984917534722222
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.9083336724175347
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9796939425998263
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9922553168402778
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.951432122124566
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9757173326280382
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.9069095187717013
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.9136403401692709
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9851752387152778
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9948815239800347
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.9710023668077257
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.979628668891059
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.9015299479166666
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.9136115180121528
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9866010877821181
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9947475857204862
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.984269036187066
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9867422315809462
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8955841064453125
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.9003991021050347
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9766828748914931
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9919637044270834
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.9805501302083334
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9943402608235677
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 1.0
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.9718825022379557
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.9948768615722656
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.9609928131103516
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.9878635406494141
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.9827184677124023
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.9915266036987305
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.9833521842956543
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.9930658340454102
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.9831886291503906
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.9799156188964844
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.9679946899414062
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.9724140167236328
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.992431640625
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.96490478515625
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.978515625
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.9703572591145834
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.966796875
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.12955729166666663
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.9970681326729911
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.9984414236886161
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.9971531459263393
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.9984327043805804
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.9999073573521206
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.9999029976981026
Final sparsity level of 0.893: 0.8930000220968994
Transfer numbers...
initialize by one_shot_gm
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.8143211033605225
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.9807372730220493
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 1.0
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6546563042534722
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6629553900824653
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9726426866319444
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9740583631727431
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8990419175889757
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.949853261311849
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7931450737847222
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8081698947482638
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9603068033854166
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9717576768663194
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8726637098524306
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9419509039984809
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8275299072265625
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8404591878255209
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9419063991970487
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9644368489583334
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8670378790961372
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9338226318359375
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8053063286675347
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8247104220920138
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9389631483289931
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9518093532986112
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.876570807562934
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9384418063693576
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7950642903645834
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8162773980034722
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9327646891276041
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9524892171223959
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8951458401150174
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9507353040907118
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.819732666015625
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8372429741753472
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9148898654513888
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9444495307074653
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.9099392361111112
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9606916639539931
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8223605685763888
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8374464246961806
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9216257731119791
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9495578342013888
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8981124030219184
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9533687167697482
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7950202094184028
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8120456271701388
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9532470703125
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9734378390842013
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8962266710069444
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9502381218804253
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8236846923828125
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8383297390407987
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9515635172526041
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9771101209852431
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.9049284193250868
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9491598341200087
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8328908284505209
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8437076144748263
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.96209716796875
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9833662245008681
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.9342036777072482
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9542876349555122
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8256513807508681
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8428327772352431
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9643351236979166
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9827694363064237
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.9569498697916666
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9667061699761285
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8172488742404513
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8249325222439237
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9430508083767362
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9769575330946181
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.948340098063151
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9844784206814237
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.9999949137369791
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.9330425262451172
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.985076904296875
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.9201755523681641
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.9773921966552734
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.952293872833252
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.9773693084716797
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.9499554634094238
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.9689273834228516
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.8661041259765625
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.896697998046875
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.8449935913085938
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.8976097106933594
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.769775390625
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.9108562469482422
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.7421875
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.8275070190429688
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.615234375
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.40852864583333337
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.9871651785714286
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.9964654105050224
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.9901918683733258
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.9975433349609375
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.9994561331612724
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.9997482299804688
Final sparsity level of 0.893: 0.8930000359420744
Transfer numbers...
initialize by one_shot_gm
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.780294931653901
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.9817733665693904
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 1.0
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6917928059895833
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6997595893012153
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9817284478081597
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9815809461805556
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.9215028550889757
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9617093404134115
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8268449571397569
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8402286105685763
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9720221625434028
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9789089626736112
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.898117913140191
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9559135437011719
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8596343994140625
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8701765272352431
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.956878662109375
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9735107421875
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8927552964952257
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9492543538411459
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8393741183810763
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8563266330295138
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9554816351996528
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9642876519097222
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.901015387641059
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9529312981499566
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8290778266059028
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8483310275607638
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9495815700954862
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9647199842664931
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.91717529296875
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9633424546983507
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8523169623480903
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8671332465277778
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9351535373263888
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9586554633246528
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.9305983649359809
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9714779324001737
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8541700575086806
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8680945502387153
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9410892062717013
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9635874430338541
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.9200702243381076
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.96503660413954
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8291863335503472
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8443162706163194
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9656863742404513
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9815402560763888
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.9189075893825955
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9624608357747396
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8569742838541666
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8697747124565972
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9652760823567709
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9850056966145834
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.9267124599880643
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9620001051161025
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8659956190321181
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8750491672092013
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9737040201822916
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9894934760199653
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.9520136515299479
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9666786193847656
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8595038519965278
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8749881320529513
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9756147596571181
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9892001681857638
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.9708073933919271
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9768252902560763
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8518998887803819
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8586985270182291
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9595184326171875
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9847327338324653
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.9643122355143229
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9896863301595052
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.9999983045789931
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.8970775604248047
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.9652767181396484
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.8935527801513672
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.9590044021606445
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.934567928314209
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.9670581817626953
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.9301314353942871
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.9590158462524414
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.8767585754394531
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.8747215270996094
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.8453788757324219
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.8729114532470703
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.859619140625
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.9244766235351562
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.80859375
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.8465321858723959
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.765625
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.2919921875
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.9869602748325893
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.9934158325195312
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.9838976178850446
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.9919281005859375
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.9979596819196429
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.9981013706752232
Final sparsity level of 0.893: 0.8930462995945246
Transfer numbers...
initialize by one_shot_gm
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.724727175138433
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.9795618514915694
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 1.0
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7343326144748263
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7420162624782987
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9892306857638888
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9876725938585069
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.9431279500325521
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9723926120334201
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8624623616536459
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8735487196180556
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9820437961154513
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9847327338324653
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.923323737250434
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9684944152832031
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.89300537109375
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.9004872639973959
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.970672607421875
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9814927842881944
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.9184252421061198
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9635005527072482
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8745524088541666
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8885345458984375
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9703470865885416
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9751993815104166
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.9250428941514757
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.966469234890408
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8653377956814237
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8811747233072916
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9651285807291666
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9759267171223959
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.938452402750651
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9746983846028646
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8858489990234375
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8978763156467013
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9541439480251737
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9713863796657987
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.9500444200303819
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9808540344238281
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8873206244574653
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8995598687065972
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9592030843098959
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9759640163845487
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.9413346184624566
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9753952026367188
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8651241726345487
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8775668674045138
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9768880208333334
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9884931776258681
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.9402779473198785
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9735039605034722
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8909233940972222
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.9012620713975694
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9772186279296875
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9911278618706597
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.9472507900661893
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9734005398220487
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8994327121310763
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.9066162109375
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9833187527126737
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9940422905815972
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.9677755567762587
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.977461920844184
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8938581678602431
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.9065958658854166
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9849667019314237
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9939185248480903
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.9821565416124132
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9851468404134115
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8875478108723959
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8928409152560763
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9738752577039931
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9909328884548612
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.9778556823730469
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9936544630262587
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 1.0
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.9553826649983724
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.9904155731201172
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.9410489400227865
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.9789037704467773
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.9694457054138184
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.9820089340209961
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.9672913551330566
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.9790668487548828
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.9539794921875
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.9596729278564453
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.9355087280273438
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.9492073059082031
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.975830078125
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.9447078704833984
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.931640625
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.9398231506347656
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.9296875
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.17317708333333337
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.9941776820591518
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.9971978323800224
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.9918757847377232
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.996840340750558
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.99957275390625
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.999497549874442
Final sparsity level of 0.893: 0.8930000151743118
Transfer numbers...
initialize by one_shot_gm
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.7840356527073179
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.9812236543450065
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 1.0
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6874474419487847
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.695465087890625
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9807552761501737
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9807773166232638
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.9189830356174045
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9604407416449653
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8232014973958334
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8366275363498263
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9707929823133681
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9781714545355903
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.89521238538954
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9543465508355035
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8559688991970487
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8668246799045138
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9552544487847222
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9724816216362847
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8899040222167969
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9475445217556424
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8354543050130209
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8527272542317709
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9536302354600694
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9629787868923612
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8982844882541232
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9513541327582465
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8252292209201388
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8446926540798612
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9476725260416666
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9633806016710069
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.9146614074707031
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9619496663411459
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8485480414496528
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8636627197265625
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9327985975477431
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9569837782118056
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.9282701280381944
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9702847798665365
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8505181206597222
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8644765218098959
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9389122856987847
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9620395236545138
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.9175139533148872
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9637485080295138
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8251173231336806
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8403049045138888
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9642622205946181
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9806298149956597
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.9162754482693143
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9610866970486112
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8529917399088541
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8659549289279513
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9637705485026041
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9842359754774306
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.9242977566189237
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.960531022813585
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8619537353515625
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8713565402560763
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9725375705295138
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9888170030381944
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.9499893188476562
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9652760823567709
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8552720811631944
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8710140652126737
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9743398030598959
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9885135226779513
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.9693094889322916
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.975724114312066
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8477969699435763
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8547498914930556
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9576772054036459
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9839274088541666
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.9625511169433594
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9891543918185763
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.9999983045789931
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.9186954498291016
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.9782514572143555
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.8949165344238281
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.9588184356689453
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.9374094009399414
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.9634838104248047
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.9332857131958008
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.9596319198608398
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.8681106567382812
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.8751716613769531
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.8373374938964844
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.8660602569580078
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.865966796875
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.8987445831298828
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.810546875
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.8409932454427084
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.740234375
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.318359375
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.9851205008370536
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.9923389979771206
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.980820792061942
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.9914616176060268
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.9982125418526786
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.9987269810267857
Final sparsity level of 0.893: 0.8930066747035139

JOB STATISTICS
==============
Job ID: 1507125
Cluster: snellius
User/Group: sliu/sliu
State: RUNNING
Nodes: 1
Cores per node: 18
CPU Utilized: 00:00:00
CPU Efficiency: 0.00% of 09:31:12 core-walltime
Job Wall-clock time: 00:31:44
Memory Utilized: 0.00 MB (estimated maximum)
Memory Efficiency: 0.00% of 0.00 MB (0.00 MB/node)
WARNING: Efficiency statistics may be misleading for RUNNING jobs.
