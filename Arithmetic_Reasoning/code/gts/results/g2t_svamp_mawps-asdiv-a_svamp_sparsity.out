#IMP

sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.5283030407589775
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.8238311648832685
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 1.0
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.11307610405815971
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.11628892686631942
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.3074883355034722
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.33334011501736116
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.216467539469401
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.28556527031792533
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.1519317626953125
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.15991719563802087
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.27914937337239587
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.3473137749565972
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.2017016940646701
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.2726508246527778
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.16363525390625
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.17083401150173616
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.25696987575954866
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.3300950792100694
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.1981243557400174
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.26777521769205725
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.15517171223958337
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.1652764214409722
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.24453226725260413
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.31434292263454866
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.20359844631618929
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.27655580308702254
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.1523963080512153
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.16166347927517366
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.2507544623480903
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.32738240559895837
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.21658070882161462
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.29665162828233504
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.16393873426649308
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.1734229193793403
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.23512776692708337
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.30517578125
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.22535790337456596
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.31284544203016496
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.1630316840277778
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.17053392198350692
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.2356109619140625
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.29184299045138884
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.2151446872287326
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.30415852864583337
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.15141465928819442
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.16053941514756942
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.28739590115017366
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.3504638671875
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.21170383029513884
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.29245546129014754
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.1586083306206597
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.16605801052517366
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.26157803005642366
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.3230133056640625
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.21401850382486975
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.2828848097059462
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.1617194281684028
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.1673973931206597
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.27603658040364587
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.3237525092230903
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.2300893995496962
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.27526262071397567
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.15804036458333337
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.16582743326822913
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.2705824110243056
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.3116607666015625
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.24533589680989587
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.2833052741156684
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.1537238226996528
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.1584252251519097
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.24011060926649308
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.28665500217013884
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.23444832695855033
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.3320138719346788
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.4862280951605903
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.5514272054036458
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.797490119934082
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.4701754252115885
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.6880702972412109
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.5193343162536621
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.40023231506347656
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.5120649337768555
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.38364505767822266
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.23041152954101562
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.2475128173828125
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.22785568237304688
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.25444602966308594
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.17236328125
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.6611709594726562
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.64453125
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.36691792805989587
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.3046875
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.32779947916666663
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.5293807983398438
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.5580596923828125
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.5193078177315849
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.5414755684988839
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.6035058157784599
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.6197226388113839
Final sparsity level of 0.2: 0.36000348095391266


sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.6681552873934813
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.9177559581712063
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 1.0
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.32715182834201384
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.33322482638888884
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7270541720920138
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.758575439453125
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5682462056477864
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6869460211859809
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.4300977918836806
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.4480421278211806
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6873779296875
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7704213460286459
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5355970594618056
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6636937459309895
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.46155802408854163
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.4784766303168403
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6495581732855903
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7423078748914931
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5290247599283855
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6502944098578559
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.43987019856770837
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.4625481499565972
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6271870930989583
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7080959743923612
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.541678958468967
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6619805230034722
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.43127102322048616
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.4542083740234375
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6341078016493056
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7149285210503472
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5696534050835503
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.690093994140625
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.45657857259114587
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.4787885877821181
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5997907850477431
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6914944118923612
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5880377027723525
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.715227762858073
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.4575822618272569
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.4757978651258681
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6024322509765625
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6834462483723958
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5691765679253472
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7029118008083768
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.43121337890625
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.4510328504774306
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6911502414279513
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7650519476996528
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5628772311740451
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6907212999131944
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.45104641384548616
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.46851603190104163
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6585286458333333
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7482791476779513
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5697665744357638
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6814058091905382
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.459197998046875
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.4730767144097222
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6877882215711806
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7608439127604166
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6085904439290364
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6795378790961372
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.4505547417534722
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.4699927435980903
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6802927652994792
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7505527072482638
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6439459058973525
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7005878024631076
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.4417504204644097
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.45126851399739587
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6219889322916667
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7185448540581597
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6221190558539497
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7768999735514323
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.9496866861979166
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.9102986653645834
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.9813261032104492
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.8810284932454427
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.964146614074707
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.9265646934509277
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.9275598526000977
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.9220876693725586
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.9174365997314453
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.7884407043457031
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.8165149688720703
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.7971954345703125
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.8360481262207031
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.68212890625
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.948974609375
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.822265625
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.8816617329915365
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.591796875
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.46875
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.9266313825334821
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.9477898733956474
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.9369430541992188
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.9571598597935268
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.9594334193638393
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.9792556762695312
Final sparsity level of 0.2: 0.6723314684990147



sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.7240389477436254
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.9640483138780804
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 1.0
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5462595621744792
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5548790825737847
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9277258978949653
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9375033908420138
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8143543667263455
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8970735337999132
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6873779296875
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7062835693359375
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9070519341362847
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9386715359157987
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7821740044487847
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8832651774088541
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7252163357204862
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7412601047092013
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8795810275607638
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9237213134765625
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7752740648057725
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.871077643500434
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6966586642795138
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7201046413845487
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8694118923611112
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9010891384548612
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7877349853515625
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8782399495442709
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6871083577473958
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7118581136067708
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8661668565538194
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9014807807074653
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8126233418782552
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8963394165039062
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7152811686197917
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7376302083333333
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8383873833550347
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8885515001085069
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8314874437120225
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9131041632758247
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.718902587890625
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7375895182291667
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8465796576605903
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8925001356336806
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8162587483723959
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.903463151719835
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6912587483723958
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7114223904079862
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9003177218967013
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9354841444227431
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.81310060289171
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8983493381076388
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7176174587673612
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.735443115234375
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8911319308810763
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9376729329427084
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8224962022569444
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.894730461968316
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7261912027994792
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7401309543185763
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9096408420138888
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9492831759982638
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8599569532606337
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.899658203125
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7192399766710069
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7386983235677083
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9106767442491319
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9473232693142362
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8931744893391927
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9179322984483507
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7065294053819444
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7161729600694444
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8721822102864584
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9341956244574653
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.878133561876085
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9552964104546441
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.999481201171875
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.9844824473063151
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.9967737197875977
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.9795716603597006
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.9936866760253906
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.9900608062744141
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.9930610656738281
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.9886155128479004
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.9907655715942383
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.9600028991699219
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.9695682525634766
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.972442626953125
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.9783802032470703
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.864990234375
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.9926471710205078
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.884765625
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.9837773640950521
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.767578125
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.529296875
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.9905624389648438
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.9925188337053571
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.9933831351143974
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.9970637730189732
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.99395751953125
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.9984675816127232
Final sparsity level of 0.2: 0.8322337702981343


sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.534832807992639
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.829340953307393
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 1.0
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.11903042263454866
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.122222900390625
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.32251485188802087
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.34934488932291663
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.22759670681423616
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.29976230197482634
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.15985616048177087
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.16822984483506942
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.29296875
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.36339823404947913
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.2120276557074653
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.28610992431640625
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.1719818115234375
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.1794891357421875
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.2702653672960069
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.3460320366753472
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.20825322469075525
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.28108512030707467
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.16321309407552087
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.17353990342881942
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.2574615478515625
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.32951863606770837
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.21403037177191842
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.290215810139974
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.16048177083333337
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.1702490912543403
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.26346333821614587
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.3423241509331597
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.22773657904730904
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.31104320949978304
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.1726531982421875
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.18246968587239587
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.24738735622829866
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.31973605685763884
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.2369011773003472
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.3279618157280816
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.1713714599609375
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.17967732747395837
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.2474450005425347
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.30632527669270837
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.22635057237413192
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.31895319620768225
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.15945773654513884
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.16881137424045134
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.30155436197916663
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.36699930826822913
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.2227762010362413
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.30703438652886283
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.16679043240017366
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.1747826470269097
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.27483622233072913
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.33906046549479163
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.22516165839301217
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.2969949510362413
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.1701439751519097
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.17650180392795134
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.28979153103298616
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.3396318223741319
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.24209001329210067
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.2894113328721788
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.16636996799045134
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.1745452880859375
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.28412373860677087
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.32757059733072913
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.25803332858615446
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.29780069986979163
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.16189914279513884
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.16688028971354163
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.2521040174696181
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.30139668782552087
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.24669604831271696
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.34877819485134554
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.5086279975043403
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.44373575846354163
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.6823997497558594
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.3741499582926432
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.566431999206543
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.4435238838195801
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.38520336151123047
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.42949533462524414
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.35939884185791016
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.209716796875
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.21311378479003906
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.19635772705078125
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.2198467254638672
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.157470703125
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.4386558532714844
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.5234375
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.26024881998697913
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.25390625
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.23763020833333337
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.4973820277622768
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.5386668613978794
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.4965166364397321
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.5139781406947544
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.5902753557477678
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.6015112740652901
Final sparsity level of 0.36: 0.3600000127375611



sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.6713128491909546
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.9249579482814526
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 1.0
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.3488905164930556
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.3553449842664931
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7572750515407987
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7866346571180556
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5990672641330295
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7173741658528645
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.45759243435329866
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.4761945936414931
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7186397976345487
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7972954644097222
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5653326246473525
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6942087809244792
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.4897698296440972
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5075531005859375
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6802961561414931
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7693956163194444
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5588963826497395
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.68016603257921
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.4672936333550347
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.4908684624565972
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6593661838107638
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.73541259765625
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5715323554144965
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6911544799804688
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.45839436848958337
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.4823286268446181
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6655680338541667
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7410159640842013
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6002701653374566
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7191463046603732
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.48472934299045134
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5071546766493056
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6308119032118056
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7183888753255208
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6191067165798612
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7436235215928819
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.4853430853949653
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5041334364149306
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6341010199652778
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7123650444878472
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5996242099338107
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7308035956488715
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.4571601019965278
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.47774081759982634
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7212948269314237
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7909189860026041
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5935787624782987
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7196803622775607
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.47786458333333337
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.4964616563585069
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6907382541232638
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7772911919487847
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6005872090657551
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7111223008897569
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.48611789279513884
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5011105007595487
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7195909288194444
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7910122341579862
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.640856424967448
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7103792826334636
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.47719150119357634
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.4981519911024306
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7131195068359375
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7815331353081597
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6771019829644097
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7317746480305989
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.4684126112196181
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.47887166341145837
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6554056803385417
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7510969373914931
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6553064982096355
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8057178921169705
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.9642791748046875
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.7399705251057942
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.9008703231811523
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.6609554290771484
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.8186588287353516
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.7706985473632812
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.8155145645141602
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.7550320625305176
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.7834396362304688
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.5732994079589844
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.5741729736328125
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.5446701049804688
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.5962600708007812
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.4541015625
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.7642822265625
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.6328125
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.5785140991210938
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.365234375
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.38671875
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.878584725516183
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.9099829537527901
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.8877040318080357
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.9041355678013393
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.9455032348632812
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.9550421578543526
Final sparsity level of 0.672: 0.6720000150081697



initialize by one_shot_gm
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.7553932128469114
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.9671920598249028
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 1.0
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5490824381510417
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5574696858723958
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9311336941189237
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9404771592881944
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.819466061062283
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.901137457953559
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6883104112413194
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7072550455729167
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9107699924045138
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9411112467447916
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7872746785481771
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8874566819932725
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7266031901041667
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7424638536241319
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8837975396050347
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9267323811848959
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7809558444552951
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8757968478732638
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7013804117838542
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7250077989366319
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8744608561197916
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9047580295138888
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7934850056966146
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.882698482937283
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6907450358072917
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7158067491319444
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8708970811631944
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9051225450303819
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8180592854817709
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9005156622992622
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7183719211154513
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7409091525607638
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8437347412109375
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.89251708984375
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.836310068766276
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9166573418511285
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7217152913411458
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7404768202039931
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8517235649956597
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8964623345269097
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8204807705349393
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9064814249674479
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6904381646050347
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7109595404730903
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9040205213758681
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9384002685546875
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8171925014919705
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9016024271647135
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7192755805121528
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7378811306423612
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8956807454427084
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9409213595920138
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8266889784071181
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8982649909125434
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7288648817274306
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7431301540798612
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9137895372178819
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9521925184461806
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8646155463324653
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9037979973687066
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7213558620876737
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7411092122395833
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9148847791883681
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9503343370225694
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.897566901312934
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9219322204589844
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7107374403211806
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7206912570529513
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8775312635633681
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9380120171440972
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8824403550889757
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9575822618272569
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.9995761447482638
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.8834463755289713
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.9655694961547852
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.8298505147298177
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.9245920181274414
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.9070916175842285
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.9512548446655273
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.8972082138061523
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.935394287109375
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.807159423828125
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.8023262023925781
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.7849998474121094
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.8291645050048828
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.686767578125
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.8824882507324219
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.734375
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.7607498168945312
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.48828125
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.47721354166666663
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.9739183698381696
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.985060555594308
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.9817592075892857
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.9870180402483258
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.994614737374442
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.9970921107700893
Final sparsity level of 0.8325: 0.832557290503672





# omp before 


initialize by one_shot_gm
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.15280021096521768
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.3747897414072633
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 1.0
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.1857215033637153
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.1900702582465278
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.4792921278211806
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5132836235894097
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.3474693298339844
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.4455587599012587
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.2488250732421875
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.26093546549479163
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.4400651719835069
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5294443766276042
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.32439253065321183
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.42650180392795134
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.2677900526258681
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.2787560356987847
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.4088372124565972
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5057017008463542
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.3193342420789931
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.4178971184624566
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.2545454237196181
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.2689734564887153
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.39011806911892366
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.4802398681640625
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.32791900634765625
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.42975446912977433
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.24873860677083337
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.26431783040364587
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.3983001708984375
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.4938286675347222
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.34820217556423616
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.4565730624728732
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.26614718967013884
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.28182644314236116
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.37383694118923616
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.46755472819010413
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.36101277669270837
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.4788335164388021
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.26569112141927087
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.27805752224392366
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.37342495388454866
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.4524366590711806
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.3462944030761719
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.46749793158637154
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.2479180230034722
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.2612931993272569
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.44957987467447913
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5312584771050347
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.34111573961046004
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.4532466464572482
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.2594078911675347
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.2715742323133681
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.4154290093315972
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5016716851128472
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.3448838127983941
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.4412583245171441
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.26379733615451384
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.27296786838107634
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.43733723958333337
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5057203504774306
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.3702655368381076
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.4340752495659722
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.25848897298177087
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.27148946126302087
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.4282463921440972
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.49033101399739587
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.39390648735894096
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.44747585720486116
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.25218539767795134
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.25981479220920134
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.3826429578993056
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.4567701551649306
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.37738588121202254
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5173958672417535
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.7238396538628472
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.4920279184977213
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.49239253997802734
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.4918403625488281
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.4912261962890625
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.4922943115234375
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.4928617477416992
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.49228858947753906
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.49268150329589844
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.4909172058105469
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.6955070495605469
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.4915962219238281
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.6965732574462891
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.681640625
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.6956672668457031
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.48046875
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.8529434204101562
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.45703125
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.01627604166666663
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.921067374093192
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.9207872663225446
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.9207829066685268
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.9210521153041294
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.9213485717773438
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.9208995274135044
Final sparsity level of 0.36: 0.36009209699685973


initialize by one_shot_gm
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.35579745059517887
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.7519455252918288
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 1.0
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.42618984646267366
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.4343702528211806
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8454369439019097
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8668501112196181
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6985770331488715
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.807519276936849
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5517493353949653
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5715009901258681
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8129221598307291
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8723805745442709
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6633474561903212
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.786606682671441
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5876261393229167
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6069878472222222
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7772861056857638
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8491923014322916
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6566196017795138
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7722655402289497
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5626881917317708
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.587615966796875
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7596825493706597
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8172539605034722
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6700596279568143
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7819285922580295
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5532023111979167
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5779012044270833
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7621544731987847
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8196733262803819
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6987639533148872
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8062778049045138
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5816633436414931
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6051449245876737
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7279696994357638
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8013322618272569
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7185071309407551
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8285776774088541
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5831756591796875
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6028069390190972
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7344394259982638
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8002149793836806
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6990492078993056
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8159162733289931
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5513288709852431
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5724521213107638
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8107944064670138
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8654819064670138
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6937637329101562
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8072403801812066
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5757700602213542
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5953742133246528
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7892100016276041
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8612213134765625
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7019521925184462
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8010427686903212
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5841488308376737
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6005333794487847
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8152347140842013
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8760189480251737
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7441151936848958
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8038584391276041
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5753072102864583
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5974087185329862
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8124237060546875
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8704494900173612
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7819671630859375
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8256624009874132
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5653737386067708
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5757225884331597
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7590789794921875
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8471120198567709
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7602339850531684
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8866250779893663
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.9917636447482638
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 1.0
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 1.0
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 1.0
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 1.0
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 1.0
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 1.0
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 1.0
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 1.0
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 1.0
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 1.0
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 1.0
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 1.0
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 1.0
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 1.0
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 1.0
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 1.0
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 1.0
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.03548177083333337
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 1.0
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 1.0
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 1.0
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 1.0
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 1.0
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 1.0
Final sparsity level of 0.672: 0.6720444857104502


sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.551518954748168
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.9382599708171206
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 1.0
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6649797227647569
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6731719970703125
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9760521782769097
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9767930772569444
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.9062016805013021
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9536310831705729
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8031073676215278
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8177490234375
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9644690619574653
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9742719862196181
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8807652791341146
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9465162489149306
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8375566270616319
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8496907552083334
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9472130669487847
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9677140977647569
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8752653333875868
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9388978746202257
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8158501519097222
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8345709906684028
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9448530409071181
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9562327067057291
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8843731350368924
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9431991577148438
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8057640923394097
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8261244032118056
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9387630886501737
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9567752414279513
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.902276357014974
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9549882676866319
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8300916883680556
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8467169867621528
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9220309787326388
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9494815402560763
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.9166090223524306
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9644118414984809
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8323805067274306
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8473154703776041
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9284735785590278
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9546322292751737
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.9051848517523872
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9573232862684462
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8053927951388888
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8217705620659722
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9577653672960069
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9764082166883681
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.9034589131673177
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9543168809678819
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8335910373263888
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8476952446831597
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9564310709635416
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9800279405381944
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.9118787977430556
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9534068637424045
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8426971435546875
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8531324598524306
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9663035074869791
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9855821397569444
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.9400995042588975
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.958428700764974
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8360070122612847
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8527069091796875
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9684431287977431
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.98516845703125
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.9615563286675347
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9701533847384982
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8273196750217013
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8349015977647569
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9489813910590278
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9797329372829862
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.9534577263726128
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9863700866699219
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.9999983045789931
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 1.0
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 1.0
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 1.0
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 1.0
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 1.0
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 1.0
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 1.0
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 1.0
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 1.0
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 1.0
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 1.0
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 1.0
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 1.0
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 1.0
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 1.0
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 1.0
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 1.0
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.07194010416666663
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 1.0
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 1.0
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 1.0
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 1.0
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 1.0
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 1.0
Final sparsity level of 0.8325: 0.832572457892946


#GMP

sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.4703794887098378
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.7882670638780804
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 1.0
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.10695054796006942
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.11015489366319442
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.2923516167534722
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.3167945014105903
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.205132802327474
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.2711529201931424
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.14368693033854163
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.1516198052300347
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.2647569444444444
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.3305426703559028
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.19109937879774308
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.25881237453884554
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.1547614203559028
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.1617889404296875
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.2438744439019097
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.3140411376953125
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.18752500745985246
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.25429111056857634
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.14705403645833337
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.15630086263020837
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.23194715711805558
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.2992943657769097
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.19274775187174475
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.2626478407118056
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.144317626953125
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.15314059787326384
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.23785230848524308
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.3117082383897569
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.20515611436631942
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.2818514506022135
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.15514628092447913
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.16411336263020837
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.22319539388020837
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.2900916205512153
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.2134967380099826
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.29754638671875
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.15395948621961808
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.16129726833767366
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.22341579861111116
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.2773607042100694
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.2038705613878038
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.2890408833821615
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.14332750108506942
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.15182664659288192
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.2728203667534722
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.333343505859375
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.20046276516384554
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.27796215481228304
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.14987352159288192
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.157135009765625
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.24794175889756942
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.30699496799045134
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.20263078477647567
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.2686331007215712
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.15287611219618058
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.15841335720486116
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.26185268825954866
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.3072119818793403
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.2178628709581163
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.26095030042860246
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.149871826171875
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.15714687771267366
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.25610012478298616
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.29571194118923616
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.23228285047743058
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.2685462103949653
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.1452246771918403
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.1497650146484375
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.22727457682291663
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.27147250705295134
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.2220285203721788
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.31489902072482634
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.6731255849202473
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.8401117324829102
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.6372261047363281
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.7913570404052734
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.7087640762329102
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.7064523696899414
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.696082592010498
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.6885671615600586
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.542724609375
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.5484561920166016
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.5376548767089844
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.5748615264892578
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.467041015625
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.7182407379150391
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.48828125
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.6049334208170574
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.263671875
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.22916666666666663
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.8259756905691964
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.8596910749162947
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.8248661586216518
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.8465652465820312
Final sparsity level of 0.36: 0.35994837957413794


sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.6321335981796479
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.9209199700064851
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 1.0
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.3518761528862847
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.3580237494574653
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7610999213324653
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7902509901258681
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6030489603678386
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.721053229437934
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.46098836263020837
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.47956339518229163
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7224612765842013
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8006201850043403
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5692172580295138
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6981510586208768
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.49351162380642366
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5115068223741319
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6845482720269097
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7729712592230903
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5627916124131944
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6839968363444011
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.47069973415798616
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.4944068060980903
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6635318332248263
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7387271457248263
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5754695468478732
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6949475606282551
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.46180386013454866
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.4857618543836806
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6696234809027778
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.74468994140625
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6043578253851997
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.722818586561415
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.48822360568576384
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5109083387586806
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6350453694661458
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7219763861762153
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6229917738172743
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7472763061523438
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.4889238145616319
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5076751708984375
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6386294894748263
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7162848578559028
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6034592522515191
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7343232896592882
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.46056620279947913
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.4808943006727431
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7249637179904513
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7942114935980903
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5974269443088107
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7232678731282551
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.4814283582899306
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5001780192057292
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6949072943793403
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7809906005859375
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6046163770887587
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7147148980034722
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.4894019232855903
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5046183268229167
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7235090467664931
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7949896918402778
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6449551052517362
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7141638861762153
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.48045857747395837
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5015631781684028
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7172037760416667
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7855580647786459
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6811964246961806
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7356512281629775
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.4716661241319444
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.482147216796875
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6596934000651042
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7555287679036459
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6591220431857638
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8093155754937066
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.8323186238606771
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.931121826171875
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.7982400258382162
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.8948345184326172
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.8648943901062012
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.8788661956787109
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.861447811126709
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.8789825439453125
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.7876510620117188
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.7901840209960938
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.776397705078125
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.8083248138427734
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.6806640625
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.8681411743164062
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.5703125
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.8152236938476562
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.41796875
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.32454427083333337
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.9481244768415179
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.9608078002929688
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.9533800397600446
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.9648568289620536
Final sparsity level of 0.672: 0.671947695999398


sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.7429056210418117
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.9671388618677043
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 1.0
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5563134087456597
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5650278727213542
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9350145128038194
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9436238606770834
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.825722164577908
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9051954481336806
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6956498887803819
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7146046956380208
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9151357014973959
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9439103868272569
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7940292358398438
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8920211791992188
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7342393663194444
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.750335693359375
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8887752956814237
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9301300048828125
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7875188191731771
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8804647657606337
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7090081108940972
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7325693766276042
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8799048529730903
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9088677300347222
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7998686896430122
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8873702155219184
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6985236273871528
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7236005995008681
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.87615966796875
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9091746012369791
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8240716722276475
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9048063490125868
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7263675265842013
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7481943766276042
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8495432535807291
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8969133165147569
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8423216078016493
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9204830593532987
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.729461669921875
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7478603786892362
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8575846354166666
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9011196560329862
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8266724480523003
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9104652404785156
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6982811821831597
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7185397677951388
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9083489312065972
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9416453043619791
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8234269883897569
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9056955973307291
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7275475396050347
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7457207573784722
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9007585313585069
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9444529215494791
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8329688178168403
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9024615817599826
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7364501953125
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7505086263020834
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9182383219401041
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9553120930989584
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.87035157945421
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9079305860731337
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7291208902994792
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7485928005642362
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9194505479600694
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9535386827256944
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.9027057223849826
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9256782531738281
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7181820339626737
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7280019124348958
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8831091986762153
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9417131212022569
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8877292209201388
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9601406521267362
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.9076290130615234
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.9644451141357422
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.8665752410888672
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.9301328659057617
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.9263029098510742
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.9362220764160156
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.9245142936706543
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.9362478256225586
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.8697128295898438
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.8809604644775391
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.8678703308105469
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.8956432342529297
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.8046875
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.9227046966552734
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.65625
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.8824144999186198
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.498046875
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.39485677083333337
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.9716785975864956
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.9814878191266742
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.979416983468192
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.9851913452148438
Final sparsity level of 0.8325: 0.8324187672020209


# SNIP

sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.9973557074588016
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.8735357895590142
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 1.0
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.10484653049045134
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.10746256510416663
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.07148064507378471
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.06944783528645837
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.12031131320529509
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.11787796020507812
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.14164225260416663
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.14670817057291663
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.08319939507378471
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.09075419108072913
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.11281755235460067
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.11116960313585067
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.14258829752604163
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.1468895806206597
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.09131537543402779
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.09736972384982634
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.12039269341362846
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.10504023234049475
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.1459435356987847
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.15258619520399308
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.07925923665364587
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.08029852973090279
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.12453248765733504
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.09590021769205725
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.14226108127170134
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.144287109375
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.07501729329427087
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.07446797688802087
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.12324608696831596
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.09296332465277779
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.13746473524305558
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.138916015625
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.06900872124565971
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.06903923882378471
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.1271252102322049
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.08296076456705725
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.1454399956597222
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.1455620659722222
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.06569247775607634
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.06472100151909721
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.1285858154296875
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.07868533664279509
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.16260782877604163
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.1666039360894097
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.08014085557725692
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.07445949978298616
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.12572267320421004
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.0692117479112413
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.17773607042100692
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.18165079752604163
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.07778422037760413
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.0863037109375
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.12263531155056429
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.0631735059950087
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.17879231770833337
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.1740654839409722
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.07918802897135413
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.08082919650607634
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.1064910888671875
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.053977542453341965
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.1544409857855903
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.1440039740668403
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.06084865993923616
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.065093994140625
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.12391281127929688
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.046188778347439285
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.2048187255859375
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.1632080078125
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.06259663899739587
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.07326592339409721
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.11923175387912321
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.03665839301215279
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.11406262715657556
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.11763191223144531
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.10611470540364587
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.10514640808105469
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.10669374465942383
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.20374584197998047
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.09481620788574219
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.17125701904296875
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.0127105712890625
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.10765647888183594
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.14785003662109375
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.4368762969970703
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.001220703125
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.813507080078125
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.052734375
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.02446492513020837
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.0
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.14095052083333337
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.1755894252232143
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.25373949323381695
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.37213897705078125
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.5102342878069197
Final sparsity level of 0.36: 0.3600000025346789



sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.9979992757137173
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.9147464737354085
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 1.0
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5620490180121528
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5810750325520833
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.4440901014539931
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.43341573079427087
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6051161024305556
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5986819797092013
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6402011447482638
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6691606309678819
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.4948357476128472
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5187784830729167
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5864626566569011
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5825089348687066
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6654917399088542
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6750064425998263
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5192854139539931
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5445658365885417
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6115353902180989
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5620897081163194
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6832038031684028
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6991170247395833
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.47313944498697913
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.47240871853298616
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6179894341362847
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.515067630343967
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6764543321397569
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6837853325737847
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.454437255859375
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.4478166368272569
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6175532870822482
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5096172756618924
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6694166395399306
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6741672092013888
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.42311435275607634
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.4236382378472222
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6275562710232205
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.47876061333550346
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6851077609592013
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6836327446831597
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.4122789171006944
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.40680948893229163
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6277860005696614
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.4542414347330729
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7350938585069444
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7352447509765625
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.47602335611979163
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.45184665256076384
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6234283447265625
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.4193729824490018
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7588263617621528
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7628394232855903
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.46416219075520837
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.4952562120225694
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6119299994574653
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.39019181993272567
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7489386664496528
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7479180230034722
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.46107991536458337
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.47491963704427087
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5666868421766493
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.3577266269259982
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7020941840277778
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6764272054036458
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.3796454535590278
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.4045393202039931
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6185963948567708
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.3167292277018229
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7606472439236112
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6808675130208333
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.38753933376736116
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.43971761067708337
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.606854756673177
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.26300557454427087
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.5569985707600911
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.5244884490966797
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.5325838724772136
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.48795509338378906
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.5029444694519043
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.6859331130981445
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.4594407081604004
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.6326513290405273
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.096832275390625
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.3724193572998047
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.5944023132324219
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.9101467132568359
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.009521484375
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.9997901916503906
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.35546875
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.15336354573567712
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.0
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.154296875
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.5214865548270089
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.6555720738002232
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.7516141619001115
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.8580638340541294
Final sparsity level of 0.672: 0.6720000089558655


sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.9985235025199775
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.9431769820038911
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 1.0
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7962832980685763
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8185611300998263
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6996527777777778
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6874270968967013
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8346337212456597
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8380703396267362
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8535478379991319
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8793419731987847
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7543894449869791
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7756754557291666
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8251389397515191
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8328175014919705
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8698866102430556
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8802354600694444
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7781507703993056
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7998080783420138
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8451063368055556
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8156161838107638
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8908945719401041
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.9005855984157987
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7353498670789931
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7293006049262153
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8477634853786893
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.771322038438585
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8889482286241319
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8923526340060763
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7136637369791667
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7028333875868056
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8457819620768229
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.765853034125434
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8850538465711806
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8894907633463541
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6759372287326388
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6789482964409722
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.856392330593533
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7387546963161893
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8959248860677084
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8913387722439237
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6621958414713542
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6608072916666667
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8533172607421875
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7110074361165364
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.9219936794704862
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.9188554551866319
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7322353786892362
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7082536485460069
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8517706129286025
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6753849453396268
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.9313642713758681
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.9331207275390625
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.716094970703125
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.74993896484375
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.844101799858941
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6404495239257812
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.9235738118489584
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.9247996012369791
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7164323594835069
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7267998589409722
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8094664679633247
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6016248067220051
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.9004431830512153
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8867950439453125
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6184929741753472
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6500430636935763
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8476164076063368
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5525266859266493
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.9199591742621528
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8769107394748263
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6247795952690972
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6817389594184028
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8340602450900607
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.47729195488823783
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.7611993153889974
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.7179946899414062
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.7382062276204426
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.6724977493286133
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.6731910705566406
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.807459831237793
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.6423101425170898
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.775813102722168
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.19000625610351562
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.5632953643798828
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.7969589233398438
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.9833774566650391
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.022705078125
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 1.0
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.669921875
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.2625465393066406
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.001953125
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.1708984375
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.6671742030552456
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.7835769653320312
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.8541281563895089
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.9428209577287946
Final sparsity level of 0.8325: 0.8325000036612029





# Random After


sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.3599382646971053
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.3620703631647212
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 0.36328125
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.3597123887803819
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.35982089572482634
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.35990566677517366
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.3606940375434028
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.3597123887803819
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.3601269192165799
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.35969882541232634
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.35921732584635413
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.35980563693576384
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.3605584038628472
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.35986879136827254
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.3599349127875434
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.36016337076822913
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.35945468478732634
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.3600921630859375
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.35979885525173616
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.35993152194552946
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.35981199476453996
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.3596716986762153
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.3598192003038194
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.36037699381510413
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.35998196072048616
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.3600989447699653
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.3597827487521701
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.3602294921875
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.3593902587890625
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.35958353678385413
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.3592003716362847
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.360144297281901
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.3601663377549913
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.3602990044487847
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.3611314561631944
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.36102634006076384
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.3600226508246528
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.3597284952799479
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.35951995849609375
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.3606855604383681
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.36083645290798616
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.35957845052083337
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.359649658203125
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.35967932807074654
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.35995610555013025
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.35927327473958337
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.3587866889105903
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.359954833984375
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.35962931315104163
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.3606236775716146
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.3602095709906684
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.36002604166666663
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.3599921332465278
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.3595648871527778
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.35992770724826384
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.36019770304361975
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.3599569532606337
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.3593275282118056
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.36062961154513884
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.35913425021701384
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.3597259521484375
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.3602125379774306
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.3597564697265625
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.3608161078559028
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.35994974772135413
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.36098225911458337
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.3607805040147569
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.3601112365722656
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.35947587754991317
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.3598870171440972
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.35975307888454866
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.35950046115451384
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.359893798828125
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.36020236545138884
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.3600807189941406
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.36031087239583337
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.3593184153238932
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.3595294952392578
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.3592878977457682
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.35999393463134766
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.3596057891845703
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.3598289489746094
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.3600311279296875
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.36000728607177734
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.36011505126953125
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.3611412048339844
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.35942840576171875
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.35993385314941406
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.366943359375
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.3595161437988281
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.400390625
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.359839121500651
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.369140625
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.35904947916666663
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.360504150390625
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.35940878731863835
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.36051613943917415
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.3591809953962054
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.3602709089006696
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.3599025181361607
Final sparsity level of 0.36: 0.3599447635664994


Transfer numbers...
initialize by random pruning
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.6719265756076129
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.673351876621271
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 0.65625
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6723649766710069
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6721106635199653
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6722734239366319
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6720394558376737
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6721738179524739
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6722471449110243
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6719529893663194
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6716444227430556
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6724277072482638
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.672943115234375
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.671730465359158
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6717274983723958
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6723005506727431
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6725870768229167
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6719546847873263
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6727498372395833
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6717478434244792
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6723276774088542
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6725379096137153
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6716274685329862
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.672393798828125
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.671722412109375
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6722191704644097
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6717910766601562
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6712629530164931
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6714409722222222
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6718377007378472
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6713019476996528
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6718279520670574
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6717775132921007
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6728600396050347
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6728074815538194
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6717919243706597
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6733449300130208
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6719419691297743
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6717207166883681
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6720971001519097
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6723225911458333
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6714664035373263
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6715596516927083
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6721746656629775
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6717567443847656
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6717258029513888
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6720496283637153
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6727176242404513
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6721750895182292
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6720182630750868
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6719008551703559
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6721937391493056
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6715511745876737
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6720937093098958
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6714426676432292
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6721589830186632
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6721984015570747
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6722700330946181
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6718868679470487
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6719156901041667
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6714155409071181
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6718054877387153
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6721178690592449
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6727328830295138
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6723107231987847
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6724904378255208
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6725955539279513
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6724349127875434
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6722598605685763
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6718224419487847
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6716054280598958
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6731126573350694
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6713409423828125
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6722191704644097
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6719741821289062
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.6721886528862847
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.6718743642171223
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.6716480255126953
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.6714293162027996
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.6713008880615234
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.6720194816589355
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.6720752716064453
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.6718626022338867
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.6724376678466797
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.6712760925292969
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.6718368530273438
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.6718788146972656
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.6721687316894531
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.677734375
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.6727199554443359
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.67578125
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.6714210510253906
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.701171875
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.6744791666666667
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.6715066092354911
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.6727534702845982
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.6723763602120536
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.6709660121372768
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.6727948869977678
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.6715523856026786
Final sparsity level of 0.672: 0.6719947607642359

sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.8324397619698929
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.8330648103112841
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 0.8229166666666666
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8329484727647569
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8322550455729166
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8324449327256944
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8322736952039931
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8325614929199219
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8324627346462674
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.832763671875
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8324839274088541
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8322279188368056
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8327450222439237
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8323614332411025
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8320566813151041
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8332536485460069
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8325992160373263
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8324618869357638
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8328365749782987
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8322881062825521
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8329391479492188
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8327178955078125
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8319922553168403
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8321702745225694
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8317447238498263
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.832617441813151
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8321639166937934
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8318837483723959
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8323092990451388
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8326755099826388
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8317192925347222
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8324250115288628
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8322419060601128
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8327568901909722
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8321109347873263
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8325415717230903
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8335757785373263
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8323800828721788
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8326267666286893
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8322804768880209
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8329789903428819
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8320024278428819
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8322923448350694
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8323343065049913
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.832405514187283
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8323296440972222
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.83258056640625
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8328755696614584
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8327891031901041
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8326165941026475
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8325712415907118
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.832611083984375
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8327280680338541
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8324432373046875
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8323449028862847
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8328200446234809
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.832574208577474
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8324347601996528
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8323347303602431
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8327043321397569
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8319447835286459
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8323656717936198
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.832665761311849
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8332061767578125
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8331858317057291
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8336825900607638
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8330468071831597
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.832940419514974
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8328971862792969
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8326195610894097
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8317006429036459
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8333231608072916
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8321448432074653
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8325085110134549
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8325360616048177
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.8329366048177084
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.8325602213541666
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.8324785232543945
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.8317991892496744
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.8320045471191406
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.8324289321899414
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.8329935073852539
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.8323512077331543
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.8330745697021484
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.8318672180175781
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.8326492309570312
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.8313636779785156
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.8323307037353516
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.839111328125
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.8336277008056641
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.8046875
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.8323186238606771
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.849609375
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.8330078125
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.8321609497070312
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.8324454171316964
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.8325849260602678
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.8319157191685268
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.832871573311942
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.8324301583426339
Final sparsity level of 0.8325: 0.8324916020706179


# random before

sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.35993829060147886
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.3620703631647212
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 1.0
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.35971408420138884
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.35982089572482634
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.35990566677517366
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.3606940375434028
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.3597128126356337
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.3601269192165799
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.35969882541232634
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.35921732584635413
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.35980563693576384
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.3605584038628472
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.35986879136827254
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.3599353366427951
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.36016337076822913
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.35945638020833337
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.3600921630859375
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.35979885525173616
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.35993152194552946
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.35981241861979163
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.3596716986762153
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.35982089572482634
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.36037699381510413
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.3599836561414931
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.36009979248046875
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.3597827487521701
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.3602294921875
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.3593902587890625
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.35958353678385413
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.3592003716362847
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.3601447211371528
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.36016718546549475
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.3602990044487847
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.3611314561631944
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.36102634006076384
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.3600226508246528
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.3597284952799479
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.35952038235134554
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.3606855604383681
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.36083645290798616
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.3595801459418403
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.359649658203125
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.35967932807074654
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.3599565294053819
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.35927327473958337
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.3587866889105903
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.359954833984375
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.3596327039930556
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.3606236775716146
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.3602095709906684
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.36002604166666663
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.3599921332465278
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.3595648871527778
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.35992770724826384
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.36019812689887154
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.3599569532606337
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.3593292236328125
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.36062961154513884
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.35913425021701384
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.3597259521484375
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.3602138095431857
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.35975731743706596
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.3608161078559028
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.35995144314236116
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.36098225911458337
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.3607805040147569
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.3601116604275174
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.35947630140516496
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.3598870171440972
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.35975307888454866
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.35950046115451384
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.3598954942491319
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.3602032131618924
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.3600807189941406
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.36031087239583337
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.3593184153238932
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.3595294952392578
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.3592878977457682
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.35999393463134766
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.3596057891845703
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.3598289489746094
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.3600311279296875
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.36000728607177734
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.36011505126953125
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.3611412048339844
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.35942840576171875
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.35993385314941406
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.366943359375
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.3595161437988281
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.400390625
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.359839121500651
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.369140625
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.35904947916666663
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.360504150390625
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.35940878731863835
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.36051613943917415
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.3591809953962054
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.3602709089006696
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.3599025181361607
Final sparsity level of 0.36: 0.35994836331201396

sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.6719266015119865
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.673351876621271
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 1.0
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6723649766710069
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6721106635199653
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6722734239366319
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6720394558376737
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6721742418077257
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6722471449110243
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6719529893663194
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6716444227430556
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6724277072482638
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.672943115234375
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.671730465359158
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6717279222276475
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6723005506727431
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6725887722439237
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6719546847873263
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6727498372395833
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6717478434244792
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6723281012641059
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6725379096137153
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6716291639539931
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.672393798828125
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.671722412109375
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6722200181749132
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6717910766601562
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6712629530164931
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6714409722222222
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6718377007378472
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6713019476996528
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6718279520670574
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6717779371473525
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6728600396050347
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6728074815538194
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6717919243706597
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6733449300130208
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6719419691297743
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6717211405436199
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6720971001519097
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6723225911458333
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6714664035373263
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6715596516927083
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6721746656629775
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6717567443847656
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6717258029513888
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6720496283637153
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6727176242404513
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6721767849392362
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6720182630750868
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6719008551703559
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6721937391493056
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6715511745876737
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6720937093098958
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6714426676432292
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.672159406873915
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6721984015570747
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.672271728515625
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6718868679470487
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6719156901041667
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6714155409071181
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6718063354492188
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6721182929144965
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6727328830295138
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6723107231987847
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6724904378255208
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6725955539279513
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6724353366427951
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6722598605685763
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6718224419487847
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6716054280598958
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6731126573350694
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6713409423828125
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6722195943196614
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6719741821289062
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.6721886528862847
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.6718743642171223
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.6716480255126953
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.6714293162027996
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.6713008880615234
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.6720194816589355
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.6720752716064453
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.6718626022338867
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.6724376678466797
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.6712760925292969
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.6718368530273438
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.6718788146972656
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.6721687316894531
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.677734375
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.6727199554443359
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.67578125
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.6714210510253906
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.701171875
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.6744791666666667
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.6715066092354911
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.6727534702845982
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.6723763602120536
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.6709660121372768
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.6727948869977678
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.6715523856026786
Final sparsity level of 0.672: 0.6719967129339188


sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.8324397878742664
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.8330648103112841
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 1.0
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8329484727647569
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8322550455729166
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8324449327256944
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8322736952039931
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8325619167751737
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8324627346462674
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.832763671875
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8324839274088541
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8322279188368056
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8327450222439237
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8323614332411025
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8320571051703559
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8332536485460069
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8325992160373263
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8324618869357638
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8328365749782987
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8322881062825521
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8329391479492188
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8327178955078125
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8319939507378472
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8321702745225694
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8317447238498263
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8326182895236545
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8321639166937934
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8318837483723959
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8323092990451388
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8326755099826388
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8317192925347222
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8324250115288628
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8322423299153646
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8327568901909722
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8321109347873263
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8325415717230903
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8335757785373263
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8323800828721788
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.832627190483941
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8322804768880209
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8329789903428819
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8320024278428819
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8322923448350694
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8323343065049913
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.832405514187283
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8323296440972222
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.83258056640625
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8328755696614584
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8327907986111112
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8326165941026475
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8325712415907118
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.832611083984375
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8327280680338541
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8324432373046875
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8323449028862847
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8328200446234809
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.832574208577474
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8324347601996528
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8323347303602431
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8327043321397569
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8319447835286459
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8323656717936198
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.832665761311849
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8332061767578125
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8331858317057291
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8336825900607638
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8330468071831597
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.832940419514974
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8328971862792969
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.8326195610894097
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.8317006429036459
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8333231608072916
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8321448432074653
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8325089348687066
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8325360616048177
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.8329366048177084
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.8325602213541666
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.8324785232543945
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.8317991892496744
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.8320045471191406
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.8324289321899414
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.8329935073852539
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.8323512077331543
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.8330745697021484
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.8318672180175781
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.8326492309570312
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.8313636779785156
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.8323307037353516
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.839111328125
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.8336277008056641
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.8046875
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.8323186238606771
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.849609375
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.8330078125
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.8321609497070312
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.8324454171316964
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.8325849260602678
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.8319157191685268
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.832871573311942
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.8324301583426339
Final sparsity level of 0.8325: 0.832492612768397

