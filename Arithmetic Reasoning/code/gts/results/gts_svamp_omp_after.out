wandb: WARNING W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.
2022-09-10 07:21:13,064 | INFO | main_after.py: 96 : main() ::	 Experiment Name: cv_svamp_augmented
2022-09-10 07:21:13,064 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:21:13,064 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:21:14,098 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:21:14,098 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 3932
2022-09-10 07:21:14,098 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 206
2022-09-10 07:21:14,098 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: ['0.01', '12.0', '1.0', '100.0', '0.1', '0.5', '3.0', '4.0', '7.0']
2022-09-10 07:21:14,098 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:21:14,098 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:21:14,675 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 4068 / 4068 = 1.0
2022-09-10 07:21:14,843 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 4071 words in input language, 21 words in output
2022-09-10 07:21:14,851 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:21:17,826 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:21:17,827 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:21:21,085 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:21:21,085 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/svamp/dense/models/cv_svamp_augmented_fold0.
2022-09-10 07:21:27,964 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:21:34,616 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.5631067961165048	
 equation acc epoch: 0.5436893203883495	
 max val acc: 0.5631067961165048	
 equation acc: 0.5436893203883495	
2022-09-10 07:21:34,616 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 6s
2022-09-10 07:21:34,618 | INFO | main_after.py: 96 : main() ::	 Experiment Name: cv_svamp_augmented
2022-09-10 07:21:34,618 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:21:34,618 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:21:35,570 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:21:35,570 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 3920
2022-09-10 07:21:35,570 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 218
2022-09-10 07:21:35,570 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: ['0.01', '12.0', '1.0', '100.0', '0.1', '0.5', '3.0', '4.0', '7.0']
2022-09-10 07:21:35,570 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:21:35,570 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:21:36,140 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 4068 / 4068 = 1.0
2022-09-10 07:21:36,304 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 4071 words in input language, 21 words in output
2022-09-10 07:21:36,317 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:21:39,260 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:21:39,260 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:21:39,338 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:21:39,338 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/svamp/dense/models/cv_svamp_augmented_fold1.
2022-09-10 07:21:45,207 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:21:53,014 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.6788990825688074	
 equation acc epoch: 0.6422018348623854	
 max val acc: 0.6788990825688074	
 equation acc: 0.6422018348623854	
2022-09-10 07:21:53,015 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 7s
2022-09-10 07:21:53,017 | INFO | main_after.py: 96 : main() ::	 Experiment Name: cv_svamp_augmented
2022-09-10 07:21:53,017 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:21:53,017 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:21:53,974 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:21:53,974 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 3911
2022-09-10 07:21:53,974 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 227
2022-09-10 07:21:53,974 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: ['0.01', '12.0', '1.0', '100.0', '0.1', '0.5', '3.0', '4.0', '7.0']
2022-09-10 07:21:53,974 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:21:53,974 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:21:54,551 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 4068 / 4068 = 1.0
2022-09-10 07:21:54,715 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 4071 words in input language, 21 words in output
2022-09-10 07:21:54,726 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:21:57,564 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:21:57,564 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:21:57,641 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:21:57,641 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/svamp/dense/models/cv_svamp_augmented_fold2.
2022-09-10 07:22:02,244 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:22:10,095 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.5991189427312775	
 equation acc epoch: 0.5682819383259912	
 max val acc: 0.5991189427312775	
 equation acc: 0.5682819383259912	
2022-09-10 07:22:10,095 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 7s
2022-09-10 07:22:10,098 | INFO | main_after.py: 96 : main() ::	 Experiment Name: cv_svamp_augmented
2022-09-10 07:22:10,098 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:22:10,098 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:22:11,158 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:22:11,158 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 3973
2022-09-10 07:22:11,158 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 165
2022-09-10 07:22:11,158 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: ['0.01', '12.0', '1.0', '100.0', '0.1', '0.5', '3.0', '4.0', '7.0']
2022-09-10 07:22:11,158 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:22:11,158 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:22:11,737 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 4068 / 4068 = 1.0
2022-09-10 07:22:11,907 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 4071 words in input language, 21 words in output
2022-09-10 07:22:11,916 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:22:14,778 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:22:14,778 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:22:14,854 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:22:14,854 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/svamp/dense/models/cv_svamp_augmented_fold3.
2022-09-10 07:22:19,617 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:22:25,502 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.5454545454545454	
 equation acc epoch: 0.509090909090909	
 max val acc: 0.5454545454545454	
 equation acc: 0.509090909090909	
2022-09-10 07:22:25,502 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 5s
2022-09-10 07:22:25,504 | INFO | main_after.py: 96 : main() ::	 Experiment Name: cv_svamp_augmented
2022-09-10 07:22:25,504 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:22:25,505 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:22:26,545 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:22:26,545 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 3954
2022-09-10 07:22:26,545 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 184
2022-09-10 07:22:26,545 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: ['0.01', '12.0', '1.0', '100.0', '0.1', '0.5', '3.0', '4.0', '7.0']
2022-09-10 07:22:26,545 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:22:26,545 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:22:27,120 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 4068 / 4068 = 1.0
2022-09-10 07:22:27,287 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 4071 words in input language, 21 words in output
2022-09-10 07:22:27,296 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:22:30,142 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:22:30,142 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:22:30,217 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:22:30,218 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/svamp/dense/models/cv_svamp_augmented_fold4.
2022-09-10 07:22:34,754 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:22:41,058 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.7282608695652174	
 equation acc epoch: 0.6793478260869565	
 max val acc: 0.7282608695652174	
 equation acc: 0.6793478260869565	
2022-09-10 07:22:41,058 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 6s
2022-09-10 07:22:41,058 | INFO | main_after.py: 272 : main() ::	 Final Val score: 0.624
Transfer numbers...
initialize by one_shot_gm
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.740285290046089
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.8110180569066148
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 0.0
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.0
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.0
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.0
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.0
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.0
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.0
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.0
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.0
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.0
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.0
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.0
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.0
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.0
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.0
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.0
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.0
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.0
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.0
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.0
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.0
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.0
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.0
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.0
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.0
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.0
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.0
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.0
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.0
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.0
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.0
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.0
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.0
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.0
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.0
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.0
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.0
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.0
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.0
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.0
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.0
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.0
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.0
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.0
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.0
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.0
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.0
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.0
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.0
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.0
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.0
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.0
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.0
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.0
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.0
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.0
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.0
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.0
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.0
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.0
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.0
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.0
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.0
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.0
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.0
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.0
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.0
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.0
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.0
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.0
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.0
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.0
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.0
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.0
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.0
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.0
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.0
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.0
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.0
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.0
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.0
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.0
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.0
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.0
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.0
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.0
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.0
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.0
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.0
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.0
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.0
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.0
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.0
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.0
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.0
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.0
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.0
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.0
Final sparsity level of 0.2: 0.20004733526899954
Transfer numbers...
initialize by one_shot_gm
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.7400002642246095
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.8110180569066148
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 0.0
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 1.52587890625e-05
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 2.2040473090290114e-05
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 5.2558051215290114e-05
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 4.9167209201339546e-05
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 3.602769639754477e-05
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 4.6200222439285454e-05
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 2.543131510412966e-05
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 2.543131510412966e-05
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 4.9167209201339546e-05
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 5.4253472222209886e-05
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 3.941853841149534e-05
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 4.365709092879477e-05
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 2.543131510412966e-05
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 2.7126736111160454e-05
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 3.221299913191977e-05
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 6.103515625e-05
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 2.924601236975466e-05
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 3.7723117404464546e-05
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 1.6954210069419773e-05
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 1.52587890625e-05
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 3.8994683159709886e-05
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 5.7644314236160454e-05
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 3.5179985894084886e-05
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 5.510118272566977e-05
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 2.034505208337034e-05
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 2.543131510412966e-05
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 3.3908420138839546e-05
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 5.4253472222209886e-05
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 3.7723117404464546e-05
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 5.7644314236160454e-05
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 1.3563368055580227e-05
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 2.543131510412966e-05
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 3.3908420138839546e-05
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 3.8994683159709886e-05
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 3.1365288628459886e-05
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 4.9591064453125e-05
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 1.6954210069419773e-05
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 3.0517578125e-05
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 3.8994683159709886e-05
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 4.408094618058023e-05
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 3.560384114587034e-05
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 4.365709092879477e-05
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 2.3735894097209886e-05
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 2.8822157118080227e-05
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 5.594889322912966e-05
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 5.2558051215290114e-05
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 2.9669867621540114e-05
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 4.4928656684040114e-05
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 1.52587890625e-05
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 2.3735894097209886e-05
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 4.747178819441977e-05
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 4.57763671875e-05
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 3.5179985894084886e-05
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 4.6200222439285454e-05
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 1.52587890625e-05
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 2.3735894097209886e-05
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 5.7644314236160454e-05
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 6.951226128470989e-05
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 3.3908420138839546e-05
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 4.57763671875e-05
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 2.034505208337034e-05
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 2.2040473090290114e-05
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 5.2558051215290114e-05
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 4.408094618058023e-05
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 3.3484564887165114e-05
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 5.340576171875e-05
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 2.034505208337034e-05
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 3.3908420138839546e-05
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 2.2040473090290114e-05
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 3.8994683159709886e-05
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 3.984239366316977e-05
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 4.9167209201339546e-05
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 8.138020833337034e-05
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.00012715657552087034
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.0008411407470703125
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.0001201629638671875
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.0006990432739257812
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.00011444091796875
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 5.817413330078125e-05
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 9.250640869140625e-05
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 3.910064697265625e-05
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 2.288818359375e-05
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 2.6702880859375e-05
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 1.9073486328125e-05
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 2.288818359375e-05
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.0
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 5.340576171875e-05
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.0
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 2.288818359375e-05
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.0
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.0
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 8.501325334819843e-05
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 9.700230189735315e-05
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 9.918212890625e-05
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 8.610316685264685e-05
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.00014277866908485315
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.00011880057198665472
Final sparsity level of 0.2: 0.20001658513519982
Transfer numbers...
initialize by one_shot_gm
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.7402120842866142
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.8110433892671854
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 0.0
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 1.6954210069197728e-06
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 1.6954210069197728e-06
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 1.6954210069197728e-06
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 5.086263020870341e-06
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 4.238552517410454e-06
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 1.6954210069197728e-06
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 1.6954210069197728e-06
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 3.3908420138395456e-06
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 6.781684027790114e-06
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.0
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 1.6954210069197728e-06
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 2.9669867621651136e-06
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 1.6954210069197728e-06
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 5.086263020870341e-06
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 6.781684027790114e-06
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 5.086263020870341e-06
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 2.119276258705227e-06
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 4.662407769084886e-06
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 5.086263020870341e-06
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 1.6954210069197728e-06
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 1.6954210069197728e-06
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 1.017252604162966e-05
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 2.119276258705227e-06
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 5.086263020870341e-06
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.0
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 1.6954210069197728e-06
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 3.3908420138395456e-06
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 1.6954210069197728e-06
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 4.238552517410454e-06
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 3.3908420138395456e-06
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 3.3908420138395456e-06
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 3.3908420138395456e-06
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.0
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 3.3908420138395456e-06
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 2.9669867621651136e-06
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 6.357828776004659e-06
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 3.3908420138395456e-06
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 1.6954210069197728e-06
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 5.086263020870341e-06
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 5.086263020870341e-06
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 8.477105034598864e-07
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 5.086263020870341e-06
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 3.3908420138395456e-06
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 1.6954210069197728e-06
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 3.3908420138395456e-06
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 6.781684027790114e-06
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 1.6954210069197728e-06
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 4.662407769084886e-06
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 5.086263020870341e-06
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 5.086263020870341e-06
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 5.086263020870341e-06
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 5.086263020870341e-06
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 1.2715657552453408e-06
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 3.3908420138395456e-06
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.0
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.0
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 8.477105034709886e-06
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 1.6954210069197728e-06
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 2.5431315103796592e-06
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 6.781684027790114e-06
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 1.6954210069197728e-06
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.0
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 6.781684027790114e-06
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 3.3908420138395456e-06
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 2.9669867621651136e-06
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 5.510118272544773e-06
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.0
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 1.6954210069197728e-06
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 1.6954210069197728e-06
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 3.3908420138395456e-06
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 3.814697265625e-06
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 5.510118272544773e-06
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 1.3563368055580227e-05
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 8.90096028649534e-06
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.000110626220703125
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 1.271565755212034e-05
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 9.1552734375e-05
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 1.1444091796875e-05
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 5.7220458984375e-06
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 8.106231689453125e-06
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 1.9073486328125e-06
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.0
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.0
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 3.814697265625e-06
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 1.9073486328125e-06
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.0
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 9.5367431640625e-06
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.0
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 2.5431315103796592e-06
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.0
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.0
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 1.1989048549154724e-05
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 7.62939453125e-06
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 1.0899135044595276e-05
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 9.80922154014685e-06
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 1.743861607139685e-05
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 7.62939453125e-06
Final sparsity level of 0.2: 0.20003245170581418
Transfer numbers...
initialize by one_shot_gm
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.7183844892900959
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.8120541504539559
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 0.20572916666666663
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.0021023220486111605
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.0023006863064236605
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.00610860188802087
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.00671217176649308
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.0041469997829861605
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.005654652913411495
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.0028652615017361605
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.0030924479166666297
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.00539652506510413
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.0068020290798611605
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.0038587782118055802
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.005252838134765625
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.0031229654947916297
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.00330437554253471
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.00499810112847221
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.0064239501953125
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.003688812255859375
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.005213843451605915
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.0029805501302083703
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.0030568440755208703
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.00465562608506942
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.006317138671875
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.0038418240017361605
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.005422380235460045
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.00286017523871529
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.0029873318142361605
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.00472344292534721
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.0065121120876736605
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.004068586561414955
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.005779266357421875
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.00299919976128471
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.0032145182291666297
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.00462171766493058
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.0058932834201388395
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.0042071872287325895
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.00625101725260413
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.00307549370659721
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.0031568739149305802
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.00460645887586808
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.00560506184895837
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.00406138102213538
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.006059858534071205
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.0028500027126736605
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.00289238823784721
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.00576273600260413
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.00718010796440971
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.00402577718098962
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.0056567721896700895
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.0028059217664930802
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.0031246609157986605
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.00493537055121529
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.0060763888888888395
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.00402323404947913
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.005428314208984375
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.0029703776041666297
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.0030873616536458703
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.00537957085503471
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.0061492919921875
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.0042987399631075895
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.0052600436740450895
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.0029246012369791297
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.0030025906032986605
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.00507269965277779
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.0057339138454861605
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.00458780924479163
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.00537702772352433
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.00298394097222221
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.0028262668185763395
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.00456915961371529
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.00544230143229163
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.0044818454318575895
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.006252288818359375
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.00978257921006942
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.018408457438151005
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.14737987518310547
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.013443628946940067
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.07513999938964844
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.015276432037353516
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.006314277648925781
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.014780521392822266
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.006730079650878906
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.0025787353515625
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.0028228759765625
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.0023651123046875
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.002712249755859375
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.002197265625
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.0067234039306640625
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.03125
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.0031725565592447547
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.009765625
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.00520833333333337
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.010519845145089302
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.0108642578125
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.009650094168526802
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.009207589285714302
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.017583574567522353
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.015428815569196397
Final sparsity level of 0.2: 0.20001939570573624
Transfer numbers...
initialize by one_shot_gm
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.7401234136161676
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.8110180569066148
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 0.0
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.0
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.0
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.0
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.0
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.0
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.0
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.0
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.0
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.0
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.0
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.0
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.0
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.0
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.0
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.0
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.0
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.0
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.0
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.0
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.0
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.0
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.0
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.0
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.0
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.0
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.0
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.0
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.0
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.0
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.0
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.0
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.0
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.0
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.0
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.0
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.0
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.0
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.0
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.0
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.0
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.0
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.0
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.0
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.0
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.0
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.0
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.0
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.0
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.0
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.0
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.0
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.0
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.0
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.0
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.0
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.0
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.0
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.0
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.0
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.0
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.0
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.0
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.0
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.0
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.0
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.0
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.0
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.0
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.0
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.0
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.0
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.0
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.0
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.0
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.0
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.0
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.0
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.0
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.0
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.0
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.0
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.0
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.0
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.0
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.0
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.0
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.0
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.0
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.0
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.0
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.0
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.0
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.0
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.0
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.0
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.0
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.0
Final sparsity level of 0.2: 0.20000407601953651
wandb: WARNING W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.
2022-09-10 07:22:44,037 | INFO | main_after.py: 96 : main() ::	 Experiment Name: cv_svamp_augmented
2022-09-10 07:22:44,037 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:22:44,037 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:22:45,080 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:22:45,080 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 3932
2022-09-10 07:22:45,080 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 206
2022-09-10 07:22:45,080 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: ['0.01', '12.0', '1.0', '100.0', '0.1', '0.5', '3.0', '4.0', '7.0']
2022-09-10 07:22:45,080 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:22:45,080 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:22:45,655 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 4068 / 4068 = 1.0
2022-09-10 07:22:45,822 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 4071 words in input language, 21 words in output
2022-09-10 07:22:45,831 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:22:48,852 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:22:48,852 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:22:52,201 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:22:52,201 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/svamp/dense/models/cv_svamp_augmented_fold0.
2022-09-10 07:22:56,124 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:23:02,902 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.529126213592233	
 equation acc epoch: 0.5097087378640777	
 max val acc: 0.529126213592233	
 equation acc: 0.5097087378640777	
2022-09-10 07:23:02,902 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 6s
2022-09-10 07:23:02,904 | INFO | main_after.py: 96 : main() ::	 Experiment Name: cv_svamp_augmented
2022-09-10 07:23:02,904 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:23:02,905 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:23:03,868 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:23:03,868 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 3920
2022-09-10 07:23:03,868 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 218
2022-09-10 07:23:03,868 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: ['0.01', '12.0', '1.0', '100.0', '0.1', '0.5', '3.0', '4.0', '7.0']
2022-09-10 07:23:03,868 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:23:03,869 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:23:04,436 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 4068 / 4068 = 1.0
2022-09-10 07:23:04,598 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 4071 words in input language, 21 words in output
2022-09-10 07:23:04,609 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:23:07,554 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:23:07,554 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:23:07,633 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:23:07,633 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/svamp/dense/models/cv_svamp_augmented_fold1.
2022-09-10 07:23:11,559 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:23:19,630 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.6238532110091743	
 equation acc epoch: 0.5871559633027523	
 max val acc: 0.6238532110091743	
 equation acc: 0.5871559633027523	
2022-09-10 07:23:19,630 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 8s
2022-09-10 07:23:19,633 | INFO | main_after.py: 96 : main() ::	 Experiment Name: cv_svamp_augmented
2022-09-10 07:23:19,633 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:23:19,633 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:23:20,597 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:23:20,597 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 3911
2022-09-10 07:23:20,597 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 227
2022-09-10 07:23:20,597 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: ['0.01', '12.0', '1.0', '100.0', '0.1', '0.5', '3.0', '4.0', '7.0']
2022-09-10 07:23:20,597 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:23:20,597 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:23:21,174 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 4068 / 4068 = 1.0
2022-09-10 07:23:21,340 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 4071 words in input language, 21 words in output
2022-09-10 07:23:21,351 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:23:24,245 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:23:24,246 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:23:24,325 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:23:24,325 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/svamp/dense/models/cv_svamp_augmented_fold2.
2022-09-10 07:23:28,078 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:23:36,150 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.5462555066079295	
 equation acc epoch: 0.5242290748898678	
 max val acc: 0.5462555066079295	
 equation acc: 0.5242290748898678	
2022-09-10 07:23:36,151 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 8s
2022-09-10 07:23:36,153 | INFO | main_after.py: 96 : main() ::	 Experiment Name: cv_svamp_augmented
2022-09-10 07:23:36,153 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:23:36,153 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:23:37,119 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:23:37,119 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 3973
2022-09-10 07:23:37,119 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 165
2022-09-10 07:23:37,119 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: ['0.01', '12.0', '1.0', '100.0', '0.1', '0.5', '3.0', '4.0', '7.0']
2022-09-10 07:23:37,119 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:23:37,119 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:23:37,696 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 4068 / 4068 = 1.0
2022-09-10 07:23:37,866 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 4071 words in input language, 21 words in output
2022-09-10 07:23:37,876 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:23:40,739 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:23:40,740 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:23:40,819 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:23:40,819 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/svamp/dense/models/cv_svamp_augmented_fold3.
2022-09-10 07:23:46,628 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:23:52,814 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.5151515151515151	
 equation acc epoch: 0.46060606060606063	
 max val acc: 0.5151515151515151	
 equation acc: 0.46060606060606063	
2022-09-10 07:23:52,814 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 6s
2022-09-10 07:23:52,816 | INFO | main_after.py: 96 : main() ::	 Experiment Name: cv_svamp_augmented
2022-09-10 07:23:52,816 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:23:52,816 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:23:53,778 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:23:53,778 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 3954
2022-09-10 07:23:53,778 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 184
2022-09-10 07:23:53,778 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: ['0.01', '12.0', '1.0', '100.0', '0.1', '0.5', '3.0', '4.0', '7.0']
2022-09-10 07:23:53,778 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:23:53,778 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:23:54,356 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 4068 / 4068 = 1.0
2022-09-10 07:23:54,528 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 4071 words in input language, 21 words in output
2022-09-10 07:23:54,538 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:23:57,374 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:23:57,374 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:23:57,452 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:23:57,452 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/svamp/dense/models/cv_svamp_augmented_fold4.
2022-09-10 07:24:00,880 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:24:07,479 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.6467391304347826	
 equation acc epoch: 0.6032608695652174	
 max val acc: 0.6467391304347826	
 equation acc: 0.6032608695652174	
2022-09-10 07:24:07,479 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 6s
2022-09-10 07:24:07,479 | INFO | main_after.py: 272 : main() ::	 Final Val score: 0.573
Transfer numbers...
initialize by one_shot_gm
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.8255627984183825
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.8448367582684825
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 1.0
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.06607394748263884
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.06804741753472221
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.18401760525173616
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.1993950737847222
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.12736596001519096
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.17073143853081596
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.08910285101996529
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.09332784016927087
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.16587320963541663
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.2092742919921875
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.11851035224066842
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.16305669148763025
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.09554206000434029
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.09986029730902779
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.1524132622612847
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.19885762532552087
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.116021474202474
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.16053729587131071
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.09071689181857634
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.09701368543836808
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.14450242784288192
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.1893700493706597
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.11932966444227433
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.16605546739366317
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.08884175618489587
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.09469774034288192
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.14924960666232634
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.19859822591145837
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.12687979804144967
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.17891947428385413
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.09550136990017366
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.10113864474826384
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.1404503716362847
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.18347337510850692
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.13251664903428817
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.18968285454644096
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.09521993001302087
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.09972805447048616
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.1404791937934028
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.17466396755642366
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.12637668185763884
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.18346701727973092
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.08867221408420134
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.09379238552517366
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.17213100857204866
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.21174282497829866
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.12421247694227433
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.1759838528103299
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.09232754177517366
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.09700859917534721
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.15532938639322913
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.19263034396701384
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.12565485636393225
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.169396718343099
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.09455023871527779
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.09779188368055558
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.16399637858072913
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.19256930881076384
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.13515430026584196
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.1634936862521701
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.09219529893663192
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.09692213270399308
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.16012403700086808
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.1844024658203125
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.14425955878363717
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.16814125908745658
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.08926222059461808
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.09268018934461808
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.14255777994791663
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.16878933376736116
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.13808737860785592
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.19775517781575525
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.2958611382378472
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.45198122660319007
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.7162189483642578
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.4003448486328125
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.6210565567016602
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.43599748611450195
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.26845645904541016
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.38948774337768555
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.19554615020751953
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.08992385864257812
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.095977783203125
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.0846405029296875
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.09479331970214844
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.06884765625
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.32011985778808594
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.533203125
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.11813100179036462
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.1953125
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.26953125
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.3180825369698661
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.3381064278738839
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.3202122279575893
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.32982417515345985
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.39813995361328125
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.41699981689453125
Final sparsity level of 0.36: 0.3600000127375611
Transfer numbers...
initialize by one_shot_gm
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.8017914946616267
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.8457461900129701
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 1.0
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.07581075032552087
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.07822502983940971
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.21037122938368058
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.22827657063802087
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.14630466037326384
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.19527223375108504
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.10209316677517366
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.10727606879340279
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.1896887885199653
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.23952060275607634
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.1362215677897135
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.1865255567762587
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.10987345377604163
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.11507500542534721
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.17443339029947913
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.22699991861979163
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.13333299424913192
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.18355178833007812
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.10429212782118058
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.11126369900173616
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.16543409559461808
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.21685960557725692
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.13684887356228304
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.18984010484483504
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.10225762261284721
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.10868157280815971
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.170257568359375
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.22689649793836808
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.14588122897677946
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.20441733466254342
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.1102752685546875
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.11627705891927087
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.1604699028862847
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.20995924207899308
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.15201738145616317
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.21609157986111116
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.10925632052951384
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.11441718207465279
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.16044108072916663
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.2002716064453125
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.14493942260742188
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.2095447116427951
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.10169643825954866
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.1077728271484375
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.19688245985243058
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.24208407931857634
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.14272901746961808
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.20089848836263025
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.10623338487413192
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.11139594184027779
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.17804294162326384
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.22113037109375
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.14429261949327254
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.1936713324652778
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.10885959201388884
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.11243862575954866
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.1876373291015625
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.2204199896918403
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.15512212117513025
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.18713251749674475
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.10653856065538192
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.11152309841579866
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.1833953857421875
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.2112206353081597
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.16576215955946183
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.19255828857421875
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.10307990180121529
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.10648600260416663
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.16325039333767366
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.19366285536024308
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.15835401746961808
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.2264077928331163
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.33722432454427087
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.3974126180013021
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.6584291458129883
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.33470280965169275
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.5352602005004883
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.3563399314880371
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.2207326889038086
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.3241434097290039
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.18848896026611328
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.0922088623046875
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.10625076293945312
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.08723068237304688
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.10029983520507812
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.07763671875
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.26473045349121094
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.595703125
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.12234242757161462
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.236328125
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.23470052083333337
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.2873077392578125
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.3006199428013393
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.2896663120814732
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.30035727364676335
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.3670174734933036
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.37824903215680805
Final sparsity level of 0.36: 0.3600000335053236
Transfer numbers...
initialize by one_shot_gm
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.8058298569664113
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.8489709995136187
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 1.0
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.07376268174913192
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.07611592610677087
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.2053002251519097
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.22271558973524308
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.14248318142361116
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.19021733601888025
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.09933980305989587
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.10448031955295134
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.18503994411892366
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.2336205376519097
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.1326794094509549
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.18182033962673616
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.10710483127170134
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.11190456814236116
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.16987101236979163
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.22142367892795134
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.129815419514974
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.17892498440212679
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.10172526041666663
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.10820176866319442
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.16134982638888884
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.2117156982421875
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.13351525200737846
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.18516201443142366
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.09950595431857634
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.10597568088107634
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.16614278157552087
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.22151692708333337
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.1420461866590712
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.19932005140516496
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.10727437337239587
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.11318122016059029
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.15684339735243058
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.20412021213107634
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.14827770657009554
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.2108328077528212
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.10652669270833337
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.11151801215277779
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.15611775716145837
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.19476148817274308
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.1412527296278212
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.2045156690809462
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.09931606716579866
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.10479397243923616
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.1916588677300347
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.23623826768663192
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.13903850979275179
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.1958834330240885
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.10338507758246529
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.10835435655381942
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.17381625705295134
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.21523539225260413
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.14049106174045134
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.1890085008409288
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.10569593641493058
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.10929022894965279
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.18300374348958337
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.21483357747395837
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.1511450873480903
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.18237135145399308
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.10342237684461808
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.10840182834201384
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.17887539333767366
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.20600382486979163
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.1613506740993924
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.18760935465494788
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.10038926866319442
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.10370890299479163
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.15912882486979163
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.1891259087456597
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.1543057759602865
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.2207794189453125
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.32930501302083337
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.4151083628336588
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.6913900375366211
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.34800211588541663
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.5616731643676758
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.35900068283081055
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.19629955291748047
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.3618783950805664
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.21012401580810547
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.09787368774414062
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.10488128662109375
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.08795928955078125
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.10274887084960938
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.081298828125
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.25975990295410156
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.572265625
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.12469228108723962
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.193359375
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.21549479166666663
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.29658617292131695
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.3148869105747768
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.28778403145926335
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.30459703717912945
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.3575199672154018
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.3737771170479911
Final sparsity level of 0.36: 0.3600000265827361
Transfer numbers...
initialize by one_shot_gm
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.7685466247637521
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.8481755633916991
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 1.0
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.08019002278645837
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.08266703287760413
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.22223917643229163
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.24123806423611116
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.15463426378038192
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.20612292819552946
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.10802205403645837
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.11373053656684029
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.2003106011284722
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.25267367892795134
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.14397811889648438
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.19683583577473962
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.11639234754774308
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.12142435709635413
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.18451944986979163
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.2397376166449653
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.14095984564887154
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.19374465942382812
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.11030917697482634
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.11766391330295134
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.17483011881510413
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.22894287109375
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.1447686089409722
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.2003868950737847
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.10821872287326384
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.1149139404296875
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.18011644151475692
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.239349365234375
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.1542426215277778
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.2155999077690972
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.11630418565538192
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.12298414442274308
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.16949971516927087
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.22131517198350692
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.16084459092881942
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.22805955674913192
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.11566840277777779
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.12100389268663192
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.16920301649305558
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.21124437120225692
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.15335803561740446
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.2209710015190972
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.10767110188802087
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.11376444498697913
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.2075347900390625
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.2554033067491319
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.15082719590928817
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.21184963650173616
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.11247931586371529
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.11778428819444442
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.18821207682291663
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.23332044813368058
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.15248065524631071
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.20447582668728304
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.11512925889756942
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.1190948486328125
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.19842868381076384
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.2327813042534722
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.16391372680664062
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.1975640190972222
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.11230807834201384
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.117919921875
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.1939019097222222
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.2234954833984375
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.17515055338541663
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.20334201388888884
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.10867648654513884
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.11248948838975692
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.17212253146701384
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.20500013563368058
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.16729778713650179
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.23907089233398438
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.35587734646267366
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.4445991516113281
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.7084274291992188
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.3470071156819662
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.551788330078125
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.3928871154785156
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.2301616668701172
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.38768434524536133
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.24115848541259766
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.09352874755859375
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.10427284240722656
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.08849334716796875
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.10094833374023438
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.0869140625
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.24288177490234375
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.53125
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.12473805745442712
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.205078125
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.21256510416666663
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.30870819091796875
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.31517573765345985
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.30079977852957585
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.29853057861328125
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.3748299734933036
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.37931060791015625
Final sparsity level of 0.36: 0.3600000127375611
Transfer numbers...
initialize by one_shot_gm
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.8358196351006333
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.8474738570038911
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 1.0
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.06468709309895837
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.06661648220486116
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.17974514431423616
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.19485134548611116
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.12447187635633683
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.16674296061197913
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.08681233723958337
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.09121195475260413
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.16222127278645837
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.20416768391927087
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.11583370632595491
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.1593390570746528
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.09328884548611116
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.09758334689670134
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.1487189398871528
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.19414096408420134
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.1132833692762587
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.15687391493055558
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.08850945366753471
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.09476725260416663
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.14110989040798616
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.1852654351128472
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.11659113566080725
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.16234588623046875
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.08695136176215279
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.09251573350694442
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.14531283908420134
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.19464450412326384
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.12408447265625
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.1748483445909288
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.09351264105902779
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.09900410970052087
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.13737148708767366
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.17889743381076384
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.1295560201009115
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.18529552883572054
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.09295654296875
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.09733751085069442
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.1371070014105903
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.1705169677734375
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.12334399753146696
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.17930052015516496
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.08670891655815971
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.09157477484809029
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.16872660319010413
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.20722791883680558
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.12124506632486975
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.17183515760633683
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.09040662977430558
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.0946807861328125
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.1519317626953125
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.18840535481770837
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.12270991007486975
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.16562143961588538
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.09230380588107634
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.09534200032552087
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.16012403700086808
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.18790181477864587
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.13197920057508683
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.15976460774739587
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.0901641845703125
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.09477742513020837
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.15676710340711808
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.1800316704644097
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.14100816514756942
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.16405868530273438
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.0873565673828125
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.09046596950954866
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.1391754150390625
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.16511705186631942
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.13491694132486975
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.193200429280599
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.2891404893663194
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.4150460561116537
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.6707572937011719
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.37423642476399743
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.5951929092407227
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.38305187225341797
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.24114513397216797
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.34860897064208984
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.1801900863647461
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.0875244140625
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.10287284851074219
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.08212661743164062
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.09748268127441406
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.065673828125
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.3995838165283203
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.66015625
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.14061482747395837
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.2421875
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.37239583333333337
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.36529432024274555
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.3757651192801339
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.37045615059988835
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.37036459786551335
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.44143567766462055
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.46140071323939735
Final sparsity level of 0.36: 0.3600000127375611
wandb: WARNING W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.
2022-09-10 07:24:10,485 | INFO | main_after.py: 96 : main() ::	 Experiment Name: cv_svamp_augmented
2022-09-10 07:24:10,485 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:24:10,485 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:24:11,519 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:24:11,519 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 3932
2022-09-10 07:24:11,519 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 206
2022-09-10 07:24:11,519 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: ['0.01', '12.0', '1.0', '100.0', '0.1', '0.5', '3.0', '4.0', '7.0']
2022-09-10 07:24:11,520 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:24:11,520 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:24:12,137 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 4068 / 4068 = 1.0
2022-09-10 07:24:12,302 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 4071 words in input language, 21 words in output
2022-09-10 07:24:12,311 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:24:15,366 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:24:15,366 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:24:18,681 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:24:18,681 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/svamp/dense/models/cv_svamp_augmented_fold0.
2022-09-10 07:24:22,846 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:24:29,313 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.35436893203883496	
 equation acc epoch: 0.33495145631067963	
 max val acc: 0.35436893203883496	
 equation acc: 0.33495145631067963	
2022-09-10 07:24:29,313 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 6s
2022-09-10 07:24:29,315 | INFO | main_after.py: 96 : main() ::	 Experiment Name: cv_svamp_augmented
2022-09-10 07:24:29,315 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:24:29,315 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:24:30,265 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:24:30,265 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 3920
2022-09-10 07:24:30,266 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 218
2022-09-10 07:24:30,266 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: ['0.01', '12.0', '1.0', '100.0', '0.1', '0.5', '3.0', '4.0', '7.0']
2022-09-10 07:24:30,266 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:24:30,266 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:24:30,918 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 4068 / 4068 = 1.0
2022-09-10 07:24:31,082 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 4071 words in input language, 21 words in output
2022-09-10 07:24:31,094 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:24:34,060 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:24:34,060 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:24:34,138 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:24:34,138 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/svamp/dense/models/cv_svamp_augmented_fold1.
2022-09-10 07:24:39,864 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:24:47,304 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.44954128440366975	
 equation acc epoch: 0.42201834862385323	
 max val acc: 0.44954128440366975	
 equation acc: 0.42201834862385323	
2022-09-10 07:24:47,304 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 7s
2022-09-10 07:24:47,307 | INFO | main_after.py: 96 : main() ::	 Experiment Name: cv_svamp_augmented
2022-09-10 07:24:47,307 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:24:47,307 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:24:48,262 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:24:48,262 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 3911
2022-09-10 07:24:48,262 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 227
2022-09-10 07:24:48,262 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: ['0.01', '12.0', '1.0', '100.0', '0.1', '0.5', '3.0', '4.0', '7.0']
2022-09-10 07:24:48,262 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:24:48,262 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:24:48,878 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 4068 / 4068 = 1.0
2022-09-10 07:24:49,041 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 4071 words in input language, 21 words in output
2022-09-10 07:24:49,052 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:24:51,908 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:24:51,908 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:24:51,988 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:24:51,988 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/svamp/dense/models/cv_svamp_augmented_fold2.
2022-09-10 07:24:57,567 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:25:04,800 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.33480176211453744	
 equation acc epoch: 0.2907488986784141	
 max val acc: 0.33480176211453744	
 equation acc: 0.2907488986784141	
2022-09-10 07:25:04,800 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 7s
2022-09-10 07:25:04,802 | INFO | main_after.py: 96 : main() ::	 Experiment Name: cv_svamp_augmented
2022-09-10 07:25:04,802 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:25:04,802 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:25:05,866 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:25:05,866 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 3973
2022-09-10 07:25:05,866 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 165
2022-09-10 07:25:05,866 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: ['0.01', '12.0', '1.0', '100.0', '0.1', '0.5', '3.0', '4.0', '7.0']
2022-09-10 07:25:05,866 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:25:05,866 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:25:06,484 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 4068 / 4068 = 1.0
2022-09-10 07:25:06,655 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 4071 words in input language, 21 words in output
2022-09-10 07:25:06,664 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:25:09,511 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:25:09,511 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:25:09,589 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:25:09,590 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/svamp/dense/models/cv_svamp_augmented_fold3.
2022-09-10 07:25:12,958 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:25:18,429 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.28484848484848485	
 equation acc epoch: 0.2727272727272727	
 max val acc: 0.28484848484848485	
 equation acc: 0.2727272727272727	
2022-09-10 07:25:18,429 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 5s
2022-09-10 07:25:18,431 | INFO | main_after.py: 96 : main() ::	 Experiment Name: cv_svamp_augmented
2022-09-10 07:25:18,431 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:25:18,431 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:25:19,469 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:25:19,469 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 3954
2022-09-10 07:25:19,469 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 184
2022-09-10 07:25:19,469 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: ['0.01', '12.0', '1.0', '100.0', '0.1', '0.5', '3.0', '4.0', '7.0']
2022-09-10 07:25:19,469 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:25:19,469 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:25:20,088 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 4068 / 4068 = 1.0
2022-09-10 07:25:20,257 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 4071 words in input language, 21 words in output
2022-09-10 07:25:20,267 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:25:23,113 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:25:23,113 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:25:23,192 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:25:23,192 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/svamp/dense/models/cv_svamp_augmented_fold4.
2022-09-10 07:25:29,047 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:25:35,801 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.44565217391304346	
 equation acc epoch: 0.41847826086956524	
 max val acc: 0.44565217391304346	
 equation acc: 0.41847826086956524	
2022-09-10 07:25:35,801 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 6s
2022-09-10 07:25:35,801 | INFO | main_after.py: 272 : main() ::	 Final Val score: 0.376
Transfer numbers...
initialize by one_shot_gm
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.8473967399864054
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.8741462994487679
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 1.0
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.14593844943576384
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.14968702528211808
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.38739522298177087
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.417633056640625
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.27615017361111116
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.3600968254937066
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.1958092583550347
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.20549519856770837
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.35365634494357634
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.43311564127604163
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.25763744778103304
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.3442213270399306
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.21068996853298616
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.21992153591579866
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.32718404134114587
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.41275533040364587
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.25307464599609375
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.3376922607421875
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.1999749077690972
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.21216159396701384
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.31199985080295134
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.3926984998914931
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.2599317762586806
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.3481797112358941
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.1961517333984375
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.2080925835503472
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.3189561631944444
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.40676371256510413
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.2766380310058594
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.3718193901909722
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.21033901638454866
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.2223273383246528
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.29888916015625
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.38219197591145837
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.28749169243706596
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.3913709852430556
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.20939297146267366
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.21917555067274308
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.29870944552951384
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.36693318684895837
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.27503628200954866
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.3812565273708768
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.19501749674479163
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.2062767876519097
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.36276753743489587
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.43689982096354163
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.2708430820041232
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.3681598239474826
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.20401509602864587
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.214141845703125
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.33230082194010413
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.40726216634114587
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.27395078870985246
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.35701921251085067
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.20810953776041663
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.21551513671875
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.35003662109375
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.40885586208767366
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.2943005032009549
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.34933598836263025
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.20365397135416663
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.21396382649739587
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.3429243299696181
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.39505343967013884
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.31367619832356775
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.3600425720214844
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.19874911838107634
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.2046356201171875
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.3048180474175347
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.3650733100043403
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.30030949910481775
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.4196357727050781
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.6025068495008681
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.5974617004394531
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.8300991058349609
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.5346190134684246
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.7510271072387695
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.6172823905944824
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.5090436935424805
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.5760774612426758
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.4008626937866211
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.20052719116210938
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.2112865447998047
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.18691253662109375
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.20923614501953125
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.146484375
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.5604419708251953
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.615234375
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.245642344156901
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.251953125
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.44661458333333337
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.517897469656808
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.5393763950892857
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.5120435442243303
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.5208990914481026
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.6190697806222099
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.6365770612444197
Final sparsity level of 0.488: 0.4880000198816714
Transfer numbers...
initialize by one_shot_gm
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.8261842184339666
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.8761095573929961
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 1.0
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.1557244194878472
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.15980190700954866
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.41002400716145837
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.4414689805772569
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.2938190036349826
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.3813023037380643
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.2090233696831597
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.21928744845920134
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.37456258138020837
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.4568549262152778
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.27402538723415804
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.3647783067491319
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.22452290852864587
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.2345191107855903
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.347076416015625
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.4357994927300347
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.2693595886230469
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.35765372382269967
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.21365017361111116
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.22610304090711808
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.3309309217664931
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.414825439453125
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.27669101291232634
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.3685213724772135
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.20916748046875
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.22193569607204866
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.3380872938368056
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.42871432834201384
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.2941737704806857
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.39300240410698783
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.2240668402777778
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.2369774712456597
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.3173455132378472
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.4030897352430556
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.3056144714355469
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.41302363077799475
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.22318691677517366
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.23391554090711808
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.3169165717230903
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.38795810275607634
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.29249021742078996
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.4024942186143663
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.20840793185763884
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.2198706732855903
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.3840959337022569
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.4602220323350694
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.2881384955512153
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.3892381456163194
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.21794467502170134
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.22816806369357634
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.35296969943576384
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.4304283989800347
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.29140048556857634
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.3779123094346788
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.22223409016927087
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.229949951171875
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.3708936903211806
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.4325171576605903
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.3129865858289931
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.3700972663031684
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.21719699435763884
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.2282782660590278
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.36371358235677087
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.41862148708767366
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.3333112928602431
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.3814875284830729
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.21211412217881942
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.21834988064236116
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.3238355848524306
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.3877936469184028
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.31952836778428817
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.4438518948025174
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.6335940890842013
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.5765171051025391
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.8048067092895508
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.4894231160481771
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.691314697265625
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.5511913299560547
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.4215078353881836
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.5186667442321777
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.36740875244140625
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.18921661376953125
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.21643638610839844
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.17905426025390625
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.20522308349609375
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.166748046875
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.4790840148925781
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.63671875
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.24219894409179688
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.298828125
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.38248697916666663
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.4907967703683036
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.5049231392996651
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.48776354108537945
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.4968327113560268
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.5876813616071428
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.594677516392299
Final sparsity level of 0.488: 0.4880000198816714
Transfer numbers...
initialize by one_shot_gm
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.8296122477950197
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.8773331104085603
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 1.0
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.1530676947699653
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.15732998318142366
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.40474955240885413
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.43576558430989587
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.28949313693576384
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.37619993421766496
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.20566134982638884
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.21591525607638884
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.3696272108289931
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.45139058430989587
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.27018144395616317
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.35981157090928817
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.2213219536675347
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.23131137424045134
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.34229024251302087
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.43060811360677087
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.26541264851888025
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.3526085747612847
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.21008470323350692
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.2228173149956597
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.32617526584201384
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.4094102647569444
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.27263641357421875
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.3636216057671441
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.20614454481336808
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.21850246853298616
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.3337622748480903
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.42355007595486116
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.29001829359266496
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.3878635830349393
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.220977783203125
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.23359510633680558
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.3130018446180556
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.3981645372178819
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.3011453416612413
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.40781148274739587
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.21989610460069442
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.23023478190104163
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.3123389350043403
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.38299560546875
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.2882648044162326
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.39752197265625
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.20503404405381942
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.21646457248263884
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.37922159830729163
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.45509677463107634
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.2838177151150174
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.3841141594780816
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.21462843153211808
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.22473314073350692
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.3478783501519097
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.42470296223958337
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.28708394368489587
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.37285656399197054
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.21858723958333337
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.22624037000868058
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.3665008544921875
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.4267900254991319
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.3085454305013021
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.36511145697699654
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.2139366997612847
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.22462972005208337
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.35879177517361116
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.41296725802951384
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.3284916347927518
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.3763817681206597
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.2087317572699653
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.2149895562065972
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.31930881076388884
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.38225979275173616
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.31471803453233504
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.4381739298502604
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.6263715955946181
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.5970465342203777
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.8279609680175781
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.5025774637858074
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.7110118865966797
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.5565733909606934
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.38842105865478516
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.5583782196044922
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.4077110290527344
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.20281982421875
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.2170886993408203
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.18347549438476562
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.21148109436035156
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.167236328125
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.4788532257080078
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.62890625
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.24746195475260413
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.291015625
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.36751302083333337
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.5044893537248885
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.5284173148018974
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.4889984130859375
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.5088261195591518
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.5843712942940849
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.5958012172154018
Final sparsity level of 0.488: 0.4880000129590839
Transfer numbers...
initialize by one_shot_gm
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.7962895352548162
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.8780424165045395
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 1.0
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.16057840983072913
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.1645270453559028
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.42156643337673616
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.453857421875
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.30277421739366317
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.39205423990885413
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.21557278103298616
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.22600131564670134
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.38538614908854163
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.46939425998263884
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.2823524475097656
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.37509028116861975
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.2319268120659722
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.2419213189019097
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.35736592610677087
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.44771152072482634
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.2775247361924913
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.36753548516167533
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.22036912706163192
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.23330179850260413
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.34071011013454866
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.4259270562065972
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.28506130642361116
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.37877231174045134
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.21562364366319442
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.2288903130425347
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.34836154513888884
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.4397515190972222
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.3031154208713107
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.4036513434516059
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.2309502495659722
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.2445899115668403
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.32673305935329866
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.41402180989583337
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.31476635403103304
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.4240218268500434
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.23038228352864587
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.240875244140625
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.3261328803168403
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.39888339572482634
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.30137125651041663
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.4133872985839844
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.21485561794704866
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.2266693115234375
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.39494662814670134
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.47242397732204866
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.29671690199110246
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.39985360039605033
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.2247314453125
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.23545498318142366
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.36282009548611116
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.44256083170572913
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.30016326904296875
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.3884273105197482
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.22903272840711808
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.23671298556857634
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.38206142849392366
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.44525146484375
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.32252841525607634
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.3808051215277778
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.22401767306857634
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.23529052734375
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.3741472032335069
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.4304640028211806
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.34336090087890625
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.3925285339355469
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.2186804877387153
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.22493998209635413
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.33380635579427087
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.3992394341362847
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.32899814181857634
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.4563471476236979
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.6494428846571181
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.6072578430175781
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.8303422927856445
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.49138895670572913
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.6987466812133789
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.5724925994873047
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.42440032958984375
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.5724225044250488
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.4355182647705078
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.18689346313476562
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.20946311950683594
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.17844390869140625
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.201904296875
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.174072265625
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.4362010955810547
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.59375
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.24104436238606775
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.296875
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.34798177083333337
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.5084032331194197
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.5185198102678572
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.49472372872488835
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.4943520682198661
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.5971766880580357
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.6004736764090401
Final sparsity level of 0.488: 0.4880000129590839
Transfer numbers...
initialize by one_shot_gm
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.8591219401754037
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.8757219722762646
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 1.0
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.1428154839409722
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.1469353569878472
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.38088480631510413
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.41051737467447913
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.2710533142089844
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.3537928263346354
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.19199625651041663
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.2014092339409722
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.34695773654513884
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.4258049858940972
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.25273429022894967
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.3380974663628472
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.20642598470052087
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.21588304307725692
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.321014404296875
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.40591939290364587
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.24833848741319442
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.33167521158854163
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.19604661729600692
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.20798916286892366
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.30595737033420134
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.38620334201388884
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.2551659478081597
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.3422266642252604
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.19256930881076384
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.20384046766493058
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.31286282009548616
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.4002397325303819
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.2712749905056424
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.36562135484483504
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.2063208685980903
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.21847195095486116
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.2937605116102431
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.37528652615017366
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.28211466471354163
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.38453589545355904
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.2052680121527778
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.21512349446614587
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.29317898220486116
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.36040751139322913
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.2698317633734809
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.37461090087890625
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.19114515516493058
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.20205349392361116
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.3560350206163194
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.4296332465277778
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.26551098293728304
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.36185412936740446
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.20034620496961808
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.20958116319444442
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.3262498643663194
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.3997887505425347
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.2688331604003906
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.3509474860297309
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.20406087239583337
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.21129353841145837
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.34354824490017366
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.4011908637152778
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.2887857225206163
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.3430832756890191
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.1997155083550347
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.20980665418836808
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.33652750651041663
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.38753933376736116
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.30758327907986116
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.35347323947482634
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.19487508138020837
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.20067850748697913
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.29933675130208337
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.35795932345920134
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.2946934170193143
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.4124340481228299
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.592742919921875
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.5779151916503906
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.8132123947143555
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.5157674153645833
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.7377414703369141
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.5856432914733887
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.47748279571533203
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.5498743057250977
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.373291015625
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.19353103637695312
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.22544479370117188
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.18137741088867188
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.21503448486328125
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.149658203125
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.6285648345947266
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.669921875
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.2791633605957031
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.27734375
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.5032552083333333
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.5539834158761161
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.5752846854073661
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.5529708862304688
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.5630144391741072
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.6714848109654018
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.687150137765067
Final sparsity level of 0.488: 0.48800004757202153
wandb: WARNING W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.
2022-09-10 07:25:38,826 | INFO | main_after.py: 96 : main() ::	 Experiment Name: cv_svamp_augmented
2022-09-10 07:25:38,826 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:25:38,826 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:25:39,855 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:25:39,856 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 3932
2022-09-10 07:25:39,856 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 206
2022-09-10 07:25:39,856 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: ['0.01', '12.0', '1.0', '100.0', '0.1', '0.5', '3.0', '4.0', '7.0']
2022-09-10 07:25:39,856 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:25:39,856 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:25:40,437 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 4068 / 4068 = 1.0
2022-09-10 07:25:40,604 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 4071 words in input language, 21 words in output
2022-09-10 07:25:40,613 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:25:43,635 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:25:43,635 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:25:46,951 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:25:46,951 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/svamp/dense/models/cv_svamp_augmented_fold0.
2022-09-10 07:25:53,474 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:25:59,824 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.12135922330097088	
 equation acc epoch: 0.11650485436893204	
 max val acc: 0.12135922330097088	
 equation acc: 0.11650485436893204	
2022-09-10 07:25:59,824 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 6s
2022-09-10 07:25:59,826 | INFO | main_after.py: 96 : main() ::	 Experiment Name: cv_svamp_augmented
2022-09-10 07:25:59,826 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:25:59,826 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:26:00,773 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:26:00,773 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 3920
2022-09-10 07:26:00,773 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 218
2022-09-10 07:26:00,773 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: ['0.01', '12.0', '1.0', '100.0', '0.1', '0.5', '3.0', '4.0', '7.0']
2022-09-10 07:26:00,773 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:26:00,773 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:26:01,349 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 4068 / 4068 = 1.0
2022-09-10 07:26:01,517 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 4071 words in input language, 21 words in output
2022-09-10 07:26:01,531 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:26:04,494 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:26:04,494 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:26:04,572 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:26:04,572 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/svamp/dense/models/cv_svamp_augmented_fold1.
2022-09-10 07:26:09,831 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:26:16,288 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.06422018348623854	
 equation acc epoch: 0.045871559633027525	
 max val acc: 0.06422018348623854	
 equation acc: 0.045871559633027525	
2022-09-10 07:26:16,289 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 6s
2022-09-10 07:26:16,291 | INFO | main_after.py: 96 : main() ::	 Experiment Name: cv_svamp_augmented
2022-09-10 07:26:16,291 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:26:16,291 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:26:17,239 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:26:17,239 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 3911
2022-09-10 07:26:17,239 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 227
2022-09-10 07:26:17,239 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: ['0.01', '12.0', '1.0', '100.0', '0.1', '0.5', '3.0', '4.0', '7.0']
2022-09-10 07:26:17,239 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:26:17,239 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:26:17,817 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 4068 / 4068 = 1.0
2022-09-10 07:26:17,984 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 4071 words in input language, 21 words in output
2022-09-10 07:26:17,995 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:26:20,894 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:26:20,894 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:26:20,981 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:26:20,981 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/svamp/dense/models/cv_svamp_augmented_fold2.
2022-09-10 07:26:26,473 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:26:33,038 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.05726872246696035	
 equation acc epoch: 0.022026431718061675	
 max val acc: 0.05726872246696035	
 equation acc: 0.022026431718061675	
2022-09-10 07:26:33,038 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 6s
2022-09-10 07:26:33,041 | INFO | main_after.py: 96 : main() ::	 Experiment Name: cv_svamp_augmented
2022-09-10 07:26:33,041 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:26:33,041 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:26:34,091 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:26:34,091 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 3973
2022-09-10 07:26:34,091 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 165
2022-09-10 07:26:34,091 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: ['0.01', '12.0', '1.0', '100.0', '0.1', '0.5', '3.0', '4.0', '7.0']
2022-09-10 07:26:34,091 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:26:34,091 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:26:34,675 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 4068 / 4068 = 1.0
2022-09-10 07:26:34,849 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 4071 words in input language, 21 words in output
2022-09-10 07:26:34,858 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:26:37,739 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:26:37,739 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:26:37,818 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:26:37,818 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/svamp/dense/models/cv_svamp_augmented_fold3.
2022-09-10 07:26:41,029 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:26:46,140 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.05454545454545454	
 equation acc epoch: 0.030303030303030304	
 max val acc: 0.05454545454545454	
 equation acc: 0.030303030303030304	
2022-09-10 07:26:46,140 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 5s
2022-09-10 07:26:46,142 | INFO | main_after.py: 96 : main() ::	 Experiment Name: cv_svamp_augmented
2022-09-10 07:26:46,142 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:26:46,142 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:26:47,180 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:26:47,180 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 3954
2022-09-10 07:26:47,180 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 184
2022-09-10 07:26:47,180 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: ['0.01', '12.0', '1.0', '100.0', '0.1', '0.5', '3.0', '4.0', '7.0']
2022-09-10 07:26:47,180 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:26:47,180 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:26:47,765 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 4068 / 4068 = 1.0
2022-09-10 07:26:47,937 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 4071 words in input language, 21 words in output
2022-09-10 07:26:47,947 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:26:50,785 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:26:50,785 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:26:50,866 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:26:50,866 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/svamp/dense/models/cv_svamp_augmented_fold4.
2022-09-10 07:26:54,180 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:27:01,402 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.059782608695652176	
 equation acc epoch: 0.043478260869565216	
 max val acc: 0.059782608695652176	
 equation acc: 0.043478260869565216	
2022-09-10 07:27:01,402 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 7s
2022-09-10 07:27:01,402 | INFO | main_after.py: 272 : main() ::	 Final Val score: 0.072
Transfer numbers...
initialize by one_shot_gm
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.8811615883732883
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.8989948119325551
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 1.0
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.21808200412326384
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.22344801161024308
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5449998643663194
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5798560248480903
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.401879628499349
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.508499993218316
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.29098002115885413
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.3049757215711806
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5034823947482638
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.595428466796875
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.37570741441514754
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.4875297546386719
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.3131934271918403
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.3255462646484375
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.46960788302951384
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5696394178602431
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.3697424994574653
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.47748989529079866
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.29732259114583337
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.3141038682725694
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.4487389458550347
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5411461724175347
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.3796348571777344
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.48957019382052946
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.2908393012152778
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.30845981174045134
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.45697021484375
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5531768798828125
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.4023963080512153
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5174933539496528
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.3100958930121528
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.32770792643229163
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.4296434190538194
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5266181098090278
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.416815439860026
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5411109924316406
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.309814453125
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.324432373046875
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.42949930826822913
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5126224093967013
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.4003948635525174
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5293939378526475
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.28997802734375
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.30543178982204866
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5118950737847222
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5955827501085069
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.395086924235026
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5151935153537326
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.3033447265625
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.3174421522352431
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.4769134521484375
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5675286187065972
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.39952384101019967
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5033035278320312
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.3089616563585069
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.3199632432725694
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5004408094618056
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5744527180989583
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.42873552110460067
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.49718899197048616
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.30318705240885413
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.31756083170572913
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.4914109971788194
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5591990152994792
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.4561152988009982
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5128970675998263
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.2962799072265625
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.30474344889322913
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.4413892957899306
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5236985948350694
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.4382061428493924
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5881979200575087
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.7982194688585069
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.6683413187662761
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.8752956390380859
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.6018352508544922
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.8042058944702148
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.7053003311157227
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.6587123870849609
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.6647725105285645
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.5486621856689453
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.29791259765625
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.31143760681152344
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.2780494689941406
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.3101692199707031
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.2158203125
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.6765785217285156
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.62890625
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.3452644348144531
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.29296875
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.5315755208333333
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.6434696742466518
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.6655524117606026
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.63653564453125
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.6451742989676339
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.7530168805803572
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.7711083548409599
Final sparsity level of 0.59: 0.5900000135682715
Transfer numbers...
initialize by one_shot_gm
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.8456807306691203
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.9015812459468223
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 1.0
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.23101806640625
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.23645528157552087
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5707906087239583
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6053331163194444
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.42350641886393225
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5330272250705295
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.3077545166015625
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.32253519694010413
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5282067192925347
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6206681993272569
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.3962906731499566
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5113466050889757
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.33118862575954866
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.3442840576171875
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.49347262912326384
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5944112141927083
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.3901540968153212
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5008218553331163
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.31467692057291663
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.33241611056857634
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.47229512532552087
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5642327202690972
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.40034145779079866
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.512955559624566
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.30814785427517366
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.3263770209418403
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.48073154025607634
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5762719048394097
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.42417102389865446
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5414979722764757
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.32803683810763884
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.3460218641493056
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.45193990071614587
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5496334499782987
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.438995361328125
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5652783711751301
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.32784525553385413
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.3432244194878472
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.4520348442925347
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.536651611328125
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.42207251654730904
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5534112718370225
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.30684238009982634
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.3230912950303819
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5368771023220487
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6203630235460069
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.4166959126790365
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5393197801378038
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.32108730740017366
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.3362104627821181
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5010189480251737
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5928802490234375
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.4213405185275607
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.527656979031033
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.32696024576822913
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.33836703830295134
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5256907145182292
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6005927191840278
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.45193396674262154
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5219273037380643
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.3208177354600694
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.33608500162760413
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5168050130208333
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5859002007378472
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.48057217068142366
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5384381612141926
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.3138512505425347
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.32251485188802087
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.46486409505208337
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5502353244357638
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.462066650390625
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6150762769911025
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.8245747884114584
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.6667842864990234
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.8663644790649414
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.5725790659586589
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.7650814056396484
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.6587371826171875
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.5729522705078125
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.6285324096679688
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.5111093521118164
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.2801399230957031
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.3189105987548828
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.26461029052734375
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.3041267395019531
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.240478515625
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.6136112213134766
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.650390625
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.34374491373697913
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.326171875
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.4541015625
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.6290043422154018
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.6432102748325893
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.6231743948800224
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.631847926548549
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.7296949114118303
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.7364872523716518
Final sparsity level of 0.59: 0.5900000412586217
Transfer numbers...
initialize by one_shot_gm
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.848616421507676
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.9018067039559015
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 1.0
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.22810702853732634
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.23368326822916663
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5653856065538194
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6002722846137153
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.41909874810112846
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5280825297037761
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.3042670355902778
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.31870693630642366
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5231882731119792
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6157192654079862
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.3919915093315972
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5066218905978732
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.32768588595920134
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.3402642144097222
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.4888526068793403
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5895046657986112
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.3860253228081597
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.4960810343424479
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.31102498372395837
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.3285963270399306
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.4675920274522569
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5597974989149306
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.39610883924696183
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5081901550292969
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.30443996853298616
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.3227606879340278
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.4761962890625
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5718960232204862
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.41971800062391496
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5367664761013455
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.3244866265190972
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.34235805935329866
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.4473588731553819
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5452796088324653
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.43461057874891496
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5605367024739583
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.3238847520616319
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.33932834201388884
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.4477471245659722
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5316687689887153
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.41767374674479163
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5485928853352864
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.30333455403645837
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.3193901909722222
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5316145155164931
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6155310736762153
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.4123772515190972
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5346594916449653
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.31749301486545134
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.3321550157335069
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.4960191514756944
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5878499348958333
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.41705873277452254
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5228051079644097
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.3233506944444444
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.3345269097222222
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5207790798611112
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5954352484809028
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.4473334418402778
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5171551174587674
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.31701321072048616
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.33211771647135413
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5118832058376737
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5806427001953125
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.47563934326171875
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5333345201280382
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.3101942274305556
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.319000244140625
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.4604254828559028
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5452016194661458
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.45731777615017366
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6097056070963542
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.8196987575954862
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.6880639394124348
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.8847265243530273
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.5842933654785156
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.7807321548461914
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.6636161804199219
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.540156364440918
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.6650924682617188
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.5580034255981445
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.2986793518066406
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.3205680847167969
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.2737312316894531
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.31179237365722656
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.248046875
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.6179771423339844
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.646484375
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.35086568196614587
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.333984375
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.44596354166666663
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.644453866141183
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.670576913016183
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.6276452200753349
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.646130153111049
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.7318812779017857
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.7415902273995536
Final sparsity level of 0.59: 0.5900000274134466
Transfer numbers...
initialize by one_shot_gm
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.8187580562601545
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.9031391861219196
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 1.0
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.2360009087456597
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.24183315700954866
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5814887152777778
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6161719428168403
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.43244171142578125
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5431819491916232
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.31483968098958337
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.3295813666449653
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5387658013237847
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6311069064670138
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.40454906887478304
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.521225823296441
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.33878580729166663
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.3518659803602431
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5037282307942708
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6043802897135417
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.39856508043077254
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5104111565483941
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.3214348687065972
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.3400099012586806
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.4821709526909722
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5740593804253472
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.4090538024902344
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.522534688313802
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.3148668077256944
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.3332655164930556
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.4909498426649306
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5857679578993056
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.4330465528700087
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5513793097601997
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.33523220486111116
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.3537767198350694
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.4617936876085069
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5591617160373263
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.4482447306315104
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5754686991373699
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.3352576361762153
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.35077243381076384
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.46151224772135413
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5464613172743056
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.4310256110297309
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5632625155978732
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.31375630696614587
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.330078125
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5470920138888888
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6303320990668403
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.4256413777669271
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5493837992350261
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.3284081353081597
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.3434007432725694
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5112999810112847
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6035274929470487
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.43028386433919275
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5376947191026475
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.33429463704427087
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.3461456298828125
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5363193088107638
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6119486490885417
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.4615300496419271
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5322736104329426
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.3279656304253472
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.34323628743489587
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.527191162109375
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5972069634331597
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.4906959533691406
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5490650600857205
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.3211279975043403
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.32986789279513884
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.47478739420572913
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5615827772352431
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.4720848931206597
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6262516445583768
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.8354000515407987
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.6878903706868489
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.881648063659668
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.5725415547688801
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.7699432373046875
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.6721186637878418
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.5688600540161133
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.6737504005432129
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.5784673690795898
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.2751731872558594
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.3058662414550781
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.2626075744628906
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.2967643737792969
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.253173828125
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.5673770904541016
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.615234375
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.3407808939615885
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.357421875
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.4287109375
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.6430010114397322
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.6551949637276786
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.6278948102678572
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.6277433122907365
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.7390038626534599
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.7425035749162947
Final sparsity level of 0.59: 0.5900000135682715
Transfer numbers...
initialize by one_shot_gm
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.8930872366043304
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.9002031655317769
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 1.0
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.2148810492621528
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.22044203016493058
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5385555691189237
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.57330322265625
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.396453857421875
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5022078620062934
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.2866075303819444
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.3008287217881944
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.4970567491319444
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5890096028645833
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.37050671047634554
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.4813321431477865
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.3087751600477431
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.3207533094618056
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.46329243977864587
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5632103814019097
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.3646829393174913
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.47131644354926217
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.2929009331597222
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.3096177842881944
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.44270833333333337
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5348256429036458
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.374518076578776
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.4836082458496094
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.2866346571180556
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.3040686713324653
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.45117526584201384
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5475311279296875
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.39696502685546875
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5115627712673612
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.30595737033420134
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.32297770182291663
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.4237348768446181
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5204637315538194
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.41094674004448783
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5347781711154513
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.3053453233506944
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.31953091091579866
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.4237060546875
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5065460205078125
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.3947838677300347
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.52308103773329
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.2855394151475694
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.30083719889322913
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5052880181206597
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.588958740234375
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.3895857069227431
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5090755886501737
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.299041748046875
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.3129967583550347
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.4705030653211806
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5606129964192708
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.3940014309353299
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.4971750047471788
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.3045060899522569
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.31536865234375
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.4942389594184028
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5672030978732638
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.42296557956271696
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.4909358554416232
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.29847208658854163
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.3127831353081597
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.484771728515625
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5519510904947917
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.44976679484049475
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.506177266438802
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.29164632161458337
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.2999284532335069
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.4354078504774306
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.5172373453776042
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.4321878221299913
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5810398525661893
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.7907562255859375
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.6502469380696614
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.8630132675170898
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.5836633046468098
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.7926692962646484
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.6780481338500977
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.6317453384399414
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.641688346862793
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.5169820785522461
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.289276123046875
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.3322486877441406
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.2721900939941406
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.3190784454345703
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.228271484375
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.7135429382324219
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.673828125
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.3776893615722656
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.306640625
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.5589192708333333
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.6757191249302456
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.7005266462053572
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.6752689906529018
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.6899130684988839
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.8008510044642857
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.816864013671875
Final sparsity level of 0.59: 0.5900000551037967
wandb: WARNING W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.
2022-09-10 07:27:04,454 | INFO | main_after.py: 96 : main() ::	 Experiment Name: cv_svamp_augmented
2022-09-10 07:27:04,454 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:27:04,454 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:27:05,483 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:27:05,483 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 3932
2022-09-10 07:27:05,483 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 206
2022-09-10 07:27:05,483 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: ['0.01', '12.0', '1.0', '100.0', '0.1', '0.5', '3.0', '4.0', '7.0']
2022-09-10 07:27:05,483 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:27:05,483 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:27:06,062 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 4068 / 4068 = 1.0
2022-09-10 07:27:06,232 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 4071 words in input language, 21 words in output
2022-09-10 07:27:06,241 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:27:09,242 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:27:09,242 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:27:12,629 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:27:12,629 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/svamp/dense/models/cv_svamp_augmented_fold0.
2022-09-10 07:27:17,617 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:27:24,323 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.043689320388349516	
 equation acc epoch: 0.038834951456310676	
 max val acc: 0.043689320388349516	
 equation acc: 0.038834951456310676	
2022-09-10 07:27:24,323 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 6s
2022-09-10 07:27:24,325 | INFO | main_after.py: 96 : main() ::	 Experiment Name: cv_svamp_augmented
2022-09-10 07:27:24,325 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:27:24,325 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:27:25,269 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:27:25,269 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 3920
2022-09-10 07:27:25,269 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 218
2022-09-10 07:27:25,269 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: ['0.01', '12.0', '1.0', '100.0', '0.1', '0.5', '3.0', '4.0', '7.0']
2022-09-10 07:27:25,270 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:27:25,270 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:27:25,842 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 4068 / 4068 = 1.0
2022-09-10 07:27:26,007 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 4071 words in input language, 21 words in output
2022-09-10 07:27:26,020 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:27:29,011 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:27:29,011 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:27:29,090 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:27:29,090 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/svamp/dense/models/cv_svamp_augmented_fold1.
2022-09-10 07:27:33,654 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:27:42,841 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.027522935779816515	
 equation acc epoch: 0.0045871559633027525	
 max val acc: 0.027522935779816515	
 equation acc: 0.0045871559633027525	
2022-09-10 07:27:42,841 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 9s
2022-09-10 07:27:42,843 | INFO | main_after.py: 96 : main() ::	 Experiment Name: cv_svamp_augmented
2022-09-10 07:27:42,843 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:27:42,843 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:27:43,788 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:27:43,788 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 3911
2022-09-10 07:27:43,788 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 227
2022-09-10 07:27:43,788 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: ['0.01', '12.0', '1.0', '100.0', '0.1', '0.5', '3.0', '4.0', '7.0']
2022-09-10 07:27:43,788 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:27:43,789 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:27:44,367 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 4068 / 4068 = 1.0
2022-09-10 07:27:44,534 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 4071 words in input language, 21 words in output
2022-09-10 07:27:44,546 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:27:47,379 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:27:47,379 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:27:47,459 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:27:47,460 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/svamp/dense/models/cv_svamp_augmented_fold2.
2022-09-10 07:27:51,573 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:27:58,421 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.022026431718061675	
 equation acc epoch: 0.0	
 max val acc: 0.022026431718061675	
 equation acc: 0.0	
2022-09-10 07:27:58,421 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 6s
2022-09-10 07:27:58,424 | INFO | main_after.py: 96 : main() ::	 Experiment Name: cv_svamp_augmented
2022-09-10 07:27:58,424 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:27:58,424 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:27:59,478 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:27:59,478 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 3973
2022-09-10 07:27:59,478 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 165
2022-09-10 07:27:59,478 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: ['0.01', '12.0', '1.0', '100.0', '0.1', '0.5', '3.0', '4.0', '7.0']
2022-09-10 07:27:59,478 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:27:59,478 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:28:00,057 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 4068 / 4068 = 1.0
2022-09-10 07:28:00,228 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 4071 words in input language, 21 words in output
2022-09-10 07:28:00,237 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:28:03,068 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:28:03,068 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:28:03,146 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:28:03,146 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/svamp/dense/models/cv_svamp_augmented_fold3.
2022-09-10 07:28:06,449 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:28:11,539 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.04242424242424243	
 equation acc epoch: 0.030303030303030304	
 max val acc: 0.04242424242424243	
 equation acc: 0.030303030303030304	
2022-09-10 07:28:11,539 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 5s
2022-09-10 07:28:11,542 | INFO | main_after.py: 96 : main() ::	 Experiment Name: cv_svamp_augmented
2022-09-10 07:28:11,542 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:28:11,542 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:28:12,484 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:28:12,484 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 3954
2022-09-10 07:28:12,484 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 184
2022-09-10 07:28:12,484 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: ['0.01', '12.0', '1.0', '100.0', '0.1', '0.5', '3.0', '4.0', '7.0']
2022-09-10 07:28:12,484 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:28:12,485 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:28:13,063 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 4068 / 4068 = 1.0
2022-09-10 07:28:13,315 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 4071 words in input language, 21 words in output
2022-09-10 07:28:13,325 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:28:16,173 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:28:16,173 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:28:16,253 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:28:16,253 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/svamp/dense/models/cv_svamp_augmented_fold4.
2022-09-10 07:28:19,171 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:28:28,109 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.021739130434782608	
 equation acc epoch: 0.021739130434782608	
 max val acc: 0.021739130434782608	
 equation acc: 0.021739130434782608	
2022-09-10 07:28:28,109 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 8s
2022-09-10 07:28:28,109 | INFO | main_after.py: 272 : main() ::	 Final Val score: 0.031
Transfer numbers...
initialize by one_shot_gm
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.8994848915332073
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.9208287735084306
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 1.0
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.2875688340928819
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.29350619845920134
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6689910888671875
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7026435004340278
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5113423665364583
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.62873289320204
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.3810662163628472
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.39738972981770837
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6274193657769097
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7162628173828125
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.48017120361328125
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6058561537000868
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.40846591525607634
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.42443339029947913
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5902031792534722
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6886461046006944
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.47375191582573783
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.5935478210449219
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.3888838026258681
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.4106835259331597
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5678626166449653
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6548919677734375
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.4856088426378038
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6053636338975694
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.38129170735677087
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.4024488661024306
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5754275851779513
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6644049750434028
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5120133293999566
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6341404385036893
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.40482584635416663
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.4252149793836806
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5429026285807292
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6389906141493056
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5292629665798612
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6597154405381944
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.40503946940104163
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.4227888319227431
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5439656575520833
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6284467909071181
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5107065836588542
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6467573377821181
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.38005405002170134
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.39873928493923616
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6329210069444444
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7125905354817708
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5049468146430122
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6341582404242622
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.39771525065104163
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.4145067003038194
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5980733235677083
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6910213894314237
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5108922322591145
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6236597696940105
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.40517679850260413
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.41832309299045134
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6267479790581597
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7020399305555556
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5466456943088107
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6201828850640191
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.3970811631944444
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.4153730604383681
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6179521348741319
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6895107693142362
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5800955030653212
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6398340861002605
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.3892907036675347
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.3993072509765625
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5608961317274306
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6547563340928819
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5597186618381076
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7184906005859375
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.9122975667317709
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.7209898630777996
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.9047126770019531
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.6547292073567708
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.8399543762207031
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.7666726112365723
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.7607822418212891
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.7271270751953125
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.6611471176147461
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.3890876770019531
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.4043693542480469
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.3634223937988281
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.4035472869873047
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.282958984375
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.7434864044189453
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.63671875
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.42953618367513025
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.33203125
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.5745442708333333
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.7373035975864956
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.7587291172572544
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.7331270490373885
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.7412490844726562
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.8424115862165178
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.8604627336774553
Final sparsity level of 0.672: 0.6720000288533448
Transfer numbers...
initialize by one_shot_gm
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.8624420778208827
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.9234304069390402
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 1.0
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.30246988932291663
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.3087853325737847
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6916164822048612
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7249026828342013
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5332836574978299
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6516541375054253
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.39992947048611116
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.4168548583984375
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.650421142578125
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7377997504340278
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5015279981825087
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6285264756944444
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.4286363389756944
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.44527859157986116
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6131439208984375
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7098609076605903
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.4948145548502604
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6156107584635417
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.40837605794270837
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.43069966634114587
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5906473795572917
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6757320827907987
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5073102315266926
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.627279069688585
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.4001142713758681
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.4220496283637153
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5980919731987847
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6842074924045138
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5341233147515191
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6560117933485243
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.4244520399305556
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.4453040228949653
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5649549696180556
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6594577365451388
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5518642001681857
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6812464396158855
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.42477925618489587
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.4429338243272569
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5663384331597222
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6496446397569444
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5329178704155816
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6683926052517362
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.3992936876085069
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.4183892144097222
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6554785834418403
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7332644992404513
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5272386338975694
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6560257805718316
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.4176313612196181
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.4350433349609375
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6214226616753472
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7134806315104167
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5332505967881944
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6458960639105903
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.4252692328559028
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.4390394422743056
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6505635579427083
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7252315945095487
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.570330301920573
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6431291368272569
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.4168853759765625
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.435699462890625
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6419050428602431
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7132822672526042
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6045820448133681
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6633059183756511
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.4089677598741319
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.4190809461805556
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5847235785590278
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6800774468315972
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5838203430175781
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7414368523491753
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.9281700981987847
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.7259852091471355
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.9009675979614258
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.6311461130777996
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.8103065490722656
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.7286033630371094
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.6843662261962891
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.7004408836364746
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.6221065521240234
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.36553955078125
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.40944671630859375
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.3447456359863281
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.39426231384277344
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.314697265625
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.6953659057617188
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.662109375
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.42911275227864587
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.357421875
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.5087890625
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.7286006382533482
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.7419182913643974
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.723433358328683
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.7317701067243303
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.824725559779576
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.8326786586216518
Final sparsity level of 0.672: 0.6720000150081697
Transfer numbers...
initialize by one_shot_gm
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.8656694260005305
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.9230124229896238
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 1.0
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.2991417778862847
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.3052588568793403
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6869591606987847
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7203369140625
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5285449557834201
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6467386881510417
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.3955061170789931
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.41257900661892366
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6457604302300347
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7332899305555556
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.49701944986979163
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6236924065483941
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.42453511555989587
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.44099934895833337
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6083882649739583
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7056681315104167
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.4903823004828559
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.611006842719184
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.40421549479166663
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.4263475206163194
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5858442518446181
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6715443929036458
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5026062859429253
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6228603786892362
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.3959130181206597
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.4177686903211806
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5934499104817708
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6800299750434028
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5294698079427083
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6514566209581163
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.42019144694010413
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.4408789740668403
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5604434543185763
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6554802788628472
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5471530490451388
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6768086751302083
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.42041354709201384
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.43876139322916663
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5618896484375
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6453382703993056
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.52831056382921
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6639247470431857
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.39463467068142366
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.41412692599826384
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6509331597222222
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.728912353515625
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5224490695529513
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.651436275906033
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.4132317437065972
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.43024868435329866
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6167127821180556
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7087877061631944
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5284843444824219
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6412493387858074
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.4206915961371528
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.434356689453125
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6454230414496528
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7205793592664931
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5653309292263455
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6382738749186199
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.41268581814236116
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.4314507378472222
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6369866265190972
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7086995442708333
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5994538201226128
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6583947075737847
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.40479532877604163
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.4147101508246528
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5797932942708333
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6749471028645833
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5787103441026475
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7367909749348958
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.9250352647569444
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.7463118235270183
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.9161624908447266
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.6417528788248699
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.8226327896118164
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.7327308654785156
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.654871940612793
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.7338418960571289
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.6694555282592773
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.3880271911621094
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.41354942321777344
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.35628509521484375
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.40418434143066406
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.318115234375
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.7031574249267578
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.66015625
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.4369926452636719
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.375
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.5074869791666667
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.7441297258649553
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.7692642211914062
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.7290682111467635
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.7467335292271206
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.829559326171875
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.838456290108817
Final sparsity level of 0.672: 0.6720000426985199
Transfer numbers...
initialize by one_shot_gm
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.8379881679183661
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.924565296692607
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 1.0
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.30852932400173616
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.3146074083116319
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7010413275824653
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7337883843315972
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5423007541232638
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6608946058485243
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.40761990017361116
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.42497762044270837
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6603529188368056
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7464921739366319
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5101449754503038
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6377368503146701
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.43704732259114587
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.45380147298177087
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6228112114800347
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7185194227430556
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5034980773925781
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6246121724446614
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.4160783555772569
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.4388342963324653
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6002129448784722
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6842973497178819
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5159924825032551
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6362453036838107
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.4077385796440972
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.42999606662326384
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6075812445746528
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6924828423394097
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5431522793240018
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.664817386203342
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.43230183919270837
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.45380655924479163
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.574066162109375
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6680399576822917
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5611457824707031
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6901601155598958
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.43303426106770837
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.45093790690104163
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5757615831163194
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6584591335720487
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5420036315917969
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6771227518717449
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.4064856635199653
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.42609829372829866
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6649712456597222
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7417093912760417
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5361709594726562
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6649487813313801
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.42528279622395837
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.44283718532986116
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6314002143012153
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7227596706814237
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5423609415690105
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6551564534505208
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.4332343207465278
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.44715542263454866
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6603936089409722
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7349904378255208
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5800518459743924
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6525874667697482
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.4245317247178819
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.443756103515625
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6517232259114583
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7230885823567708
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6145189073350694
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6730851067437066
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.41687350802951384
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.4271765814887153
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5946468777126737
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6904568142361112
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5937978956434462
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7507781982421875
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.9343024359809028
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.7409286499023438
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.9112491607666016
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.6310927073160808
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.8142757415771484
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.7378249168395996
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.6750802993774414
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.7399101257324219
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.6816987991333008
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.358428955078125
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.3946571350097656
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.34014129638671875
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.38554954528808594
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.33544921875
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.6547584533691406
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.63671875
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.4251976013183594
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.400390625
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.4755859375
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.7391422816685268
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.752093723842076
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.7254834856305803
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.7260164533342635
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.8321816580636161
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.8365129743303572
Final sparsity level of 0.672: 0.6720000080855822
Transfer numbers...
initialize by one_shot_gm
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.9064395941095527
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.9216470087548638
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 1.0
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.285308837890625
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.2914276123046875
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6653832329644097
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6990458170572917
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.507781982421875
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6250826517740886
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.37790256076388884
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.3943328857421875
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6235724555121528
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7125311957465278
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.4770045810275607
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6023258633083768
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.40551418728298616
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.4215766059027778
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5864800347222222
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6849907769097222
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.4704437255859375
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.589917500813802
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.3856743706597222
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.40732150607638884
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5642632378472222
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6516316731770833
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.48226165771484375
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.601981692843967
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.3784264458550347
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.3997650146484375
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5717078314887153
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6612582736545138
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5085110134548612
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6307097540961372
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.40155368381076384
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.4218427870008681
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5394134521484375
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6355692545572917
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5258013407389324
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6561186048719618
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.4019622802734375
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.41942172580295134
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5404781765407987
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6249643961588542
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5072614881727431
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6432100931803386
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.37732272677951384
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.3959503173828125
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6291944715711806
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7091929117838542
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5015284220377605
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6305970085991753
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.39464992947048616
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.41146511501736116
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5943094889322917
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6872660319010417
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5073615180121528
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.619895511203342
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.40215725368923616
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.41518147786458337
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6227518717447917
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6983049180772569
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5429666307237413
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6166292826334636
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.3942396375868056
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.41209920247395837
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6138814290364583
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6853569878472222
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5760760837131076
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6360113355848525
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.3863355848524306
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.3962877061631944
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.5569644504123263
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.6507398817274306
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5560116238064237
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7147500779893663
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.9092797173394097
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.7051048278808594
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.8947772979736328
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.6384487152099609
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.8299980163574219
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.7424278259277344
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.7406320571899414
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.7054619789123535
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.6303653717041016
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.379913330078125
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.4319572448730469
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.3606452941894531
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.41751670837402344
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.3017578125
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.7572212219238281
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.677734375
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.45808283487955725
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.330078125
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.587890625
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.7668871198381697
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.7910995483398438
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.7692533220563615
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.7845785958426339
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.8832789829799107
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.8975710187639508
Final sparsity level of 0.672: 0.6720000080855822
wandb: WARNING W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.
2022-09-10 07:28:31,105 | INFO | main_after.py: 96 : main() ::	 Experiment Name: cv_svamp_augmented
2022-09-10 07:28:31,105 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:28:31,105 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:28:32,140 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:28:32,141 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 3932
2022-09-10 07:28:32,141 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 206
2022-09-10 07:28:32,141 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: ['0.01', '12.0', '1.0', '100.0', '0.1', '0.5', '3.0', '4.0', '7.0']
2022-09-10 07:28:32,141 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:28:32,141 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:28:32,722 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 4068 / 4068 = 1.0
2022-09-10 07:28:32,888 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 4071 words in input language, 21 words in output
2022-09-10 07:28:32,897 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:28:35,945 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:28:35,945 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:28:39,272 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:28:39,272 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/svamp/dense/models/cv_svamp_augmented_fold0.
2022-09-10 07:28:42,771 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:28:51,456 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.06310679611650485	
 equation acc epoch: 0.04854368932038835	
 max val acc: 0.06310679611650485	
 equation acc: 0.04854368932038835	
2022-09-10 07:28:51,456 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 8s
2022-09-10 07:28:51,459 | INFO | main_after.py: 96 : main() ::	 Experiment Name: cv_svamp_augmented
2022-09-10 07:28:51,459 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:28:51,459 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:28:52,417 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:28:52,417 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 3920
2022-09-10 07:28:52,417 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 218
2022-09-10 07:28:52,418 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: ['0.01', '12.0', '1.0', '100.0', '0.1', '0.5', '3.0', '4.0', '7.0']
2022-09-10 07:28:52,418 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:28:52,418 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:28:52,992 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 4068 / 4068 = 1.0
2022-09-10 07:28:53,157 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 4071 words in input language, 21 words in output
2022-09-10 07:28:53,169 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:28:56,116 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:28:56,117 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:28:56,196 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:28:56,196 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/svamp/dense/models/cv_svamp_augmented_fold1.
2022-09-10 07:28:59,575 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:29:09,627 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.027522935779816515	
 equation acc epoch: 0.0	
 max val acc: 0.027522935779816515	
 equation acc: 0.0	
2022-09-10 07:29:09,627 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 10s
2022-09-10 07:29:09,630 | INFO | main_after.py: 96 : main() ::	 Experiment Name: cv_svamp_augmented
2022-09-10 07:29:09,630 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:29:09,630 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:29:10,585 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:29:10,585 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 3911
2022-09-10 07:29:10,585 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 227
2022-09-10 07:29:10,585 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: ['0.01', '12.0', '1.0', '100.0', '0.1', '0.5', '3.0', '4.0', '7.0']
2022-09-10 07:29:10,585 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:29:10,585 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:29:11,165 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 4068 / 4068 = 1.0
2022-09-10 07:29:11,332 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 4071 words in input language, 21 words in output
2022-09-10 07:29:11,343 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:29:14,229 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:29:14,229 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:29:14,307 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:29:14,307 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/svamp/dense/models/cv_svamp_augmented_fold2.
2022-09-10 07:29:17,401 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:29:24,449 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.05286343612334802	
 equation acc epoch: 0.004405286343612335	
 max val acc: 0.05286343612334802	
 equation acc: 0.004405286343612335	
2022-09-10 07:29:24,449 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 7s
2022-09-10 07:29:24,451 | INFO | main_after.py: 96 : main() ::	 Experiment Name: cv_svamp_augmented
2022-09-10 07:29:24,451 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:29:24,451 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:29:25,412 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:29:25,412 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 3973
2022-09-10 07:29:25,412 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 165
2022-09-10 07:29:25,412 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: ['0.01', '12.0', '1.0', '100.0', '0.1', '0.5', '3.0', '4.0', '7.0']
2022-09-10 07:29:25,412 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:29:25,412 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:29:25,989 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 4068 / 4068 = 1.0
2022-09-10 07:29:26,159 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 4071 words in input language, 21 words in output
2022-09-10 07:29:26,168 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:29:29,022 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:29:29,022 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:29:29,100 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:29:29,100 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/svamp/dense/models/cv_svamp_augmented_fold3.
2022-09-10 07:29:32,168 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:29:37,428 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.04242424242424243	
 equation acc epoch: 0.024242424242424242	
 max val acc: 0.04242424242424243	
 equation acc: 0.024242424242424242	
2022-09-10 07:29:37,428 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 5s
2022-09-10 07:29:37,431 | INFO | main_after.py: 96 : main() ::	 Experiment Name: cv_svamp_augmented
2022-09-10 07:29:37,431 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:29:37,431 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:29:38,391 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:29:38,391 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 3954
2022-09-10 07:29:38,391 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 184
2022-09-10 07:29:38,391 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: ['0.01', '12.0', '1.0', '100.0', '0.1', '0.5', '3.0', '4.0', '7.0']
2022-09-10 07:29:38,391 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:29:38,391 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:29:38,972 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 4068 / 4068 = 1.0
2022-09-10 07:29:39,143 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 4071 words in input language, 21 words in output
2022-09-10 07:29:39,153 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:29:41,981 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:29:41,981 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:29:42,060 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:29:42,060 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/svamp/dense/models/cv_svamp_augmented_fold4.
2022-09-10 07:29:45,141 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:29:54,580 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.0	
 equation acc epoch: 0.0	
 max val acc: 0.0	
 equation acc: 0.0	
2022-09-10 07:29:54,580 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 9s
Traceback (most recent call last):
  File "/home/sliu/miniconda3/envs/prune_cry/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/sliu/miniconda3/envs/prune_cry/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/gpfs/home6/sliu/Projects/SVAMP/code/gts/src/main_after.py", line 276, in <module>
    main()
  File "/gpfs/home6/sliu/Projects/SVAMP/code/gts/src/main_after.py", line 266, in main
    folds_scores.append(float(best_acc[w][0])/best_acc[w][1])
ZeroDivisionError: float division by zero
Transfer numbers...
initialize by one_shot_gm
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.9113986755611924
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.9387438189040207
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 1.0
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.3541887071397569
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.36059909396701384
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7633904351128472
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7923414442274306
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6060244242350261
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7238112555609809
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.4639434814453125
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.4824540879991319
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7252163357204862
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8028021918402778
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5718646579318576
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.700792948404948
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.4964667426215278
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5142161051432292
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6867760552300347
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7748887803819444
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5652881198459201
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6865980360243056
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.4732988145616319
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.4968431260850694
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6659749348958333
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7410413953993056
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5780694749620225
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6974809434678819
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.4642808702256944
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.48825581868489587
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6721462673611112
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7465447319878472
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6068424648708768
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7251900566948785
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.4905853271484375
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5133700900607638
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.637054443359375
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7241668701171875
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6256607903374566
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7496350606282551
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.4914771185980903
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5101437038845487
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6407826741536458
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7180870903862847
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6062855190700955
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7368181016710069
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.46331278483072913
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.48383585611979163
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7272186279296875
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7962290445963541
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6003040737575955
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7260013156467013
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.4841528998480903
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5032314724392362
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6977115207248263
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7831844753689237
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6075981987847222
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7177395290798612
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.49273342556423616
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5081973605685763
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7263268364800347
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7974836561414931
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6480216979980469
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7171842787000868
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.4837714301215278
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5050676133897569
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7199367947048612
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7878604465060763
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6844423082139757
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7387504577636719
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.47500271267361116
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.4852769639756944
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6623653835720487
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7582295735677084
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6629981994628906
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8121969434950087
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.9669562445746528
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.7640940348307291
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.9255495071411133
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.6998742421468098
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.8676338195800781
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.8127408027648926
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.8306922912597656
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.7752962112426758
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.7456502914428711
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.4724769592285156
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.4886207580566406
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.44348907470703125
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.48891639709472656
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.353759765625
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.787750244140625
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.646484375
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.5005836486816406
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.361328125
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.6083984375
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.806933811732701
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.8270557948521206
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.8068531581333706
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.8140934535435268
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.9011633736746651
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.917466299874442
Final sparsity level of 0.738: 0.7380000198816714
Transfer numbers...
initialize by one_shot_gm
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.8939171868264862
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.9399496392671854
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 1.0
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.364593505859375
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.3712073432074653
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.776214599609375
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8039686414930556
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6198984781901042
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7368960910373263
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.47649807400173616
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.49560038248697913
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7386203342013888
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8142072889539931
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5855060153537326
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7142571343315972
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5095723470052083
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.527862548828125
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7004682752821181
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7867991129557291
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5790443420410156
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6997142367892795
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.48627217610677087
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5103573269314237
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6802046034071181
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7529246012369791
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5917867024739583
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7104606628417969
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.4772796630859375
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5015394422743056
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6859063042534722
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.75799560546875
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6207665337456597
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7379764980740018
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5040503607855903
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5268707275390625
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6506907145182292
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7359636094835069
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6394699944390191
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7619391547309028
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5049404568142362
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5236290825737847
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6545257568359375
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7305484347873263
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6203231811523438
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7492747836642795
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.4760606553819444
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.49686177571614587
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7401190863715278
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8069407145182291
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.614418453640408
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7385300530327691
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.4978010389539931
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5168863932291667
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7115105523003472
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7954830593532987
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6218363444010417
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7305560641818576
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5061848958333333
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5215504964192708
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7400190565321181
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.809844970703125
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.66265869140625
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7306666904025607
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.4973008897569444
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5183529324001737
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7337985568576388
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8012746175130209
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6995065477159288
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7523248460557725
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.4884406195746528
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.49878946940104163
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6769663492838542
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7726474338107638
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6778543260362413
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8241988288031684
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.9722730848524306
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.7673110961914062
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.9220781326293945
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.6749318440755208
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.8399190902709961
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.7758941650390625
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.759953498840332
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.7496037483215332
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.7016525268554688
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.4371299743652344
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.48593711853027344
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.41448211669921875
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.47030067443847656
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.375
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.7449245452880859
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.67578125
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.495782216389974
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.388671875
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.5387369791666667
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.7963954380580357
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.808886936732701
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.7937545776367188
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.8009283883231026
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.8840353829520089
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.8926021030970982
Final sparsity level of 0.738: 0.7380000268042589
Transfer numbers...
initialize by one_shot_gm
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.8982385544116185
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.9396507174124513
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 1.0
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.3605499267578125
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.3669348822699653
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7717098659939237
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7997368706597222
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6145850287543403
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7319577534993489
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.47134060329861116
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.4906395806206597
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7336815728081597
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8099433051215278
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.580307854546441
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7092170715332031
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5047895643446181
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5227779812282987
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6957482231987847
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7824486626519097
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5738758511013455
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.69482421875
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.4814317491319444
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5051371256510417
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6749827067057292
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7483639187282987
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.58660888671875
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7056905958387587
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.47243245442708337
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.4964463975694444
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6808624267578125
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7539621988932291
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6156497531467013
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7332255045572917
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.4989454481336806
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5218387179904513
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6458502875434028
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7316640218098958
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6344341701931424
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.757346683078342
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.4999627007378472
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5186292860243056
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6496412489149306
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7259775797526042
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6151568094889324
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.744712405734592
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.4711286756727431
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.49179585774739587
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7355075412326388
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8031633165147569
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6091910468207465
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.733963860405816
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.4925164116753472
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.511749267578125
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7062021891276042
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7907782660590278
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6164521111382378
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7257694668240018
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5011393229166667
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5163319905598958
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7347734239366319
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8051571316189237
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6571905348036025
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7256838480631511
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.4924689398871528
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5133836534288194
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7288801405164931
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7966105143229166
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6938785976833768
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7473907470703125
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.48345947265625
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.4938591851128472
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6717868381076388
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7675628662109375
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6722738477918837
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8198051452636719
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.9704776340060763
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.7862116495768229
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.9349288940429688
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.6842803955078125
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.8503866195678711
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.7792363166809082
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.7350301742553711
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.7799243927001953
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.7461624145507812
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.4634361267089844
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.4907493591308594
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.42841339111328125
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.4816551208496094
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.387451171875
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.7536563873291016
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.666015625
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.5036913553873699
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.412109375
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.5374348958333333
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.8113501412527901
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.8339800153459822
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.8002341134207589
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.8156193324497768
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.8890936715262276
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.8979197910853794
Final sparsity level of 0.738: 0.7380000268042589
Transfer numbers...
initialize by one_shot_gm
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.854921027926987
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.9416494406614786
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 1.0
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.37652248806423616
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.38328891330295134
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7907443576388888
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8174624972873263
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6358278062608507
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.751739501953125
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.4912872314453125
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5103759765625
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7542012532552084
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8268280029296875
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6009877522786458
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.729231940375434
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5252617730034722
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5434112548828125
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7163848876953125
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8001437717013888
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5942446390787761
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7146559821234809
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5011562771267362
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5251736111111112
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6962263319227431
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7662472195095487
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6074129740397136
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7252506679958768
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.4920366075303819
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5165812174479167
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7016059027777778
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7706807454427084
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6365199618869357
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.751946767171224
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5191650390625
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5421786838107638
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6662851969401042
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7494150797526042
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.65538575914171
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7759140862358941
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5202670627170138
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5389980740017362
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6706865098741319
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7445661756727431
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6361160278320312
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7631645202636719
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.49070231119791663
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5117424858940972
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7551184760199653
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8194512261284722
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6303049723307292
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7526986863878038
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5128885904947917
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5323876274956597
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7273339165581597
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8092515733506944
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6378682454427083
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7451752556694878
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5215301513671875
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5371127658420138
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7558542887369791
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8241204155815972
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.679207271999783
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7458072238498263
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5126919216579862
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5338728162977431
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7500508626302084
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8159383138020834
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.716356913248698
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7677252027723525
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.50347900390625
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5140431722005208
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6937272813585069
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7885962592230903
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6947678460015191
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8378045823838975
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.977752685546875
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.7816073099772135
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.930842399597168
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.6787980397542317
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.8461894989013672
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.7862515449523926
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.7535343170166016
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.7879734039306641
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.7574939727783203
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.4362373352050781
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.47489356994628906
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.414520263671875
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.4665203094482422
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.41015625
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.7149639129638672
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.65625
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.497979482014974
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.443359375
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.5100911458333333
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.8092128208705357
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.821671622140067
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.799300057547433
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.8003790719168526
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.8931841169084821
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.8982315063476562
Final sparsity level of 0.738: 0.738000054494609
Transfer numbers...
initialize by one_shot_gm
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.9169514334444113
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.9392302002269779
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 1.0
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.35236104329427087
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.358734130859375
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7609625922309028
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7899068196614584
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.603196038140191
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7212282816569011
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.46148342556423616
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.4802178276909722
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7224494086371528
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8003370496961806
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.569321526421441
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6982142130533855
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.49382358127170134
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5119747585720487
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6843821207682292
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7727678087022569
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5627115037706163
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6839184231228299
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.4708930121527778
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.49458821614583337
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6630638970269097
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7386389838324653
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.5755432976616753
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.6948640611436632
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.4618971082899306
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.48575168185763884
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6692487928602431
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7441541883680556
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6042772928873699
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7227325439453125
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.48829989963107634
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5110422770182292
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6342044406467013
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7216542561848958
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6229926215277778
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7471046447753906
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.4890814887152778
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5077853732638888
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6379920111762153
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7155320909288194
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6034863789876301
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7343766954210069
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.46051364474826384
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.48112148708767366
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7247331407335069
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7937062581380209
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.59760496351454
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7235488891601562
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.4817284478081597
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5007680257161458
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6946733262803819
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7807210286458334
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.604779561360677
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7150739034016926
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.49002583821614587
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5055185953776042
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.72344970703125
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7946082221137153
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6451416015625
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7145360310872395
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.4814436170789931
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5020734998914931
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.716888427734375
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7853800455729166
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.681427001953125
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7359373304578993
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.47250705295138884
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.48271687825520837
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.6593729654947917
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7554202609592013
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6600142584906684
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8097059461805556
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.9657508002387153
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.7504412333170573
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.9176340103149414
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.6851711273193359
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.858917236328125
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.7911443710327148
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.8160600662231445
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.7548727989196777
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.7167625427246094
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.4637413024902344
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.5194549560546875
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.44152069091796875
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.5059585571289062
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.364013671875
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.7862300872802734
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.69140625
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.5236193339029949
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.36328125
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.6119791666666667
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.8325097220284599
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.8545575823102678
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.8378023420061385
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.8523668561662947
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.9318673270089286
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.9441026960100446
Final sparsity level of 0.738: 0.7380000475720214
wandb: WARNING W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.
2022-09-10 07:29:57,628 | INFO | main_after.py: 96 : main() ::	 Experiment Name: cv_svamp_augmented
2022-09-10 07:29:57,628 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:29:57,628 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:29:58,663 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:29:58,664 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 3932
2022-09-10 07:29:58,664 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 206
2022-09-10 07:29:58,664 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: ['0.01', '12.0', '1.0', '100.0', '0.1', '0.5', '3.0', '4.0', '7.0']
2022-09-10 07:29:58,664 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:29:58,664 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:29:59,239 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 4068 / 4068 = 1.0
2022-09-10 07:29:59,405 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 4071 words in input language, 21 words in output
2022-09-10 07:29:59,414 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:30:02,450 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:30:02,450 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:30:05,748 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:30:05,748 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/svamp/dense/models/cv_svamp_augmented_fold0.
2022-09-10 07:30:09,097 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:30:19,653 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.0048543689320388345	
 equation acc epoch: 0.0	
 max val acc: 0.0048543689320388345	
 equation acc: 0.0	
2022-09-10 07:30:19,653 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 10s
2022-09-10 07:30:19,656 | INFO | main_after.py: 96 : main() ::	 Experiment Name: cv_svamp_augmented
2022-09-10 07:30:19,656 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:30:19,656 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:30:20,615 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:30:20,616 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 3920
2022-09-10 07:30:20,616 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 218
2022-09-10 07:30:20,616 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: ['0.01', '12.0', '1.0', '100.0', '0.1', '0.5', '3.0', '4.0', '7.0']
2022-09-10 07:30:20,616 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:30:20,616 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:30:21,188 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 4068 / 4068 = 1.0
2022-09-10 07:30:21,350 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 4071 words in input language, 21 words in output
2022-09-10 07:30:21,361 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:30:24,308 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:30:24,308 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:30:24,387 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:30:24,387 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/svamp/dense/models/cv_svamp_augmented_fold1.
2022-09-10 07:30:29,554 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:30:37,628 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.009174311926605505	
 equation acc epoch: 0.0045871559633027525	
 max val acc: 0.009174311926605505	
 equation acc: 0.0045871559633027525	
2022-09-10 07:30:37,628 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 8s
2022-09-10 07:30:37,631 | INFO | main_after.py: 96 : main() ::	 Experiment Name: cv_svamp_augmented
2022-09-10 07:30:37,631 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:30:37,631 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:30:38,587 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:30:38,587 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 3911
2022-09-10 07:30:38,587 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 227
2022-09-10 07:30:38,587 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: ['0.01', '12.0', '1.0', '100.0', '0.1', '0.5', '3.0', '4.0', '7.0']
2022-09-10 07:30:38,587 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:30:38,587 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:30:39,169 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 4068 / 4068 = 1.0
2022-09-10 07:30:39,334 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 4071 words in input language, 21 words in output
2022-09-10 07:30:39,345 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:30:42,173 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:30:42,173 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:30:42,250 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:30:42,250 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/svamp/dense/models/cv_svamp_augmented_fold2.
2022-09-10 07:30:47,869 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:30:54,773 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.03524229074889868	
 equation acc epoch: 0.004405286343612335	
 max val acc: 0.03524229074889868	
 equation acc: 0.004405286343612335	
2022-09-10 07:30:54,773 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 6s
2022-09-10 07:30:54,775 | INFO | main_after.py: 96 : main() ::	 Experiment Name: cv_svamp_augmented
2022-09-10 07:30:54,776 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:30:54,776 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:30:55,743 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:30:55,744 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 3973
2022-09-10 07:30:55,744 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 165
2022-09-10 07:30:55,744 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: ['0.01', '12.0', '1.0', '100.0', '0.1', '0.5', '3.0', '4.0', '7.0']
2022-09-10 07:30:55,744 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:30:55,744 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:30:56,329 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 4068 / 4068 = 1.0
2022-09-10 07:30:56,499 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 4071 words in input language, 21 words in output
2022-09-10 07:30:56,509 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:30:59,333 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:30:59,333 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:30:59,410 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:30:59,410 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/svamp/dense/models/cv_svamp_augmented_fold3.
2022-09-10 07:31:02,738 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:31:07,885 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.05454545454545454	
 equation acc epoch: 0.048484848484848485	
 max val acc: 0.05454545454545454	
 equation acc: 0.048484848484848485	
2022-09-10 07:31:07,885 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 5s
2022-09-10 07:31:07,887 | INFO | main_after.py: 96 : main() ::	 Experiment Name: cv_svamp_augmented
2022-09-10 07:31:07,887 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:31:07,888 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:31:08,843 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:31:08,843 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 3954
2022-09-10 07:31:08,843 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 184
2022-09-10 07:31:08,843 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: ['0.01', '12.0', '1.0', '100.0', '0.1', '0.5', '3.0', '4.0', '7.0']
2022-09-10 07:31:08,843 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:31:08,843 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:31:09,427 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 4068 / 4068 = 1.0
2022-09-10 07:31:09,598 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 4071 words in input language, 21 words in output
2022-09-10 07:31:09,608 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:31:12,468 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:31:12,468 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:31:12,548 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:31:12,548 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/svamp/dense/models/cv_svamp_augmented_fold4.
2022-09-10 07:31:15,670 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:31:22,944 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.03804347826086957	
 equation acc epoch: 0.03804347826086957	
 max val acc: 0.03804347826086957	
 equation acc: 0.03804347826086957	
2022-09-10 07:31:22,944 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 7s
2022-09-10 07:31:22,944 | INFO | main_after.py: 272 : main() ::	 Final Val score: 0.027
Transfer numbers...
initialize by one_shot_gm
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.9210651515716701
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.9527272819390402
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 1.0
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.41629367404513884
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.42435201009114587
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8334943983289931
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8564317491319444
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6854239569769965
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7962714301215278
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5392896864149306
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5588938395182292
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8001285129123263
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8627997504340278
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6502579583062066
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7747955322265625
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5745832655164931
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5937110053168403
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7639719645182291
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8386942545572916
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.643511454264323
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7603628370496962
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5495791965060763
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5743764241536458
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7456580268012153
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8062659369574653
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6569048563639324
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7701043023003472
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5397745768229167
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5645599365234375
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7490403917100694
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8090532090928819
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6854548984103732
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7948735555013021
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5680762396918403
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5915934244791667
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7142401801215278
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7900916205512153
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.705060323079427
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8176485697428385
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5696953667534722
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5889858669704862
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7203030056423612
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7877892388237847
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6857710944281684
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.804909176296658
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5386928982204862
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5600263807508681
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7987128363715278
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8555687798394097
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6804559495713975
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7959293789333768
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5630544026692708
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.58245849609375
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7757229275173612
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8499908447265625
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6887813144259982
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7898601955837674
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.57171630859375
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.58770751953125
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8026394314236112
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8655582004123263
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7307417127821181
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7920485602484809
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5629492865668403
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5850270589192708
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7992451985677084
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8590359157986112
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7687182956271701
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8140936957465278
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5534634060329862
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5634070502387153
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7448086208767362
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8348151312934028
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7475051879882812
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8771726820203993
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.9894036187065972
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.7997283935546875
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.9413337707519531
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.7391033172607422
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.8896894454956055
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.848243236541748
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.8790435791015625
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.8136658668518066
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.8081932067871094
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.5485572814941406
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.5628738403320312
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.5164413452148438
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.5653762817382812
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.41455078125
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.8204002380371094
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.6640625
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.5612831115722656
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.396484375
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.6302083333333333
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.8589303152901786
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.8769269670758929
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.8622196742466518
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.8676834106445312
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.9384972708565849
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.9523555210658482
Final sparsity level of 0.791: 0.7910000214877116
Transfer numbers...
initialize by one_shot_gm
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.9100982501077622
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.953862171692607
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 1.0
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.42529805501302087
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.43350558810763884
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8425157335069444
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8640374077690972
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6962619357638888
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.805336422390408
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5502251519097222
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5697547064887153
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8097093370225694
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8699815538194444
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6611065334743924
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7843780517578125
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5854678683810763
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6046142578125
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7740698920355903
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8464558919270834
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6541930304633247
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7700059678819444
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5604536268446181
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5852423773871528
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7561882866753472
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8144870334201388
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6676784091525607
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7795304192437066
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5509677463107638
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5759446885850694
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7588687472873263
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8167860243055556
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6962784661187066
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8038669162326388
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5791676839192708
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6028883192274306
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7243262396918403
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7983127170138888
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7158012390136719
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8261214362250434
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5804256863064237
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6002892388237847
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7309400770399306
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7969122992621528
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6964933607313368
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8135664198133681
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5493503146701388
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.570831298828125
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8077172173394097
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8628116183810763
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6914761861165364
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8049574957953559
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5744188096788194
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5938381618923612
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7859208848741319
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8585052490234375
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6997032165527344
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7989137437608507
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5829976399739583
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5989413791232638
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8120998806423612
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8734707302517362
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7417517768012153
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8015653822157118
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5741136338975694
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5960795084635417
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8091261121961806
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8676079644097222
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7797872755262587
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8235354953342013
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5647193060980903
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5747426350911458
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7559611002604166
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8445044623480903
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7587763468424479
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8847003512912326
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.9911363389756944
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.8027642567952474
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.937932014465332
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.7143325805664062
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.8642787933349609
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.8144001960754395
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.819331169128418
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.7900400161743164
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.7674131393432617
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.5073966979980469
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.5572681427001953
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.4818000793457031
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.5428066253662109
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.4384765625
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.7812328338623047
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.68359375
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.5551643371582031
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.44921875
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.5651041666666667
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.8496170043945312
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.8606338500976562
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.8495679582868303
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.8559831891741072
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.9254270281110492
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.9340526035853794
Final sparsity level of 0.791: 0.7910000076425366
Transfer numbers...
initialize by one_shot_gm
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.9117642380798435
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.953532851005188
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 1.0
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.42169528537326384
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.42969089084201384
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8390163845486112
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8613993326822916
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.692100101047092
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8019765218098959
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5457967122395833
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5653923882378472
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8064151340060763
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8673604329427084
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6570417616102431
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7808113098144531
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5812123616536458
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6004231770833333
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7704857720269097
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8437822129991319
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6501477559407551
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7663103739420573
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5563371446397569
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5812835693359375
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7523227267795138
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8115268283420138
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6635937160915799
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7760704888237847
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5465732150607638
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5716010199652778
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7551337348090278
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8140224880642362
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6922090318467882
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.800604502360026
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5749443901909722
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5986650254991319
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7206759982638888
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7953898111979166
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7116928100585938
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8229827880859375
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5764651828342013
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5959845648871528
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7270067003038194
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7936299641927084
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6925464206271701
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8104315863715278
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5452372233072917
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5667555067274306
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8043297661675347
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8600785997178819
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6874287923177083
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8016755845811632
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5697733561197917
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5895521375868056
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7822723388671875
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8552432590060763
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6955498589409722
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7955301072862413
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5785132514105903
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5946417914496528
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8084343804253472
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8703426784939237
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7376696268717449
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7980639139811198
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.569915771484375
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5917883978949653
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8057132297092013
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8644951714409722
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7757576836480035
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.820145501030816
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5604332817925347
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5705888536241319
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7520463731553819
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8410966661241319
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7546429104275174
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8820406595865885
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.9905497233072916
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.8202444712320963
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.9492998123168945
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.7233390808105469
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.8738574981689453
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.8179559707641602
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.8004207611083984
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.8186097145080566
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.8074617385864258
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.5358161926269531
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.5642185211181641
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.4981040954589844
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.5556201934814453
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.4521484375
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.7908210754394531
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.669921875
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.5647900899251301
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.453125
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.5667317708333333
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.8641629900251115
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.8839024135044643
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.8571221487862724
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.8698327200753349
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.9309212820870536
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.9389997209821429
Final sparsity level of 0.791: 0.7910000145651241
Transfer numbers...
initialize by one_shot_gm
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.8705268586906064
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.9551743879701686
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 1.0
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.4395684136284722
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.4477403428819444
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8561774359809028
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8762715657552084
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7130381266276042
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8197021484375
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5674048529730903
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5868598090277778
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8251495361328125
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8812764485677084
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6778026156955295
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.799293941921658
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6033393012152778
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6222449408637153
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7899525960286459
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8589816623263888
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6709285312228732
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7848336961534288
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5779300265842013
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6027357313368056
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7732289632161459
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8276824951171875
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6845919291178386
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7941623263888888
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5680338541666667
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.593170166015625
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7747684054904513
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8297322591145834
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7128842671712239
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8179550170898438
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5966542561848958
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6202443440755208
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7410769992404513
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8119710286458334
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7326978047688801
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8394889831542969
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5981920030381944
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6179521348741319
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7478790283203125
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8114607069227431
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7136442396375868
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8270797729492188
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5666724310980903
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5880177815755208
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8222503662109375
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8744015163845487
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7085054185655382
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.819067637125651
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5917934841579862
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6116841634114583
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8018815782335069
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8714277479383681
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.716996086968316
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8132294548882378
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6008182101779513
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.616943359375
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8275434705946181
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8862592909071181
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7592150370279948
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8164054022894965
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.591644287109375
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6136796739366319
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8252122667100694
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8809678819444444
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7972195943196615
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8384284973144531
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5822652180989583
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5925343831380208
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7735443115234375
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8594292534722222
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7764566209581163
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8968781365288628
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.9934624565972222
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.8144969940185547
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.9447078704833984
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.7196311950683594
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.8708772659301758
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.8238000869750977
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.8116693496704102
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.8249797821044922
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.8139972686767578
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.5067520141601562
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.5474052429199219
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.4841041564941406
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.5407199859619141
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.46875
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.7587146759033203
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.662109375
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.561169942220052
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.48828125
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.5419921875
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.8604976109095982
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.8714196341378349
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.854504176548549
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.856030055454799
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.9323512486049107
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.9377485002790179
Final sparsity level of 0.791: 0.7910222568388519
Transfer numbers...
initialize by one_shot_gm
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.9260347242945721
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.9531756647211413
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 1.0
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.4147203233506944
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.42268032497829866
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.831512451171875
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8547261555989584
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6831800672743056
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7943233913845487
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5372653537326388
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5566694471571181
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7981397840711806
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8612179226345487
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6480365329318576
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7727495829264323
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5721978081597222
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5912967258029513
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7619188096788194
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8370598687065972
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6411026848687066
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7582511901855469
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5472005208333333
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5720079210069444
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7433505588107638
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8043772379557291
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6545880635579426
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7680486043294271
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5377926296657987
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5624864366319444
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7466922336154513
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8070848253038194
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6833220587836372
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7929746839735243
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5658586290147569
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5893147786458333
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7117140028211806
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7880537245008681
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7027041117350261
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8157170613606771
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5671776665581597
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5865359836154513
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7177191840277778
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.7856496175130209
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6835369533962674
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.803015391031901
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5363854302300347
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5577748616536458
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7964867485894097
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8538903130425347
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6782400343153212
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7941966586642795
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.560943603515625
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5804239908854167
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7735392252604166
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8482038709852431
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.6863890753851997
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7878744337293837
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5692545572916667
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5855153401692708
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8004438612196181
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8635236952039931
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7283897399902344
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.7899030049641927
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5604536268446181
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5823822021484375
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7969106038411459
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8569810655381944
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7662985059950087
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8119451734754775
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5510186089409722
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5612131754557292
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7423824734157987
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8328145345052084
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7451032002766926
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8753339979383681
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.9889695909288194
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.7885926564534506
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.9351034164428711
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.7261575063069661
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.8823966979980469
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.8292942047119141
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.8678579330444336
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.7948770523071289
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.7825241088867188
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.5390548706054688
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.5954627990722656
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.5145187377929688
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.5846977233886719
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.4248046875
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.8100872039794922
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.705078125
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.5784924825032551
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.375
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.6302083333333333
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.8798043387276786
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.8986522129603794
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.8874697004045758
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.8996473039899554
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.9605157034737724
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.9700262887137276
Final sparsity level of 0.791: 0.7910000353328868
wandb: WARNING W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.
2022-09-10 07:31:25,985 | INFO | main_after.py: 96 : main() ::	 Experiment Name: cv_svamp_augmented
2022-09-10 07:31:25,985 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:31:25,985 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:31:27,020 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:31:27,020 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 3932
2022-09-10 07:31:27,020 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 206
2022-09-10 07:31:27,021 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: ['0.01', '12.0', '1.0', '100.0', '0.1', '0.5', '3.0', '4.0', '7.0']
2022-09-10 07:31:27,021 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:31:27,021 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:31:27,595 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 4068 / 4068 = 1.0
2022-09-10 07:31:27,759 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 4071 words in input language, 21 words in output
2022-09-10 07:31:27,768 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:31:30,877 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:31:30,877 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:31:34,208 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:31:34,208 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/svamp/dense/models/cv_svamp_augmented_fold0.
2022-09-10 07:31:39,730 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:31:48,821 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.08737864077669903	
 equation acc epoch: 0.07766990291262135	
 max val acc: 0.08737864077669903	
 equation acc: 0.07766990291262135	
2022-09-10 07:31:48,821 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 9s
2022-09-10 07:31:48,823 | INFO | main_after.py: 96 : main() ::	 Experiment Name: cv_svamp_augmented
2022-09-10 07:31:48,823 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:31:48,823 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:31:49,779 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:31:49,779 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 3920
2022-09-10 07:31:49,779 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 218
2022-09-10 07:31:49,779 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: ['0.01', '12.0', '1.0', '100.0', '0.1', '0.5', '3.0', '4.0', '7.0']
2022-09-10 07:31:49,779 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:31:49,779 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:31:50,346 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 4068 / 4068 = 1.0
2022-09-10 07:31:50,506 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 4071 words in input language, 21 words in output
2022-09-10 07:31:50,518 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:31:53,472 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:31:53,472 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:31:53,551 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:31:53,551 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/svamp/dense/models/cv_svamp_augmented_fold1.
2022-09-10 07:31:56,818 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:32:05,010 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.01834862385321101	
 equation acc epoch: 0.009174311926605505	
 max val acc: 0.01834862385321101	
 equation acc: 0.009174311926605505	
2022-09-10 07:32:05,010 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 8s
2022-09-10 07:32:05,013 | INFO | main_after.py: 96 : main() ::	 Experiment Name: cv_svamp_augmented
2022-09-10 07:32:05,013 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:32:05,013 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:32:05,963 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:32:05,963 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 3911
2022-09-10 07:32:05,963 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 227
2022-09-10 07:32:05,963 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: ['0.01', '12.0', '1.0', '100.0', '0.1', '0.5', '3.0', '4.0', '7.0']
2022-09-10 07:32:05,963 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:32:05,963 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:32:06,538 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 4068 / 4068 = 1.0
2022-09-10 07:32:06,705 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 4071 words in input language, 21 words in output
2022-09-10 07:32:06,716 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:32:09,559 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:32:09,559 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:32:09,638 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:32:09,638 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/svamp/dense/models/cv_svamp_augmented_fold2.
2022-09-10 07:32:12,647 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:32:21,292 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.06167400881057269	
 equation acc epoch: 0.0	
 max val acc: 0.06167400881057269	
 equation acc: 0.0	
2022-09-10 07:32:21,292 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 8s
2022-09-10 07:32:21,295 | INFO | main_after.py: 96 : main() ::	 Experiment Name: cv_svamp_augmented
2022-09-10 07:32:21,295 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:32:21,295 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:32:22,353 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:32:22,354 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 3973
2022-09-10 07:32:22,354 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 165
2022-09-10 07:32:22,354 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: ['0.01', '12.0', '1.0', '100.0', '0.1', '0.5', '3.0', '4.0', '7.0']
2022-09-10 07:32:22,354 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:32:22,354 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:32:22,939 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 4068 / 4068 = 1.0
2022-09-10 07:32:23,108 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 4071 words in input language, 21 words in output
2022-09-10 07:32:23,117 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:32:26,074 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:32:26,074 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:32:26,153 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:32:26,153 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/svamp/dense/models/cv_svamp_augmented_fold3.
2022-09-10 07:32:32,014 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:32:46,485 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.0	
 equation acc epoch: 0.0	
 max val acc: 0.0	
 equation acc: 0.0	
2022-09-10 07:32:46,485 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 14s
2022-09-10 07:32:46,487 | INFO | main_after.py: 96 : main() ::	 Experiment Name: cv_svamp_augmented
2022-09-10 07:32:46,487 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:32:46,487 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:32:47,519 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:32:47,520 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 3954
2022-09-10 07:32:47,520 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 184
2022-09-10 07:32:47,520 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: ['0.01', '12.0', '1.0', '100.0', '0.1', '0.5', '3.0', '4.0', '7.0']
2022-09-10 07:32:47,520 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:32:47,520 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:32:48,100 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 4068 / 4068 = 1.0
2022-09-10 07:32:48,270 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 4071 words in input language, 21 words in output
2022-09-10 07:32:48,280 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:32:51,168 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:32:51,168 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:32:51,249 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:32:51,249 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/svamp/dense/models/cv_svamp_augmented_fold4.
2022-09-10 07:32:57,225 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:33:14,481 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.0	
 equation acc epoch: 0.0	
 max val acc: 0.0	
 equation acc: 0.0	
2022-09-10 07:33:14,482 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 17s
Traceback (most recent call last):
  File "/home/sliu/miniconda3/envs/prune_cry/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/sliu/miniconda3/envs/prune_cry/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/gpfs/home6/sliu/Projects/SVAMP/code/gts/src/main_after.py", line 276, in <module>
    main()
  File "/gpfs/home6/sliu/Projects/SVAMP/code/gts/src/main_after.py", line 266, in main
    folds_scores.append(float(best_acc[w][0])/best_acc[w][1])
ZeroDivisionError: float division by zero
Transfer numbers...
initialize by one_shot_gm
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.9294929322507377
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.9631084833009079
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 1.0
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.47175089518229163
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.4798109266493056
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8837771945529513
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9001210530598959
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7484978569878472
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8485840691460503
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6046498616536458
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6239166259765625
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8560757107204862
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9033643934461806
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7138548956976997
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8299412197536893
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6409573025173612
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6591339111328125
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8229387071397569
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8834720187717013
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7068574693467882
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8161358303493924
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6157277425130208
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6404079861111112
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8081342909071181
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8545328776041666
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7202915615505643
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8247222900390625
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6056196424696181
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6308542887369792
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8086157904730903
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8565385606553819
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7478705512152778
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8468907674153646
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6342824300130208
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6579267713758681
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7763332790798612
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8403099907769097
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7678036159939237
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8670722113715278
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6364644368489583
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6565500895182292
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7839745415581597
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8414950900607638
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.749595218234592
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8551631503634982
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6048092312282987
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6257781982421875
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8517710367838541
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8979305691189237
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7448658413357205
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8482839796278212
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6308949788411458
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6509009467230903
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8355950249565972
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8973744710286459
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.754187266031901
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8434147304958768
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6403164333767362
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6567891438802083
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8594377305772569
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9116770426432291
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7958895365397135
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8478804694281684
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6320343017578125
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6531711154513888
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8583814832899306
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9078301323784722
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8331705729166666
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8691016303168403
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6222432454427083
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6322920057508681
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8102908664279513
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8887210422092013
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8140898810492622
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9206593831380209
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.9969499376085069
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.8296241760253906
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.9529809951782227
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.7729574839274088
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.9072847366333008
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.8760161399841309
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.9118890762329102
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.8448104858398438
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.8546657562255859
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.614959716796875
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.6273002624511719
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.5820388793945312
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.6323509216308594
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.47216796875
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.8464107513427734
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.6796875
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.6134745279947917
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.435546875
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.646484375
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.8966914585658482
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.9122423444475446
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.9022369384765625
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.9067633492606026
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.9616404942103794
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.9726159232003349
Final sparsity level of 0.8325: 0.8325000130144645
Transfer numbers...
initialize by one_shot_gm
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.9207638059948937
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.9641293774319066
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 1.0
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.48060946994357634
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.4886406792534722
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8905588785807291
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9056888156467013
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7569779290093316
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8555509779188368
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6140611436631944
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.633392333984375
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8634813096788194
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9086134168836806
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7227630615234375
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8374549018012153
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6507703993055556
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6686520046657987
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8312954372829862
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8893653021918403
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7157775031195747
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.823989020453559
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6259528266059028
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6506076388888888
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8172590467664931
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8614654541015625
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7291878594292535
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8325462341308594
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6158209906684028
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6409454345703125
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8170335557725694
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8632371690538194
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7566341824001737
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8542539808485243
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6443718804253472
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6681942409939237
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7853224012586806
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8475443522135416
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7762578328450521
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.873893313937717
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6469336615668403
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.666778564453125
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7933163113064237
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8488837348090278
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7584542168511285
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8622428046332465
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6149851481119792
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6361033121744792
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8590630425347222
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9035864935980903
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7540253533257378
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8557073805067275
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6416286892361112
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6615125868055556
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8439297146267362
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9035288492838541
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7633917066786025
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8510025872124566
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6514180501302083
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6676550971137153
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8669535319010416
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9176381429036459
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8047239515516493
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8558099534776475
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6431850857204862
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6641269259982638
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8665178087022569
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9137759738498263
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8419142829047309
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8769141303168403
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6332482231987847
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6434054904513888
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8195936414930556
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8957434760199653
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8233316209581163
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9263975355360243
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.9975670708550347
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.8325042724609375
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.9503250122070312
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.7497698465983074
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.8846511840820312
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.8458528518676758
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.8636970520019531
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.8234248161315918
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.8185844421386719
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.5720558166503906
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.6210517883300781
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.5452499389648438
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.6094875335693359
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.49462890625
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.8091220855712891
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.69921875
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.6083704630533855
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.498046875
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.5852864583333333
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.8899263654436383
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.8994413103376117
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.8918958391462054
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.897200448172433
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.9524710518973214
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.9605952671595982
Final sparsity level of 0.8325: 0.8325000268596396
Transfer numbers...
initialize by one_shot_gm
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.9218643273981233
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.9638608544098574
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 1.0
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.47671847873263884
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.4847700330946181
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8879411485460069
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9035746256510416
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7536515129937066
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.852958255343967
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.610321044921875
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6293436686197917
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8606787787543403
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9065907796223959
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.719259050157335
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.834525638156467
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6468522813585069
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6649136013454862
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8280809190538194
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8871222601996528
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7123947143554688
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.820977528889974
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6218855116102431
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6465793185763888
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8138326009114584
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8588222927517362
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7258563571506076
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.82958009507921
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6118655734592013
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6372053358289931
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8139834933810763
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.860687255859375
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.753312004937066
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8515023125542535
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6404995388454862
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6640404595269097
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7818518744574653
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8448944091796875
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.773095448811849
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8713823954264323
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6430681016710069
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6627383761935763
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7899898952907987
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8460049099392362
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.75507566663954
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8595945570203993
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6108042399088542
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6318071153428819
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8563961452907987
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9015858968098959
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7505230373806424
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.852821773952908
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6372358534071181
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6571926540798612
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8406541612413194
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9011620415581597
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7598576015896268
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8480317857530382
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6468082004123263
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.66302490234375
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8641781277126737
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9154001871744791
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8014522128634982
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8527675204806857
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6389889187282987
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6599917941623263
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8635999891493056
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9116566975911459
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8385632832845052
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8739772372775607
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6288876003689237
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6392923990885417
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8162095811631944
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8933037651909722
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8196809556749132
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.92425537109375
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.9973551432291666
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.8481521606445312
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.959625244140625
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.7573769887288412
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.8930263519287109
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.8489527702331543
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.8494482040405273
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.8494076728820801
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.8530826568603516
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.6015777587890625
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.6293258666992188
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.5627975463867188
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.6226520538330078
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.5126953125
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.8194751739501953
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.6796875
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.6167526245117188
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.4921875
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.58984375
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.9026314871651786
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.9193224225725446
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.8988233293805804
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.909623282296317
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.9575511387416294
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.9645353044782367
Final sparsity level of 0.8325: 0.832500019937052
Transfer numbers...
initialize by one_shot_gm
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.8994992425561192
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.9646309581712063
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 1.0
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.4899766710069444
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.4978705512152778
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8970743815104166
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9110785590277778
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7656834920247396
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8626213073730469
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6246795654296875
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6438496907552083
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8710801866319444
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9140777587890625
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7317564222547743
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8451105753580729
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6616465250651042
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6793789333767362
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8398267957899306
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8954484727647569
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7249281141493056
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8319502936469184
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6366729736328125
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6614566379123263
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8266245524088541
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.86865234375
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.738236321343316
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8404290941026475
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6267513699001737
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6520165337456597
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8257717556423612
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8703545464409722
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7653804355197482
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8616689046223959
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6551700168185763
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6788143581814237
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7946929931640625
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8552229139539931
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7849375406901041
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8810068766276041
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6580573187934028
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6777937147352431
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8029496934678819
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8567555745442709
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7673322889539931
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8695271809895834
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6262664794921875
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6472405327690972
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8663482666015625
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9094560411241319
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7631501091851128
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.863236321343316
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6533237033420138
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6729855007595487
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8521813286675347
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9097493489583334
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7726673550075955
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8588684929741753
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.66302490234375
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6790093315972222
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8745235866970487
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9234670003255209
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.813704596625434
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.863975101047092
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6546834309895833
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6757032606336806
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8744388156467013
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9199473063151041
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8503540886773003
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8846346537272135
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6449652777777778
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6553395589192708
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8290201822916666
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9026540120442709
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.832604726155599
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9321751064724393
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.9981129964192709
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.8392365773518881
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.9542818069458008
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.7516460418701172
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.8890657424926758
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.8506536483764648
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.85137939453125
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.8517942428588867
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.8524951934814453
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.5650749206542969
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.6053981781005859
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.5414543151855469
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.6000328063964844
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.519287109375
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.7886829376220703
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.681640625
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.6106427510579426
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.533203125
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.5673828125
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.8944222586495536
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.9040233067103794
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.8914555140904018
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.8927536010742188
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.9551348005022321
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.9603151593889508
Final sparsity level of 0.8325: 0.8325000130144645
Transfer numbers...
initialize by one_shot_gm
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.9338431832123081
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.9634732692931258
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 1.0
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.47033352322048616
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.4785139295789931
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8824513753255209
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8986968994140625
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7467405531141493
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8471849229600694
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6026763916015625
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6219940185546875
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8544786241319444
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9023590087890625
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7119382222493489
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.828422122531467
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6389431423611112
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6573621961805556
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8213551839192709
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8823123508029513
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7051018608940972
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8143997192382812
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6138746473524306
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6384141710069444
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8061608208550347
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8530748155381944
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7185079786512587
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8232735527886285
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6035800509982638
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6289452446831597
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8067796495225694
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8551211886935763
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7461602952745225
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8453954060872396
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6323530409071181
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6561516655815972
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7742869059244791
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8388010660807291
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7659678988986545
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8656145731608073
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6346164279513888
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6544477674696181
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.7819942898220487
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8397013346354166
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7476213243272569
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8536775377061632
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6027255588107638
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6237335205078125
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8500637478298612
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8966437445746528
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7430013020833333
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.846761491563585
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.628936767578125
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6487765842013888
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8334943983289931
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8959927029079862
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7522735595703125
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8418015374077691
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6384802924262153
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6547410753038194
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8574540879991319
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9102427164713541
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7940194871690538
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8461736043294271
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6298472086588542
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6510467529296875
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8564910888671875
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9063805474175347
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8312038845486112
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.867454104953342
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6196967230902778
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6302286783854167
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8082105848524306
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8870069715711806
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8119994269476997
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.919386969672309
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.9967922634548612
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.8205204010009766
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.9483413696289062
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.7614943186442057
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.9016809463500977
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.8596692085266113
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.9044914245605469
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.8277339935302734
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.8321847915649414
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.6038742065429688
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.6600494384765625
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.5803337097167969
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.6529788970947266
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.487060546875
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.8300819396972656
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.716796875
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.6256446838378906
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.43359375
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.6471354166666667
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.9134466988699776
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.9294466291155133
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.9223131452287946
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.9325812203543526
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.9767674037388393
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.9840709141322544
Final sparsity level of 0.8325: 0.8325000130144645
wandb: WARNING W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.
2022-09-10 07:33:17,591 | INFO | main_after.py: 96 : main() ::	 Experiment Name: cv_svamp_augmented
2022-09-10 07:33:17,591 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:33:17,591 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:33:18,636 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:33:18,636 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 3932
2022-09-10 07:33:18,636 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 206
2022-09-10 07:33:18,636 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: ['0.01', '12.0', '1.0', '100.0', '0.1', '0.5', '3.0', '4.0', '7.0']
2022-09-10 07:33:18,636 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:33:18,636 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:33:19,211 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 4068 / 4068 = 1.0
2022-09-10 07:33:19,377 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 4071 words in input language, 21 words in output
2022-09-10 07:33:19,386 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:33:22,370 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:33:22,370 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:33:25,668 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:33:25,668 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/svamp/dense/models/cv_svamp_augmented_fold0.
2022-09-10 07:33:31,840 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:33:40,248 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.05825242718446602	
 equation acc epoch: 0.04854368932038835	
 max val acc: 0.05825242718446602	
 equation acc: 0.04854368932038835	
2022-09-10 07:33:40,248 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 8s
2022-09-10 07:33:40,251 | INFO | main_after.py: 96 : main() ::	 Experiment Name: cv_svamp_augmented
2022-09-10 07:33:40,251 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:33:40,251 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:33:41,226 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:33:41,227 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 3920
2022-09-10 07:33:41,227 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 218
2022-09-10 07:33:41,227 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: ['0.01', '12.0', '1.0', '100.0', '0.1', '0.5', '3.0', '4.0', '7.0']
2022-09-10 07:33:41,227 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:33:41,227 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:33:41,798 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 4068 / 4068 = 1.0
2022-09-10 07:33:41,959 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 4071 words in input language, 21 words in output
2022-09-10 07:33:41,971 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:33:44,954 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:33:44,954 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:33:45,034 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:33:45,034 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/svamp/dense/models/cv_svamp_augmented_fold1.
2022-09-10 07:33:50,997 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:34:02,155 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.013761467889908258	
 equation acc epoch: 0.0045871559633027525	
 max val acc: 0.013761467889908258	
 equation acc: 0.0045871559633027525	
2022-09-10 07:34:02,155 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 11s
2022-09-10 07:34:02,158 | INFO | main_after.py: 96 : main() ::	 Experiment Name: cv_svamp_augmented
2022-09-10 07:34:02,158 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:34:02,158 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:34:03,122 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:34:03,122 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 3911
2022-09-10 07:34:03,122 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 227
2022-09-10 07:34:03,122 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: ['0.01', '12.0', '1.0', '100.0', '0.1', '0.5', '3.0', '4.0', '7.0']
2022-09-10 07:34:03,122 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:34:03,122 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:34:03,698 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 4068 / 4068 = 1.0
2022-09-10 07:34:03,862 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 4071 words in input language, 21 words in output
2022-09-10 07:34:03,873 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:34:06,952 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:34:06,953 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:34:07,031 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:34:07,031 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/svamp/dense/models/cv_svamp_augmented_fold2.
2022-09-10 07:34:12,665 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:34:30,748 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.013215859030837005	
 equation acc epoch: 0.0	
 max val acc: 0.013215859030837005	
 equation acc: 0.0	
2022-09-10 07:34:30,748 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 18s
2022-09-10 07:34:30,751 | INFO | main_after.py: 96 : main() ::	 Experiment Name: cv_svamp_augmented
2022-09-10 07:34:30,751 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:34:30,751 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:34:31,716 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:34:31,716 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 3973
2022-09-10 07:34:31,716 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 165
2022-09-10 07:34:31,716 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: ['0.01', '12.0', '1.0', '100.0', '0.1', '0.5', '3.0', '4.0', '7.0']
2022-09-10 07:34:31,716 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:34:31,716 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:34:32,292 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 4068 / 4068 = 1.0
2022-09-10 07:34:32,463 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 4071 words in input language, 21 words in output
2022-09-10 07:34:32,473 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:34:35,369 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:34:35,369 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:34:35,448 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:34:35,448 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/svamp/dense/models/cv_svamp_augmented_fold3.
2022-09-10 07:34:40,434 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:35:05,483 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.0	
 equation acc epoch: 0.0	
 max val acc: 0.0	
 equation acc: 0.0	
2022-09-10 07:35:05,483 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 25s
2022-09-10 07:35:05,485 | INFO | main_after.py: 96 : main() ::	 Experiment Name: cv_svamp_augmented
2022-09-10 07:35:05,485 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:35:05,485 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:35:06,453 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:35:06,453 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 3954
2022-09-10 07:35:06,453 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 184
2022-09-10 07:35:06,453 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: ['0.01', '12.0', '1.0', '100.0', '0.1', '0.5', '3.0', '4.0', '7.0']
2022-09-10 07:35:06,453 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:35:06,453 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:35:07,033 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 4068 / 4068 = 1.0
2022-09-10 07:35:07,206 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 4071 words in input language, 21 words in output
2022-09-10 07:35:07,216 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:35:10,076 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:35:10,076 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:35:10,155 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:35:10,156 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/svamp/dense/models/cv_svamp_augmented_fold4.
2022-09-10 07:35:13,626 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:35:25,804 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.0	
 equation acc epoch: 0.0	
 max val acc: 0.0	
 equation acc: 0.0	
2022-09-10 07:35:25,804 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 12s
Traceback (most recent call last):
  File "/home/sliu/miniconda3/envs/prune_cry/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/sliu/miniconda3/envs/prune_cry/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/gpfs/home6/sliu/Projects/SVAMP/code/gts/src/main_after.py", line 276, in <module>
    main()
  File "/gpfs/home6/sliu/Projects/SVAMP/code/gts/src/main_after.py", line 266, in main
    folds_scores.append(float(best_acc[w][0])/best_acc[w][1])
ZeroDivisionError: float division by zero
Transfer numbers...
initialize by one_shot_gm
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.936866404928877
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.9715238934824902
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 1.0
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5277116563585069
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.536102294921875
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9192165798611112
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9302961561414931
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.799995846218533
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8878101772732205
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6653883192274306
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6843922932942708
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8967573377821181
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9320543077256944
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7671275668674045
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8727073669433594
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7036488850911458
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7195621066623263
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8684268527560763
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9162072075737847
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7604895697699653
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8605605231391059
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6779581705729167
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7020670572916667
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8573828803168403
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8919932047526041
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7733692593044705
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8680157131618924
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6675381130642362
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6925743950737847
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8546617296006944
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8927510579427084
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7988103230794271
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8868492974175347
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6954837375217013
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7185329861111112
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8259785970052084
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8791792127821181
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8176248338487413
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9042616950141059
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6982896592881944
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7175021701388888
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8337317572699653
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8821173773871528
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8010105556911893
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8937543233235677
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6673024495442708
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6879611545138888
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8903995090060763
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9282175699869791
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7975044250488281
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8884230719672309
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6955244276258681
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7146759033203125
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8802676730685763
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9300486246744791
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8071445888943143
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.88475587632921
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7054392496744792
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7203470865885417
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8998497856987847
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9425930447048612
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8462223476833768
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8900989956325955
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6975165473090278
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7180633544921875
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9003668891059028
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9399447970920138
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8810340033637153
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9089817470974393
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6870812310112847
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6973893907335069
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8599633110894097
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9256625705295138
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8648796081542969
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9490297105577257
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.9992167154947916
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.8547579447428385
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.9620742797851562
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.8022378285725912
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.9218454360961914
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.897852897644043
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.9351119995117188
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.8701467514038086
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.8891048431396484
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.6729545593261719
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.6828517913818359
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.6403121948242188
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.6910057067871094
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.516845703125
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.8681144714355469
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.69140625
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.6586545308430989
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.466796875
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.6598307291666667
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.9240635463169643
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.9373506818498883
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.9311468941824776
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.9346346173967633
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.9759881155831474
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.9845352172851562
Final sparsity level of 0.866: 0.8660000270257817
Transfer numbers...
initialize by one_shot_gm
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.929439776476342
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.9725219884889753
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 1.0
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5368635389539931
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5453559027777778
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9243350558810763
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9346805148654513
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8083788553873698
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8935415479871962
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6749945746527778
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6943596733940972
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9027828640407987
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9360182020399306
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7757805718315972
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8791961669921875
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7136518690321181
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7293260362413194
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8749932183159722
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9206899007161459
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7692972819010416
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8672188652886285
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6880238850911458
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7118818495008681
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.864654541015625
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8973337809244791
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.782059563530816
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.874352773030599
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6773732503255208
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7023722330729167
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8615485297309028
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8980068630642362
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8071539137098525
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8927459716796875
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7052391899956597
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7280680338541667
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8333672417534722
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8846503363715278
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8256280687120225
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9095882839626737
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7082248263888888
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7274136013454862
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8412255181206597
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8881869845920138
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.80938720703125
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8992474873860677
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6772816975911458
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6976793077256944
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8963334825303819
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9325408935546875
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8060133192274306
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8940531412760416
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7059563530815972
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7248992919921875
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8868696424696181
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9346669514973959
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8155483669704862
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8905512491861979
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7156728108723958
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7302669949001737
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9056939019097222
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9466044108072916
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8541289435492622
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8959282769097222
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7078009711371528
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7281053331163194
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9066094292534722
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9444325764973959
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8881992763943143
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9145350986056857
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6973826090494792
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7076839870876737
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8674858940972222
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9311235215928819
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.872573004828559
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9527189466688368
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.9993760850694444
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.8569755554199219
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.9597034454345703
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.7804152170817057
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.9013557434082031
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.870781421661377
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.8959722518920898
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.8504080772399902
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.8578271865844727
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.6291999816894531
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.676422119140625
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.6023597717285156
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.6673202514648438
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.54638671875
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.8310222625732422
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.72265625
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.6535428365071614
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.529296875
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.6038411458333333
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.9187741960797992
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.9271970476422992
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.9226553780691964
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.9269965035574776
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.9696044921875
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.9765951974051339
Final sparsity level of 0.866: 0.8660000201031942
Transfer numbers...
initialize by one_shot_gm
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.9302882742299148
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.9721572024967574
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 1.0
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5329538981119792
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5411207411024306
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9223192003038194
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9330529106987847
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8050248887803819
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8913209703233507
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.670989990234375
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6899990505642362
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9004855685763888
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9345245361328125
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7722943623860677
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8766937255859375
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7096065945095487
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7255164252387153
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8726416693793403
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9189063178168403
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.765862782796224
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8645574781629775
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6841447618272569
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7079925537109375
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8619011773003472
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8953264024522569
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.778680165608724
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.871917724609375
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6733500162760417
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6984795464409722
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.858917236328125
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8959774441189237
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8038885328504775
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8905593024359809
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7012752956814237
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7242380777994792
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8305206298828125
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8825903998480903
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8225292629665799
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9075537787543403
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7044932047526042
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7235090467664931
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8384823269314237
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8859710693359375
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8062672085232205
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8971871270073785
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6731414794921875
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6937120225694444
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.894073486328125
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9309573703342013
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8027258978949653
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.891869862874349
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7015601264105903
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7206556532118056
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8842841254340278
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9329613579644097
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8122918870713975
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8882196214463975
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7113460964626737
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7262013753255208
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9034627278645834
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.945037841796875
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8510653177897135
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8936733669704862
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7038523356119792
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7242906358506944
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9043155246310763
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9428032769097222
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8854505750868056
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9125082227918837
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6936119927300347
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7036878797743056
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8647037082248263
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9291975233289931
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8694962395562066
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9513320922851562
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.9993133544921875
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.8711776733398438
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.9674348831176758
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.7870063781738281
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.9086198806762695
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.873621940612793
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.8856735229492188
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.8738231658935547
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.8867874145507812
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.6592369079589844
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.6858558654785156
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.6205558776855469
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.6808891296386719
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.566650390625
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.842071533203125
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.69140625
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.6616350809733074
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.51171875
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.6064453125
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.9296667916434151
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.943861825125558
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.9288526262555804
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.9375653948102679
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.9740851266043526
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.9795554024832589
Final sparsity level of 0.866: 0.8660000201031942
Transfer numbers...
initialize by one_shot_gm
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.9151875528449219
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.9728361097600519
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 1.0
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5443200005425347
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5526919894748263
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9286244710286459
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9382798936631944
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.815267350938585
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.898233625623915
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6831987169053819
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7024807400173612
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.90771484375
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9392276340060763
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7828920152452257
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8844066196017795
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7216152615017362
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7375590006510417
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8804287380642362
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.924591064453125
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7764316134982638
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8725102742513021
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6963890923394097
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7197791205512153
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8705919053819444
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9020012749565972
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7889493306477865
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8794898986816406
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6854960123697917
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7104610866970487
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8673180474175347
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9023691813151041
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8136855231391059
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8975067138671875
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7134229871961806
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7359280056423612
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8397589789496528
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8895941840277778
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8321846856011285
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9139154222276475
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7165578206380208
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7353227403428819
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8477969699435763
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8934393988715278
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8161752488878038
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9036971198187934
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6850399441189237
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7056545681423612
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9011603461371528
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9361453586154513
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8127644856770834
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8986659579806857
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7139265272352431
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7326304117838542
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8921729193793403
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9385291205512153
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8223368326822916
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8952543470594618
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7237006293402778
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7381066216362847
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9107903374565972
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9501003689236112
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8605461120605469
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9008441501193576
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7159356011284722
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.73583984375
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9117312961154513
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9481201171875
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8939806620279948
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9192059834798177
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7057969835069444
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7157728407118056
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8738369411892362
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9354570176866319
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8787583245171441
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9557829962836372
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.9995032416449653
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.8610591888427734
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.9619770050048828
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.7805569966634115
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.9047079086303711
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.8734579086303711
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.8828096389770508
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.874424934387207
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.8833913803100586
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.6193161010742188
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.6586475372314453
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.5958404541015625
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.654327392578125
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.5712890625
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.8143386840820312
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.703125
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.6544532775878906
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.560546875
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.5865885416666667
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.920623779296875
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.929335457938058
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.9200483049665179
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.9213485717773438
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.9705429077148438
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.9754671369280133
Final sparsity level of 0.866: 0.8660000339483692
Transfer numbers...
initialize by one_shot_gm
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.9408220804734905
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.9718532141699092
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 1.0
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5261671278211806
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5344560411241319
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.91796875
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9293178982204862
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7982771131727431
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8865326775444878
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6633978949652778
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6824357774522569
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8954874674479166
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9312405056423612
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7653177049424913
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8714324103461372
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7016279432508681
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7177547878689237
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8669772677951388
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9151187472873263
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7587653266059028
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8591181437174479
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6759219699435763
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6999867757161458
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8558213975694444
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8909115261501737
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7716700236002604
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8668259514702691
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6653120252821181
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6906924777560763
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8531341552734375
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8915863037109375
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7971844143337674
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8857366773817275
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6935662163628472
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7166375054253472
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8243594699435763
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8778296576605903
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8160243564181857
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.90312745836046
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6965738932291667
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7157830132378472
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8322635226779513
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.8807457817925347
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7993562486436632
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8925442165798612
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6653493245442708
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6859792073567708
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8892076280381944
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9272918701171875
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.7958420647515191
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8872354295518663
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6938646104600694
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7127939860026042
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8789469401041666
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9289703369140625
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8054250081380209
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8835228814019097
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7034403483072917
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7184939914279513
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.89837646484375
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9413570827907987
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8447291056315104
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8888566758897569
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6955210367838542
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7159322102864583
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8990224202473959
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9388444688585069
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8794500562879775
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9078386094835069
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.6849483913845487
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6952853732638888
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8581661648220487
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9244842529296875
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8631579081217448
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9481743706597222
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.9991658528645834
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.8474960327148438
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.958552360534668
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.7925771077473959
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.9171857833862305
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.8842630386352539
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.9300403594970703
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.8548984527587891
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.8697214126586914
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.6624488830566406
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.7151813507080078
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.6397209167480469
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.7118721008300781
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.541748046875
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.8483753204345703
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.7265625
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.6665802001953125
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.474609375
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.6614583333333333
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.9374683925083706
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.9509233747209821
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.9466836111886161
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.9550083705357143
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.9864774431501117
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.9915509905133929
Final sparsity level of 0.866: 0.8660000131806067
wandb: WARNING W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.
2022-09-10 07:35:28,864 | INFO | main_after.py: 96 : main() ::	 Experiment Name: cv_svamp_augmented
2022-09-10 07:35:28,864 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:35:28,864 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:35:29,903 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:35:29,903 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 3932
2022-09-10 07:35:29,903 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 206
2022-09-10 07:35:29,903 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: ['0.01', '12.0', '1.0', '100.0', '0.1', '0.5', '3.0', '4.0', '7.0']
2022-09-10 07:35:29,903 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:35:29,903 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:35:30,481 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 4068 / 4068 = 1.0
2022-09-10 07:35:30,646 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 4071 words in input language, 21 words in output
2022-09-10 07:35:30,655 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:35:33,694 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:35:33,694 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:35:37,014 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:35:37,014 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/svamp/dense/models/cv_svamp_augmented_fold0.
2022-09-10 07:35:43,606 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:35:52,390 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.06310679611650485	
 equation acc epoch: 0.05825242718446602	
 max val acc: 0.06310679611650485	
 equation acc: 0.05825242718446602	
2022-09-10 07:35:52,390 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 8s
2022-09-10 07:35:52,393 | INFO | main_after.py: 96 : main() ::	 Experiment Name: cv_svamp_augmented
2022-09-10 07:35:52,393 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:35:52,393 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:35:53,356 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:35:53,356 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 3920
2022-09-10 07:35:53,356 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 218
2022-09-10 07:35:53,356 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: ['0.01', '12.0', '1.0', '100.0', '0.1', '0.5', '3.0', '4.0', '7.0']
2022-09-10 07:35:53,356 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:35:53,356 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:35:53,926 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 4068 / 4068 = 1.0
2022-09-10 07:35:54,088 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 4071 words in input language, 21 words in output
2022-09-10 07:35:54,100 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:35:57,035 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:35:57,035 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:35:57,115 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:35:57,115 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/svamp/dense/models/cv_svamp_augmented_fold1.
2022-09-10 07:36:02,771 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:36:13,132 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.009174311926605505	
 equation acc epoch: 0.0	
 max val acc: 0.009174311926605505	
 equation acc: 0.0	
2022-09-10 07:36:13,132 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 10s
2022-09-10 07:36:13,134 | INFO | main_after.py: 96 : main() ::	 Experiment Name: cv_svamp_augmented
2022-09-10 07:36:13,134 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:36:13,134 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:36:14,094 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:36:14,094 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 3911
2022-09-10 07:36:14,094 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 227
2022-09-10 07:36:14,094 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: ['0.01', '12.0', '1.0', '100.0', '0.1', '0.5', '3.0', '4.0', '7.0']
2022-09-10 07:36:14,094 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:36:14,094 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:36:14,676 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 4068 / 4068 = 1.0
2022-09-10 07:36:14,842 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 4071 words in input language, 21 words in output
2022-09-10 07:36:14,853 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:36:17,730 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:36:17,731 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:36:17,810 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:36:17,810 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/svamp/dense/models/cv_svamp_augmented_fold2.
2022-09-10 07:36:23,950 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:36:41,611 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.00881057268722467	
 equation acc epoch: 0.0	
 max val acc: 0.00881057268722467	
 equation acc: 0.0	
2022-09-10 07:36:41,611 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 17s
2022-09-10 07:36:41,613 | INFO | main_after.py: 96 : main() ::	 Experiment Name: cv_svamp_augmented
2022-09-10 07:36:41,613 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:36:41,613 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:36:42,572 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:36:42,572 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 3973
2022-09-10 07:36:42,572 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 165
2022-09-10 07:36:42,572 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: ['0.01', '12.0', '1.0', '100.0', '0.1', '0.5', '3.0', '4.0', '7.0']
2022-09-10 07:36:42,572 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:36:42,572 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:36:43,148 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 4068 / 4068 = 1.0
2022-09-10 07:36:43,318 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 4071 words in input language, 21 words in output
2022-09-10 07:36:43,327 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:36:46,196 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:36:46,196 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:36:46,274 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:36:46,274 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/svamp/dense/models/cv_svamp_augmented_fold3.
2022-09-10 07:36:49,512 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:37:23,723 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.006060606060606061	
 equation acc epoch: 0.0	
 max val acc: 0.006060606060606061	
 equation acc: 0.0	
2022-09-10 07:37:23,723 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 34s
2022-09-10 07:37:23,725 | INFO | main_after.py: 96 : main() ::	 Experiment Name: cv_svamp_augmented
2022-09-10 07:37:23,725 | DEBUG | main_after.py: 97 : main() ::	 Created Relevant Directories
2022-09-10 07:37:23,725 | INFO | main_after.py: 99 : main() ::	 Loading Data...
2022-09-10 07:37:24,688 | DEBUG | main_after.py: 104 : main() ::	 Data Loaded...
2022-09-10 07:37:24,688 | DEBUG | main_after.py: 105 : main() ::	 Number of Training Examples: 3954
2022-09-10 07:37:24,688 | DEBUG | main_after.py: 106 : main() ::	 Number of Testing Examples: 184
2022-09-10 07:37:24,689 | DEBUG | main_after.py: 107 : main() ::	 Extra Numbers: ['0.01', '12.0', '1.0', '100.0', '0.1', '0.5', '3.0', '4.0', '7.0']
2022-09-10 07:37:24,689 | DEBUG | main_after.py: 108 : main() ::	 Maximum Number of Numbers: 7
2022-09-10 07:37:24,689 | INFO | main_after.py: 116 : main() ::	 Creating Vocab...
2022-09-10 07:37:25,269 | DEBUG | pre_data.py: 42 : trim() ::	 keep_words 4068 / 4068 = 1.0
2022-09-10 07:37:25,440 | DEBUG | pre_data.py: 279 : prepare_data() ::	 Indexed 4071 words in input language, 21 words in output
2022-09-10 07:37:25,451 | INFO | main_after.py: 124 : main() ::	 Initializing Models...
2022-09-10 07:37:28,339 | DEBUG | main_after.py: 142 : main() ::	 Models Initialized
2022-09-10 07:37:28,340 | INFO | main_after.py: 144 : main() ::	 Loading Models on GPU 0...
2022-09-10 07:37:28,418 | DEBUG | main_after.py: 154 : main() ::	 Models loaded on GPU 0
2022-09-10 07:37:28,418 | INFO | main_after.py: 161 : main() ::	 loading pretrained models from /home/sliu/project_space/pruning_cfails/Math/gts/svamp/dense/models/cv_svamp_augmented_fold4.
2022-09-10 07:37:31,381 | INFO | main_after.py: 196 : main() ::	 Starting Validation
2022-09-10 07:37:44,571 | INFO | logger.py: 33 : print_log() ::	 
 val acc epoch: 0.0	
 equation acc epoch: 0.0	
 max val acc: 0.0	
 equation acc: 0.0	
2022-09-10 07:37:44,571 | DEBUG | main_after.py: 259 : main() ::	 Validation Completed...
Time Taken: 0h 0m 13s
Traceback (most recent call last):
  File "/home/sliu/miniconda3/envs/prune_cry/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/sliu/miniconda3/envs/prune_cry/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/gpfs/home6/sliu/Projects/SVAMP/code/gts/src/main_after.py", line 276, in <module>
    main()
  File "/gpfs/home6/sliu/Projects/SVAMP/code/gts/src/main_after.py", line 266, in main
    folds_scores.append(float(best_acc[w][0])/best_acc[w][1])
ZeroDivisionError: float division by zero
Transfer numbers...
initialize by one_shot_gm
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.9435824764166584
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.9777860530155642
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 1.0
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5782708062065972
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.587188720703125
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9453277587890625
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9518212212456597
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8439576890733507
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9172554016113281
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7185465494791667
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7367909749348958
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9270850287543403
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9515940348307291
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8132137722439237
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9050178527832031
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7561764187282987
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7717725965711806
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9026031494140625
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9391733805338541
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8069652981228299
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8942286173502604
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7312910291883681
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.75433349609375
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8949161105685763
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9199659559461806
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8187870449490018
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9005868699815538
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7203691270616319
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7448560926649306
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8903605143229166
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9201592339409722
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8416108025444878
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9167599148220487
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7479214138454862
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7688632541232638
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8657243516710069
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9088270399305556
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8593597412109375
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9312994215223525
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7509918212890625
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7685411241319444
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8737369113498263
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9134945339626737
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8443433973524306
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9218779669867622
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7203301323784722
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7402716742621528
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9200693766276041
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9503038194444444
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8416493733723959
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9175847371419271
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7498338487413194
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7676120334201388
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9141116672092013
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9535098605685763
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8512085808648003
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.91522216796875
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7595638699001737
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7730153401692709
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9301215277777778
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9633772108289931
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8871756659613715
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.920654296875
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7520785861545138
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7718878851996528
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9315982394748263
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9618733723958334
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.9176606072319878
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9373580084906684
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7424553765190972
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7517055935329862
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8987698025173612
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9518500434027778
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.904443105061849
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9674097696940104
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.9998440212673612
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.8765818277994791
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.9694509506225586
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.8286285400390625
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.934290885925293
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.9159364700317383
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.9519834518432617
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.8915128707885742
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.9156093597412109
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.7248344421386719
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.7328224182128906
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.6938514709472656
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.7431068420410156
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.566650390625
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.8868274688720703
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.705078125
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.6986719767252605
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.5
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.6783854166666667
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.944718497140067
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.9555653163364956
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.9519838605608258
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.9549462454659599
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.9850921630859375
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.9914463588169643
Final sparsity level of 0.893: 0.8930000220968994
Transfer numbers...
initialize by one_shot_gm
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.9372102854869193
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.9785206914721142
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 1.0
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5878838433159722
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5965203179253472
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9491950141059028
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9551323784722222
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8513598971896701
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9219033983018663
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7278866238064237
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7458326551649306
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9318440755208334
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9545542399088541
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8211873372395834
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9102545844184028
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7652621799045138
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7808736165364584
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9081183539496528
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9428761800130209
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8149545457628038
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8997251722547743
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7406667073567708
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7635972764756944
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9010416666666666
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9243808322482638
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8265520731608073
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9058329264322916
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7299092610677083
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7543521457248263
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8962368435329862
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9247046576605903
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8491299947102865
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9216062757703993
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7570868598090278
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7778710259331597
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8723907470703125
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9136267768012153
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8661914401584201
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9354837205674913
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7602691650390625
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7775251600477431
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8800184461805556
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9185299343532987
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.851724836561415
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9263174268934462
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7297431098090278
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7497100830078125
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9246554904513888
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9536607530381944
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8490397135416666
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9221683078342013
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7597317165798612
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7770250108506944
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9195624457465278
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9569871690538194
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8586697048611112
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9199545118543837
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7691158718532987
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7821773952907987
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.93475341796875
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9664103190104166
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8938471476236979
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9254451327853732
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7618136935763888
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7811381022135416
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9364267985026041
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9651099310980903
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.9234051174587674
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9416830274793837
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7523142496744791
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7614322238498263
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9049953884548612
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9559139675564237
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.9107076856825087
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9700249565972222
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.9998847113715278
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.8786411285400391
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.9670839309692383
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.8086490631103516
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.916041374206543
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.8920907974243164
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.9210948944091797
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.8738846778869629
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.8891448974609375
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.6826362609863281
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.7261466979980469
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.6559638977050781
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.7199211120605469
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.595458984375
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.8506622314453125
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.73828125
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.6943995157877605
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.552734375
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.6253255208333333
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.9407969883510044
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.9478585379464286
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.9454269409179688
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.9490247453962054
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.9808175223214286
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.986647469656808
Final sparsity level of 0.893: 0.8930000082517243
Transfer numbers...
initialize by one_shot_gm
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.9378674794422892
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.9782851005188068
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 1.0
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5836334228515625
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5924394395616319
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9477437337239584
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.953857421875
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8483890957302518
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9200032552083334
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7238583034939237
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7418297661675347
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9299350314670138
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9533877902560763
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.81812498304579
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9082361857096354
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7617560492621528
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7772437201605903
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9059906005859375
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9414655897352431
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8117807176378038
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8975431654188368
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7367655436197917
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7599419487847222
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8987240261501737
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9228159586588541
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8234490288628472
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9037933349609375
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7260521782769097
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7504950629340278
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8940395779079862
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9230787489149306
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8462596469455295
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9197196960449219
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7533365885416666
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7743292914496528
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8697001139322916
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9119059244791666
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8635491265190972
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9338565402560763
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7566189236111112
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7740190294053819
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8775634765625
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9166378445095487
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8488697475857205
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9246686299641927
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7259589301215278
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7457733154296875
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9228312174479166
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9523349338107638
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8460922241210938
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9203681945800781
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7555999755859375
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7731679280598959
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.917449951171875
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9557037353515625
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.855733659532335
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9180954827202691
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7652113172743056
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7784288194444444
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9329545762803819
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9651082356770834
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.891159905327691
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9235509236653646
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7577395968967013
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7773759629991319
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9346839057074653
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9640265570746528
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.921112060546875
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9400147332085503
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7484690348307292
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7574818929036459
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9027336968315972
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9543067084418403
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.908288319905599
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9690204196506076
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.9998745388454862
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.8912525177001953
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.9738636016845703
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.8141969045003256
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.92236328125
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.8946428298950195
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.9140071868896484
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.8945713043212891
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.9135370254516602
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.7119827270507812
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.7359695434570312
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.6744804382324219
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.7334022521972656
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.616943359375
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.8621139526367188
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.712890625
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.7016830444335938
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.54296875
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.6233723958333333
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.9497179303850446
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.9613189697265625
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.951279776436942
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.9578138078962054
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.9844480242047992
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.9886670793805804
Final sparsity level of 0.893: 0.8930000082517243
Transfer numbers...
initialize by one_shot_gm
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.9257209446185218
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.9789412086575875
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 1.0
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5949961344401042
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.6036037868923612
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.952362060546875
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9578603108723959
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8572319878472222
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9255040486653646
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7353702121310763
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7530246310763888
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9354977077907987
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9568549262152778
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8273374769422743
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9142061869303385
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7725304497612847
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7878774007161459
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9125213623046875
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9457600911458334
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8211101955837674
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9039353264702691
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7482638888888888
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7707112630208334
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9060041639539931
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9280209011501737
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8323805067274306
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9099438985188802
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7372538248697917
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7615152994791666
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9008127848307291
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9283175998263888
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8546070522732205
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9253188239203559
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7645484076605903
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7846934000651041
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8776262071397569
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9176618787977431
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8716422186957465
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9387893676757812
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7675408257378472
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7847730848524306
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8853929307725694
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9225938585069444
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8574133978949653
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.929833730061849
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7371826171875
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7569427490234375
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9284006754557291
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9563920762803819
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8546905517578125
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9257931179470487
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7668575710720487
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7840389675564237
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9236433241102431
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9597490098741319
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8642578125
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9236683315700955
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7764994303385416
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7893134223090278
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9383578830295138
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9687856038411459
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8988613552517362
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.929146236843533
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7688852945963541
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7880435519748263
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9401821560329862
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9676988389756944
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.9277670118543837
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9450018141004775
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7598130967881944
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7688479953342013
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9099951850043403
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9587470160590278
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.9155625237358941
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9721065097384982
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.9999203152126737
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.8812065124511719
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.968724250793457
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.8082695007324219
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.9188432693481445
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.8939104080200195
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.9090909957885742
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.8944354057312012
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.9091758728027344
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.6724090576171875
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.7087898254394531
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.6483955383300781
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.7064933776855469
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.623779296875
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.8374595642089844
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.720703125
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.6967315673828125
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.607421875
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.6048177083333333
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.9414400373186383
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.9489560808454242
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.94342041015625
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.9441201346261161
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.9811488560267857
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.9854670933314732
Final sparsity level of 0.893: 0.8930000359420744
Transfer numbers...
initialize by one_shot_gm
sparsity of layer roberta_layer.embeddings.word_embeddings.weight with tensor torch.Size([50265, 768]) is 0.9471670977154415
sparsity of layer roberta_layer.embeddings.position_embeddings.weight with tensor torch.Size([514, 768]) is 0.9781204401750972
sparsity of layer roberta_layer.embeddings.token_type_embeddings.weight with tensor torch.Size([1, 768]) is 1.0
sparsity of layer roberta_layer.encoder.layer.0.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.5767228868272569
sparsity of layer roberta_layer.encoder.layer.0.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.5858205159505208
sparsity of layer roberta_layer.encoder.layer.0.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9444257948133681
sparsity of layer roberta_layer.encoder.layer.0.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9511464436848959
sparsity of layer roberta_layer.encoder.layer.0.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.842529296875
sparsity of layer roberta_layer.encoder.layer.0.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9162767198350694
sparsity of layer roberta_layer.encoder.layer.1.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7166697184244792
sparsity of layer roberta_layer.encoder.layer.1.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7349022759331597
sparsity of layer roberta_layer.encoder.layer.1.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9261084662543403
sparsity of layer roberta_layer.encoder.layer.1.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9509785970052084
sparsity of layer roberta_layer.encoder.layer.1.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8116586473253038
sparsity of layer roberta_layer.encoder.layer.1.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9039967854817709
sparsity of layer roberta_layer.encoder.layer.2.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.75433349609375
sparsity of layer roberta_layer.encoder.layer.2.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7701280381944444
sparsity of layer roberta_layer.encoder.layer.2.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.901458740234375
sparsity of layer roberta_layer.encoder.layer.2.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9385308159722222
sparsity of layer roberta_layer.encoder.layer.2.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.805216047498915
sparsity of layer roberta_layer.encoder.layer.2.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8930091857910156
sparsity of layer roberta_layer.encoder.layer.3.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7292650010850694
sparsity of layer roberta_layer.encoder.layer.3.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7525617811414931
sparsity of layer roberta_layer.encoder.layer.3.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8935377332899306
sparsity of layer roberta_layer.encoder.layer.3.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9190707736545138
sparsity of layer roberta_layer.encoder.layer.3.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8172183566623263
sparsity of layer roberta_layer.encoder.layer.3.output.dense.weight with tensor torch.Size([768, 3072]) is 0.8995162116156684
sparsity of layer roberta_layer.encoder.layer.4.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7185007731119792
sparsity of layer roberta_layer.encoder.layer.4.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7432234022352431
sparsity of layer roberta_layer.encoder.layer.4.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8890550401475694
sparsity of layer roberta_layer.encoder.layer.4.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9193030463324653
sparsity of layer roberta_layer.encoder.layer.4.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.840232425265842
sparsity of layer roberta_layer.encoder.layer.4.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9157587687174479
sparsity of layer roberta_layer.encoder.layer.5.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7463158501519097
sparsity of layer roberta_layer.encoder.layer.5.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7671932644314237
sparsity of layer roberta_layer.encoder.layer.5.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8643595377604166
sparsity of layer roberta_layer.encoder.layer.5.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9077165391710069
sparsity of layer roberta_layer.encoder.layer.5.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8579864501953125
sparsity of layer roberta_layer.encoder.layer.5.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9304105970594618
sparsity of layer roberta_layer.encoder.layer.6.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7491370307074653
sparsity of layer roberta_layer.encoder.layer.6.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7667626274956597
sparsity of layer roberta_layer.encoder.layer.6.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8723008897569444
sparsity of layer roberta_layer.encoder.layer.6.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9122958713107638
sparsity of layer roberta_layer.encoder.layer.6.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8428654140896268
sparsity of layer roberta_layer.encoder.layer.6.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9209336174858941
sparsity of layer roberta_layer.encoder.layer.7.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7185431586371528
sparsity of layer roberta_layer.encoder.layer.7.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7385525173611112
sparsity of layer roberta_layer.encoder.layer.7.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9189436170789931
sparsity of layer roberta_layer.encoder.layer.7.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9494900173611112
sparsity of layer roberta_layer.encoder.layer.7.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8401315477159288
sparsity of layer roberta_layer.encoder.layer.7.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9165768093532987
sparsity of layer roberta_layer.encoder.layer.8.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7480875651041667
sparsity of layer roberta_layer.encoder.layer.8.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7659064398871528
sparsity of layer roberta_layer.encoder.layer.8.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9129621717664931
sparsity of layer roberta_layer.encoder.layer.8.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9526587592230903
sparsity of layer roberta_layer.encoder.layer.8.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8497263590494791
sparsity of layer roberta_layer.encoder.layer.8.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9141133626302084
sparsity of layer roberta_layer.encoder.layer.9.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7576989067925347
sparsity of layer roberta_layer.encoder.layer.9.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7712792290581597
sparsity of layer roberta_layer.encoder.layer.9.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9290805392795138
sparsity of layer roberta_layer.encoder.layer.9.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9626905653211806
sparsity of layer roberta_layer.encoder.layer.9.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.8858371310763888
sparsity of layer roberta_layer.encoder.layer.9.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9196154276529948
sparsity of layer roberta_layer.encoder.layer.10.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7501661512586806
sparsity of layer roberta_layer.encoder.layer.10.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7696753607855903
sparsity of layer roberta_layer.encoder.layer.10.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.9304775661892362
sparsity of layer roberta_layer.encoder.layer.10.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9612562391493056
sparsity of layer roberta_layer.encoder.layer.10.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.9162868923611112
sparsity of layer roberta_layer.encoder.layer.10.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9364166259765625
sparsity of layer roberta_layer.encoder.layer.11.attention.self.query.weight with tensor torch.Size([768, 768]) is 0.7404378255208333
sparsity of layer roberta_layer.encoder.layer.11.attention.self.key.weight with tensor torch.Size([768, 768]) is 0.7494930691189237
sparsity of layer roberta_layer.encoder.layer.11.attention.self.value.weight with tensor torch.Size([768, 768]) is 0.8973778618706597
sparsity of layer roberta_layer.encoder.layer.11.attention.output.dense.weight with tensor torch.Size([768, 768]) is 0.9510175916883681
sparsity of layer roberta_layer.encoder.layer.11.intermediate.dense.weight with tensor torch.Size([3072, 768]) is 0.903167724609375
sparsity of layer roberta_layer.encoder.layer.11.output.dense.weight with tensor torch.Size([768, 3072]) is 0.9668634202745225
sparsity of layer roberta_layer.pooler.dense.weight with tensor torch.Size([768, 768]) is 0.9998287624782987
sparsity of layer rnn.weight_ih_l0 with tensor torch.Size([2048, 768]) is 0.8708057403564453
sparsity of layer rnn.weight_hh_l0 with tensor torch.Size([2048, 512]) is 0.9668264389038086
sparsity of layer rnn.weight_ih_l0_reverse with tensor torch.Size([2048, 768]) is 0.8205738067626953
sparsity of layer rnn.weight_hh_l0_reverse with tensor torch.Size([2048, 512]) is 0.9303703308105469
sparsity of layer rnn.weight_ih_l1 with tensor torch.Size([2048, 1024]) is 0.9045357704162598
sparsity of layer rnn.weight_hh_l1 with tensor torch.Size([2048, 512]) is 0.948857307434082
sparsity of layer rnn.weight_ih_l1_reverse with tensor torch.Size([2048, 1024]) is 0.8780655860900879
sparsity of layer rnn.weight_hh_l1_reverse with tensor torch.Size([2048, 512]) is 0.8993053436279297
sparsity of layer concat_l.weight with tensor torch.Size([512, 512]) is 0.7146072387695312
sparsity of layer concat_r.weight with tensor torch.Size([512, 1024]) is 0.7627754211425781
sparsity of layer concat_lg.weight with tensor torch.Size([512, 512]) is 0.692596435546875
sparsity of layer concat_rg.weight with tensor torch.Size([512, 1024]) is 0.7634868621826172
sparsity of layer ops.weight with tensor torch.Size([4, 1024]) is 0.587890625
sparsity of layer attn.attn.weight with tensor torch.Size([512, 1024]) is 0.8652935028076172
sparsity of layer attn.score.weight with tensor torch.Size([1, 512]) is 0.736328125
sparsity of layer score.attn.weight with tensor torch.Size([512, 1536]) is 0.7037734985351562
sparsity of layer score.score.weight with tensor torch.Size([1, 512]) is 0.50390625
sparsity of layer embeddings.weight with tensor torch.Size([4, 768]) is 0.677734375
sparsity of layer generate_l.weight with tensor torch.Size([512, 1792]) is 0.9550715855189732
sparsity of layer generate_r.weight with tensor torch.Size([512, 1792]) is 0.9662671770368304
sparsity of layer generate_lg.weight with tensor torch.Size([512, 1792]) is 0.9643314906529018
sparsity of layer generate_rg.weight with tensor torch.Size([512, 1792]) is 0.9704066685267857
sparsity of layer merge.weight with tensor torch.Size([512, 1792]) is 0.992173331124442
sparsity of layer merge_g.weight with tensor torch.Size([512, 1792]) is 0.9957253592354911
Final sparsity level of 0.893: 0.8930000082517243

JOB STATISTICS
==============
Job ID: 1507127
Cluster: snellius
User/Group: sliu/sliu
State: RUNNING
Nodes: 1
Cores per node: 18
CPU Utilized: 00:00:00
CPU Efficiency: 0.00% of 04:59:42 core-walltime
Job Wall-clock time: 00:16:39
Memory Utilized: 0.00 MB (estimated maximum)
Memory Efficiency: 0.00% of 0.00 MB (0.00 MB/node)
WARNING: Efficiency statistics may be misleading for RUNNING jobs.
